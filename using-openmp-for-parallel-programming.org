*  Chapter04 OpenMP Language Features
** 4.1 Introduction
*** Most commonly used features of the OpenMP library routines:
**** Parallel Construct 
**** Work-sharing Construct 
1. Loop Construct
2. Section Construct
3. Single COnstruct
4. Workshare Construct(Fortran only)
**** Data-sharing, No Wait, and Schedule Clauses (see [[https://msdn.microsoft.com/en-us/library/2kwb957d.aspx][OpenMP Clauses]])
*** Features enable the programmer to orchestrate the actions of different threads (4.6)
**** Barrier Construct
**** Critical Construct
**** Atomic Construct
**** Locks
**** Master Construct

** 4.2 Terminology
- OpenMP Directive
- Executable directive 
- Construct -- An OpenMP executable directive and the associated statement, loop, or structured block.

** 4.3 Parallel Construct
#+BEGIN_SRC C
  #pragma omp parallel
     {
       printf("The parallel region is executed by thread %d\n",
              omp_get_thread_num());
       if ( omp_get_thread_num() == 2 ) {
         printf("  Thread %d does things differently\n",
                omp_get_thread_num());
       }  /*-- End of parallel region --*/
#+END_SRC
- The construct is used to specify the computation that should be executed in parallel
- A team of treads is created; But it does not distribute the work of the region among the threads in paralle.
- If user does not use appropriate syntax to specify the action, the work will be replicated
- Implied barrier that forces all threads to wait until the work inside the region has been completed.
- The thread that encounters the parallel construct becomes the /master/ of the new team.
- ~omp get thread num()~ is used to obtain the number of each thread executing the parallel region
- Note that one cannot make any assumptions about the order in which the threads will execute the first ~printf~ statement.

** 4.4 Sharing the Work among Threads in an OpenMP Program
OpenMP's Work-sharingconstructs are used to distribute computation among the threads in a team.
Two main rules regarding work-sharing construct are as follow:
- Each work-sharing region must be encountered by all threads in a team or by none at all.
- THe sequence of work-sharing regions and barrier regions encountered must be the same for every thread in a team

*** 4.4.1 Loop Construct
- The loop must an inteeger counter variable whose value is changed by a fixed amount at each iteration until a specified bound is reached.
#+BEGIN_SRC C
  #pragma omp parallel shared(n) private(i)
  {
  #pragma omp for
    for (i=0; i<n; i++)
      printf("Thread %d executes loop iteration %d\n",
             omp_get_thread_num(),i);
  } /*-- End of parallel region --*/
#+END_SRC
- use a parallel directive to define a parallel region, then share its work among threads via the ~for~ work sharing directive
- the ~#pragma omp for~ directive states that iterations of the loop following it will be distributed.
- Clauses state which data in the region is shared and which is private.
- The order of result is *not* deterministic.

*** 4.4.2 The Section Construct
- It let us specify several different code regions to get different threads to carry out different kinds of work.
- It consists of two directives:
  - first, ~#pragma omp sections~ indicate the start of the construct.
  - second, ~#pragma omp section~ mark each distinct section.
- Each section must be a structured block of code that is independent of the other section.
- At run time, the specified code blocks are executed by the threads in the team. Each thread executes one code block at a time, and each code block will be executed exactly once.
- The most common use is to execute function or subroutine calls in parallel.
**** Example of parallel sections
#+BEGIN_SRC C
  #pragma omp parallel
  {
  #pragma omp sections
    {
  #pragma omp section
      (void) funcA();
  #pragma omp section
      (void) funcB();
    } /*-- End of sections block --*/
  } /*-- End of parallel region --*/
#+END_SRC
1. It contains one ~sections~ construct comprising two sections.
2. It limits the parallelism to two threads.
3. *Note* one cannot make any assumption on the specific order in which section blocks are executed. Even if when there are only thread which makes it executed sequentially.

*** 4.4.3 The Single Construct
- The /single construct/ is associated with the structured block of code immediately following it and specifies that this block should be executed by one thread only.
#+BEGIN_SRC C
  #pragma omp parallel shared(a,b) private(i)
  {
  #pragma omp single
    {
      a = 10;
      printf("Single construct executed by thread %d\n",
             omp_get_thread_num());
      /* A barrier is automatically inserted here */
  #pragma omp for
      for (i=0; i<n; i++)
        b[i] = a;
    } /*-- End of parallel region --*/
    printf("After the parallel region:\n");
    for (i=0; i<n; i++)
      printf("b[%d] = %d\n",i,b[i]);
#+END_SRC 
- One thread initializes the shared variable ~a~. This variable is then used to initialize vector ~b~ in the parallelized ~for~-loop.
- One might think the single construct could be ommited in this case, since every thread would write the same value of 10 to the same variable ~a~. *It is not true!*.
  - multiple threads could cause memory consistency problem.
  - multiple stores to the memory address could cause bad performance.
- *A barries is enssential before the ~#paragma omp for~ loop*
*** 4.4.4 Workshare Construct
(only supported in Fortran)
*** 4.4.5 Combined Parallel Work-sharing Construct
- Combined parallel work-sharing constructs are shortcuts that can be used when a parallel region comprises precisely one work-sharing construct, that is, the work- sharing region includes all the code in the parallel region.
** 4.5 Clauses to Control Parallel and Work-sharing Construct
- The OpenMP directive support a number of /clauses/, optional additions that control the behavior of the construct they apply to.
- Since the caluses are evaluated in external context, thus any variables that appear in them must be defined there.
  
*** 4.5.1 Shared Clause
- specify which data will be shared among the threads, so there is one unique instance of these variables, and each thread can freely read or modify the values.
- be careful about memory consistency.

*** 4.5.2 Private Clause
#+BEGIN_SRC C
  #pragma omp parallel for private(i,a)
  for (i=0; i<n; i++)
    {
      a = i+1;
      printf("Thread %d has a value of a = %d for i = %d\n",
             omp_get_thread_num(),a,i);
    } /*-- End of parallel for --*/
#+END_SRC
- The values of private data are /undefined/ upon entry to and exit from the specific construct, even if the corresponding variable was defined prior to the region. *Be careful!*

*** 4.5.3 Lastprivate Clause
- It ensures that the last value of a data object listed is accessible after the corresponding construct has completed execution.
#+BEGIN_SRC C
  #pragma omp parallel for private(i) lastprivate(a)
  for (i=0; i<n; i++)
    {
      a = i+1;
      printf("Thread %d has a value of a = %d for i = %d\n",
             omp_get_thread_num(),a,i);
    } /*-- End of parallel for --*/
  printf("Value of a after parallel for: a = %d\n",a);
#+END_SRC
- The same functionality can be implemented by using an additional shared variable.
#+BEGIN_SRC C
  #pragma omp parallel for private(i) private(a) shared(a_shared)
  for (i=0; i<n; i++)
    {
      a = i+1;
      printf("Thread %d has a value of a = %d for i = %d\n",
             omp_get_thread_num(),a,i);
      if ( i == n-1 ) a_shared = a;
    } /*-- End of parallel for --*/
#+END_SRC

*** 4.5.4 Fristprivate Clause
The private data is also undefined on entry to the construct where it is specified. This could be a problem if we need to pre-initialize private variable with values that are available prior to the region in which they will be used.
- use ~firstprivate~ construct to help out.
- ~firstprivate~ cluase is supported on ~parallel~ construct, plus the work-sharing ~loop~, ~sections~, and ~single~ constructs.
#+BEGIN_SRC C
  for(i=0; i<vlen; i++) a[i] = -i-1;

  indx = 4;
  #pragma omp parallel default(none) firstprivate(indx) private(i,TID) shared(n,a)
  TID = omp_get_thread_num();

  indx += n*TID;
  for(i=indx; i<indx+n; i++)
    a[i] = TID + 1;
  } /*-- End of parallel region --*/

  printf("After the parallel region:\n");
  for (i=0; i<vlen; i++)
    printf("a[%d] = %d\n",i,a[i]);
#+END_SRC
- The code block shows each thread in a parallel region needs access to a thread-specific section of a vector but access start at a offset.
- The offset = 4.
- The length of each thread's section of the array is ginven by ~n~.
- This example could be implemented by using a shared variable, ~offset~ which contains the initial offset into vector ~a~.
#+BEGIN_SRC C
  #pragma omp parallel default(none) private(i,TID,indx)  \
    shared(n,offset,a)
     {
       TID = omp_get_thread_num();
       indx = offset + n*TID;
       for(i=indx; i<indx+n; i++)
         a[i] = TID + 1;
     } /*-- End of parallel region --*/

#+END_SRC
- In general, /read-only/ variables can be passed in as ~shared~ variables instead of ~firstprivate~.
  
  
*** 4.5.5 Default Clause
- It is used to give variables a default data-sharing attribute: ~default(shared)~ assigns the shared attribute to all variables referenced in the construct.

*** 4.5.6 Nowait Clause
It allows the programmer to fine-tune a program's performance.
- It could suppress the implicit barrier at the end of work-sharing constructs. So when threads reach the end of the construct, they will immediately proceed to perform other work.
- *Note*, the barrier at the end of a parallel region cannot be suppressed.
- Once a parallel program runs correctly, one can try to identify places where a barrier is not needed and insert the ~nowait~ clause.
#+BEGIN_SRC C
  #pragma omp for nowait
  for (i=0; i<n; i++)
    {
      ............
    }

#+END_SRC

*** 4.5.7 Schedule Clause
The ~schedule~ clause is supported on the loop construct only. It is used to control the manner in which loop iterations are distributed over the threads.
#+BEGIN_SRC C
  #pragma omp parallel for default(none) schedule(runtime)        \
    private(i,j) shared(n)
  for (i=0; i<n; i++)
    {
      printf("Iteration %d executed by thread %d\n",
             i, omp_get_thread_num());
      for (j=0; j<i; j++)
        system("sleep 1");
    } /*-- End of parallel for --*/

#+END_SRC
- Syntax is ~schedule(kind [, chunk_size])~.
***** schedule kinds could be:
- static
  Iterations are divided into chunks of size ~chunk_size~, the ~chunk_size~ need not be a constant. It is the most efficient from a performance point of view, others have high overheads.
- dynamic
  The iterations are assigned to threads as the threads request them. The thread executes the chunk of iterations (controlled through the ~chunk_size~ parameter), then requests another chunk until there are no more chunks to work on.
- guided
  Similar to dynamic, see OpenMP standard for detail. (a little confusing)
- runtime, very convenient.
- The schedule is make at run times.
- Both the ~dynamic~ and ~guided~ schedules are useful for handling poorly balanced and unpredictable workloads. The difference between them is that with the ~guided~ schedule, the size of the chunk (of iterations) decreases over time.

** 4.6 OpenMP Synchronization Construct
*** 4.6.1 Barrier Construct
A barrier is a point in the execution of a program where threads wiat for each other.
#+BEGIN_SRC C
  #pragma omp parallel private(TID)
    {
      TID = omp_get_thread_num();
      if (TID < omp_get_num_threads()/2 ) system("sleep 3");
      (void) print_time(TID,"before");
  #pragma omp barrier
      (void) print_time(TID,"after ");
    } /*-- End of parallel region --*/
#+END_SRC

*** 4.6.2 Ordered Construct
It allows the execution of a structured block within a parallel loop in sequential order. Such as, to enforce an ordering on the printing of data computed by different threads.

It could als be used to help determine wheter there are any data races in the associated code.

*** 4.6.3 Critical Construct
It ensures that multiple threads do not attempt to update the same shared data simultaneously.
The associated code is referred to as a critical region/section.
#+BEGIN_SRC C
  sum = 0;
  for (i=0; i<n; i++)
    sum += a[i];
#+END_SRC
The code loop sums up the elements of a vector a. A parallel version could be:
1) let each thread independently add up a subset of the elements of the vector.
2) store the result in a private variable.
3) When all threads are doen, they add up their private contributions to get the sum.
#+BEGIN_SRC C
  /*--pseudo parallel code for the summation--*/
  /*-- Executed by thread 0 --*/   
  sumLocal = 0;
  for (i=0; i<n/2; i++)
    sumLocal += a[i];
  sum += sumLocal;

  /*-- Executed by thread 1 --*/
  sumLocal = 0;
  for (i=n/2-1; i<n; i++)
    sumLocal += a[i];
  sum += sumLocal;
#+END_SRC
We must ensure that only one update of sum could take place at any time. Otherwise, it could cause *data race* problem. 

The below shows a simple implementation of a reduction operation:
#+BEGIN_SRC C
  sum = 0;
  #pragma omp parallel shared(n,a,sum) private(TID,sumLocal)
  {
    TID = omp_get_thread_num();
    sumLocal = 0;
  #pragma omp for
    for (i=0; i<n; i++)
      sumLocal += a[i];
  #pragma omp critical (update_sum)
    {
      sum += sumLocal;
      printf("TID=%d: sumLocal=%d sum = %d\n",TID,sumLocal,sum);
    }
  } /*-- End of parallel region --*/
  printf("Value of sum after parallel region: %d\n",sum);
#+END_SRC
When the threads are finished with their part of the ~for~-loop, they enter the critical region. By definition, only one thread at a time updates ~sum~.


Another common situation where this construct is useful is when minima and maxima are formed:
#+BEGIN_SRC C
  #pragma omp parallel private(ix, LScale, lssq, Temp) shared(Scale, ssq, x)
    {
  #pragma omp for
      for(ix = 1, ix<N, ix++)
        {
          LScale = ....;
        
  #pragma omp critical
          {
            if(Scale < LScale){
              ssq = (Scale/LScale) *ssq + lssq;
              Scale = LScale;
            }else
              ssq = ssq + (LScale / Scale) * Lssq
                } /* End of critical region --*/
        }
    } /*-- End of parallel region --*/
#+END_SRC
*** 4.6.4 Atomic Construct
The atomic construct, which also enables multiple threads to update shared data without interference, can be an efficient alternative to the critical region.

In contrast to other constructs, it is applied only to the (single) assignment statement that immediately follows it; this statement must have a certain form in order for the construct to be valid, and thus its range of applicability is strictly limited.

#+BEGIN_SRC C
  int ic, i, n;
  ic = 0;
  #pragma omp parallel shared(n,ic) private(i)
  for (i=0; i++, i<n)
    {
  #pragma omp atomic
      ic = ic + 1;
    }
  printf("counter = %d\n", ic);

#+END_SRC
The supported operations are: ~+, *, -, /, &, ^, |, <<, >>~

Another example of using atomic:
#+BEGIN_SRC C
  int ic, i, n;
  ic = 0;
  #pragma omp parallel shared(n,ic) private(i)
  for (i=0; i++, i<n)
    {
  #pragma omp atomic
      ic = ic + bigfunc();
    }
  printf("counter = %d\n", ic);
#+END_SRC
*Note* the ~atomic~ construct does not protect the execution of function ~bigfunc~. It is only the update to the memory location of the variable ~ic~ that will occur atomically. If you do not want the function ~bigfunc~ to be executed in parallem among threads, use ~critical~ construct.

*** 4.6.5 Locks
- /simple locks/ which may not be locked if already  in a locked state, with type ~omp_lock_t~.
- /nestable locks/ which may be locked multiple times by the same thread, with type ~omp_nest_lock_t~.

The general procedure to use locks is:
1. Define the lock variable.
2. Initialize the lock via a call to ~omp_init_lock~.
3. Set the lock using ~omp_set_lock~ or ~omp_test_lock~. 
4. Unset a lock after the work is done via a call to ~omp_unset_lock~.
5. Remove the lock associated via a call to ~omp_destory_lock~.

*** 4.6.6 Master Construct
The ~master~ construct defines a block of code that is guaranteed to be executed by the master thread only.

It lacks the implied barrier on entry or exit.
#+BEGIN_SRC C
  #pragma omp parallel shared(a,b) private(i)
   {
  #pragma omp master
     {
       a = 10;
       printf("Master construct is executed by thread %d\n",
              omp_get_thread_num());
     }
  #pragma omp barrier
  #pragma omp for
     for (i=0; i<n; i++)
       b[i] = a;
   } /*-- End of parallel region --*/
  printf("After the parallel region:\n");
  for (i=0; i<n; i++)
    printf("b[%d] = %d\n",i,b[i]);
#+END_SRC
Comparing with single construct:
The initailization of variable a is now guaranteed to be performed by the master thread.
And ~barrier~ needs to be inserted for correctness.
** 4.7 Interaction with the Execuation Environment
There are variables that could be queried or modified in the OpenMP environment.
****** Internal control variables. They could not be accessed or modified directly at application level, but could be queried and modified through OpenMP functions and environment variables.

- nthreads-var
- dyn-var
- nest-var
- run-sched-var
- def-sched-var

To control those variable, ~need~ include ~omp.h~ header file:
****** Control the number of threads in a parallel region by setting value ~nthread-var~.
- ~OMP_NUM_THREADS~, used at command-line
- ~omp_set_num_threads~, used as ~omp_set_num_threads(scalar-integer-expression)~.
- ~num-threads~, used together with a prallel construct.
- ~omp_get_max_threads()~ routine, returns the largest number of threads available for the next parallel region.


****** Optimize the use of system resources for thoughout by controling the value of ~dyn-var~: 
- ~OMP_DYNAMIC(flay)~, flag could be ~true~ or ~false~.
- ~omp_set_dynamic~, adjusts the value of ~dyn-var~ at run time. ~omp_get_dynamic~ can be used to retrieve the current setting at run time. 

****** Set if the execution of parallel is nested or not via variable ~nest-var~
- ~OMP_NESTED~
- ~omp_set_nested~

****** Control the default schedule to be applied to parallel loops in a program
- its value decide the manner when schedule type is ~runtime~.

****** Other useful library routines:
- ~omp_get_num_threads~, retrieve the number of threads in the current team
- ~omp_get_thread_num~, returns the number of the calling thread as an integer value.
- ~omp_get_num_procs~, returns an integer as the total number of processors available to the program at the instant in which it is called.
- ~omp_in_parallel~, returns true if it is called from within an active parallel region.

** 4.8 More OpenMP Clauses
*** 4.8.1 If Clause
Since some overheads incurred with the creation and termination of a parallel region, sush it is worth to use ~if~ cluase to specify whether there is enough work worth of doing so.
#+BEGIN_SRC C
  #pragma omp parallel if (n > 5) default(none)   \
    private(TID) shared(n)
     {
       TID = omp_get_thread_num();
  #pragma omp single
       {
         printf("Value of n = %d\n",n);
         printf("Number of threads in parallel region: %d\n",
                omp_get_num_threads());
       }
       printf("Print statement executed by thread %d\n",TID);
     }  /*-- End of parallel region --*/

#+END_SRC
- if n > 5, the parallel region is executed by the number of threads available. Otherwise, one thread executes the region, so the region is then an /inactive/ parallel region.
- A ~#pragma omp single~ is used to avoid executing the first two print statements multiple times.

*** 4.8.2 Num_threads Clause
The ~num-threads~ clause is supported on the ~parallel~ construct only and can be used to specify how many threads should be in the team executing the parallel region. 
Code shows the usage of ~if~ and ~num-threads~ cluases:
#+BEGIN_SRC C
  #pragma omp parallel if (n > 5) num_threads(n) default(none)    \
    private(TID) shared(n)
     {
       TID = omp_get_thread_num();
  #pragma omp single
       {
         printf("Value of n = %d\n",n);
         printf("Number of threads in parallel region: %d\n",
                omp_get_num_threads());
       }
       printf("Print statement executed by thread %d\n",TID);
     }  /*-- End of parallel region --*/

#+END_SRC

*** 4.8.3 Ordered Clause
It does not take any arguments and is supported on the loop construct only.
#+BEGIN_SRC C
  #pragma omp parallel for default(none) ordered schedule(runtime) private(i,TID) shared(n,a,b)
  for (i=0; i<n; i++)
    {
      TID = omp_get_thread_num();
      printf("Thread %d updates a[%d]\n",TID,i);
      a[i] += i;
  #pragma omp ordered
      {printf("Thread %d prints value of a[%d] = %d\n",TID,i,a[i]);}
    }  /*-- End of parallel for --*/
#+END_SRC

*** 4.8.4 Reduction Clause
OpenMP provides the ~reduction~ clause for specifying some forms of recurrence calculations (involving mathematically associative and commutative operators) so that they can be performed in parallel without code modification.
#+BEGIN_SRC C
  #pragma omp parallel for default(none) shared(n,a) reduction(+:sum)
  for (i=0; i<n; i++)
    sum += a[i];
  /*-- End of parallel reduction --*/
  printf("Value of sum after parallel region: %d\n",sum);
#+END_SRC
- specify that ~sum~ will hold the result of a Reduction
- identified via the ~+~ operator.
****** Things need to be noticed when using with C++
- Aggregate types (including arrays), pointer types, and reference types are not supported.
- A reduction variable must not be const-qualified.
- The operator specified on the clause can not be overloaded with respect to the variables that appear in the clause.

*** 4.8.5 Copying Clause
*** 4.8.6 Copyprivate Cluase

** 4.9 Advanced OpenMP Construct
*** 4.9.1 Nested Parallelism
*** 4.9.2 Flush Directive
Each thread executing an OpenMP code potentially has its own /temporary view/ of the values of shared data. (/relaxed consistancy model/)

Sometimes update values of shared values must become visible to other threads in-between synchronization points.
*** 4.9.3 Threadprivate Directive
By default, global data is shared, which is often appropriate. But in some situations we may need, or would prefer to have, private data that persists throughout the computation. This is where the ~threadprivate~ directive comes in handy.

By default, the threadprivate copies are not allocated or defined. The programmer must take care of this task in the parallel region.
** Summary 

* Chapter05 How to Get Good Performance by Using OpenMP
** Performance considerations for sequential programs
*** Suboptimal usage of the cache memory subsystem
- cache miss at the highest level in the cache hierarchy is expensive since it implies that data must be fectched from main memory before it can be used.
*** Memory access patterns and performance
- A major goal is to organize data accesses so that values are used as often as possible while they are still in cache.
- The most common strategies for doing so are based on the fact that programming languages typically specify that the elements of arrays be stored contiguously in memory. Thus, if an arry element is fetched into cache, "nearby" elements of the array wil be in the same cache block and will be fetched as part of the same transactions.
  - rowwise storage, when an array element is transferred to cache, neibhors in the same rwo are typically also transferred as part of the same cache line.
  - Therefor, matrix-based compuation should access the elements of the array row by row, not column by column.
*** Translation-Lookaside Buffer (TLB)
- In fact, the physical pages that are available to a program may be spread out in memory, and so the virtual pages must be mapped to the physical ones. A page table that records this mapping which is used to locate the physical page corresponding to a virtual page when it is referenced during execution. However, the page table resides in main memory, and this procedure is time-consuming.
- A special cache was developed that stores recently accessed entries in the page table. It is known as TLB.
- The TLB is on the critical path for performance. Whenever data is needed for a calculation and the information needed to determine its physical location is not in the TLB, the processor waits until the requested information is available. Only then it is able to transfer the values and resume execution.
- Just as with data cache, it is important to make good use of the TLB entries: a page should be heavily referenced while its location is stored in the TLB. Whenever a program does not access data in storage order, frequent cache reload plus a large number of TLB misses may result.

*** Loop Optimization
- loop interchange
  - If any memory location is referenced more than once in the loop nest and if at least one of those references modifies its value, then their relative ordering must not be changed by the transformation.
- loop unrolling
  - A short loop nest: loop overheads are relatively high when each interation has small number of operations.
    #+BEGIN_SRC c
      for (int i=1; i<n; i++) {
        a[i] = b[i] + 1;
        c[i] = a[i] + a[i-1] + b[i-1];
       }
    #+END_SRC
  - An unrolled loop: previous code has been unrolled by a factor of 2 to reduce the loop overheads
    #+BEGIN_SRC c
      for (int i=1; i<n; i+=2) {
        a[i] = b[i] + 1;
        c[i] = a[i] + a[i-1] + b[i-1];
        a[i+1] = b[i+1] + 1;
        c[i+1] = a[i+1] + a[i] + b[i];
       }
    #+END_SRC
    - In this example, the loop body executes 2 iterations in one pass. The number is called "unroll factor". A higher value tends to give better performance but also increase the number of registers needed.
    - Nowadays, a programmer seldom needs to apply this transformation manually, since compilers are very good at doing this. They are also very good at determining the optimal unroll factor.
*** Using pointers and contiguous memory in C
- pointer aliasing problem

** Measuring OpenMP performance
How to meausre and identify what factors determin overall program performance?
- Common practice is to use a standard operating system command.
  #+BEGIN_SRC sh
    $ /bin/time ./program.exe
    real   5.4
    user   3.2
    sys    1.0
  #+END_SRC
  These three numbers can be used to get a first impresstion on the performance.
  - The first number tells us that the program took 5.4 seconds from beginning to end.
  - The second number shows it spent 3.2 seconds in user mode; this is the time the program spent executing outside any operating system services.
  - The third number is the time spent on operating system services, such as input/output routines.
  - The sum of user and system time is referred to as the /CPU time/.
  - The "real" time is also referred to as /wall-clock time/ or /elapsed time/.
  - The difference between the elapsed time and the CPU time is commonly caused by the application did not get a full processor to itself.
- Parallel overhead
  - In includes the time to create, start and stop threads, the extra work needed to figure out what each task is to perform, the time spent waiing in barriers and at critical sections and locks, and the time spent computing some operations redundantly.
*** Understanding the Performance of an OpenMP Program
The observable performance of OpenMP program is influenced by at least the following factors, in addition to those that play a role in sequential performance:
- The manner in which memory is accessed by the individual threads.
- The fraction of the work that is sequential, or replicated.
- The amount of time spent handling OpenMP constructs.
- The load imbalance between synchronization points.

*** Overheads of the OpenMP translation
*** Interaction with execution environment
On an SMP system we strongly recommend that a program use fewer than the total number of processors, even if the system is dedicated to a single application.
** Best practices
*** Optimize barrier use
*** Avoid the ordered construct
*** Avoid large critical regions
*** Maximize parallel regions
*** Avoid parallel regions in inner loops
A bad example:
#+BEGIN_SRC c
  for (i=0; i<n; i++)
    for (j=0; j<n; j++)
  #pragma omp parallel for
      for (k=0; k<n; k++)
        { .........}
#+END_SRC
A good example:
#+BEGIN_SRC c
  #pragma omp parallel
  for (i=0; i<n; i++)
    for (j=0; j<n; j++)
  #pragma omp for
      for (k=0; k<n; k++)
        { .........}
#+END_SRC
It reduces the overhead of parallel construct.

*** Address poor load balance

** Additional performance considerations
(read details later)
** Case study: The matrix times vector product (5.6)

* Using OpenMP in the real world
** Scalability challenges for OpenMP
To create a parallel program that scales to a large number of thread, a programmer must carefully consider the nature of the parallelism that can be exploited.
- Data parallelism 
  - each thread carries out the same work, but on its own portion of the data set.
  - synchronization must be inserted to ensure that the correct ordering of accessing is enforced. (suited for computational fluid dynamics code that find the dscrete solution of partial differential equations)
  - Characteristic is that the same computations are performed on a large number of points of a computational grid.
- Task parallelism
  - give the threads different kinds of work to do.
- OpenMP with NUMA
  - OpenMP assuems that the cost of acessing a memory location is *uniform* across the system.
  - Given the large remote-access latencies of there cc-NUMA architectures, obtaining a program with a high level of data locality is potentially the most important challenge for performance.

** Achieving scalability on cc-NUMA architectures
OpenMP does not provide a direct means of optimizing code for cc-NUMA systems and therefor can only indirectly influence the performance of a program on such a platform.

*** Memory placement and thread binding
- data allocation, more accurately page allocation is under control of the operating system. On a cc-NUMA architecture, the pages of data belonging to a given program may be distributed across the nodes of the system.
- First touch policy
  - the thread initialize a data object gets the page associated with that data item in the memory local to the processor its is currently executing on.
- Some example scenarios that may lead to inefficient thread and memory placement while an OpenMP code is executed:
  1) Unwanted thread migration
  2) The application requires irregular or changing memory access patterns.
  3) Inefficient thread placement for hybrid codes.
     Such as the remote memory access to the data of their master threads.
  4) Changing thread theam compositions in nested OpenMP codes.

** Combining OpenMP and Message Passing
** Performance analysis of OpenMP programs
*** Several approaches can be used to obtain performance data
**** sampling
- based on periodic clock-based OS interrupts or hardware counter traps, such as the overflow hardware counter registers.
- At the sampling points, the values of performance data (such as the program counter, the time, call stacks and hardware-counter overflow data) are collected and recorded.
- Unix ~gprof~ tool.
**** code instrumentation
calls to a tracing library are inserted in the code by the programmer, the compiler, or a tool. These library calls will write performance information into a performance profile file during program execution.
*** Interpreting timing information (6.6.2)
/pending/
Timing information can be used to obtain the following metrics:
1) Parallelization coverage
2) Useful parallel time
3) Estimating the parallel efficiency
4) The workshare duration
5) Thread synchronization
6) Load balancing
7) Work scheduling

*** Using hardware counters
A small set of counters that are most commonly available and useful to measure.
**** The /number of executed instructions/ per thread provides various interesting insights. 
- Comparing this metric between threads can point to a workload imbalance within parallel regions.

**** The number of /instruction per second/ during the thread useful time is an indicator of how well the processor pipeline are being used.
- If the ratio is low for certain routines, one might want to check whether threads spend their time waiting for work, are waiting in a synchronization phase or waiting for the cache and memory system.

**** The /number of data cache and translation-llokaside buffer (TLB) misses/ 
- Both provide measures for the efficiency of memory accesses. 
- Insight is gained by comparing values between threads, observing the behavior over a period of time or by combining the values with other metrics. For example, an interesting statistic is the ratio of cache or TLB misses per instruction. If this ratio is high for certain routines or threads, it is worth trying to change the memory access pattern.

**** The /cost per cache miss/
- Can provide a measure for *the impact of remote memory access on a cc-NUMA architectures* or DSM system. An estimate of this metric can be obtained by combining the following attributes:
- elapsed execution time
- number of instructions executed during the useful parallel time
- number of millions of instructions per sencond that could be obtained if the cost for a cache miss were zero.
- number of cache misses that occurred during the useful parallel time.

More interesting than the absolute value of this metric is how it compares between threads. If the cost per cache miss for certain routines varies widely between the threads, some data could be stored in such a way that it causes remote memory access for some threads, thereby increasing the overall execu- tion time. 


