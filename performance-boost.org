* Performance monitor on AMD
** What criteria do I need to measure to identify the performance bottleneck?
** How to monitor each of those criteria?
* Performance bost using libnuma
** How to use libnuma to boost?
- check numa architectures on Linux
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ numactl --hardware
    available: 8 nodes (0-7)
    node 0 cpus: 0 4 8 12 16 20 24 28        
    node 0 size: 65526 MB
    node 0 free: 62754 MB
    node 1 cpus: 32 36 40 44 48 52 56 60     
    node 1 size: 65536 MB
    node 1 free: 60707 MB
    node 2 cpus: 2 6 10 14 18 22 26 30      
    node 2 size: 65536 MB
    node 2 free: 63404 MB
    node 3 cpus: 34 38 42 46 50 54 58 62   
    node 3 size: 65536 MB
    node 3 free: 63005 MB
    node 4 cpus: 3 7 11 15 19 23 27 31
    node 4 size: 65536 MB
    node 4 free: 62909 MB
    node 5 cpus: 35 39 43 47 51 55 59 63
    node 5 size: 65536 MB
    node 5 free: 63202 MB
    node 6 cpus: 1 5 9 13 17 21 25 29
    node 6 size: 65536 MB
    node 6 free: 11 MB
    node 7 cpus: 33 37 41 45 49 53 57 61
    node 7 size: 65520 MB
    node 7 free: 51391 MB
    node distances:
    node   0   1   2   3   4   5   6   7 
    0:  10  16  16  22  16  22  16  22 
    1:  16  10  22  16  16  22  22  16 
    2:  16  22  10  16  16  16  16  16 
    3:  22  16  16  10  16  16  22  22 
    4:  16  16  16  16  10  16  16  22 
    5:  22  22  16  16  16  10  22  16 
    6:  16  22  16  22  16  22  10  16 
    7:  22  16  16  22  22  16  16  10 
  #+END_SRC
  - There are seven processors, each has 7 cores
    
*** How to retrieve the core id in which a thread is running? 
  Use =sched_getcpu()=
  #+BEGIN_SRC c
    #include <stdio.h>
    #include <sched.h>
    #include <omp.h>

    int main() {
    #pragma omp parallel
      {
        int thread_num = omp_get_thread_num();
        int cpu_num = sched_getcpu();
        printf("Thread %3d is running on CPU %3d\n", thread_num, cpu_num);
      }

      return 0;
    }
  #+END_SRC
  
  Use command line to check the affinity and execution result:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ GOMP_CPU_AFFINITY='0,1,2,3' ./bin/openmp_with_numa 10 10 4
    The number of highest possible node in the system is: 63
    Thread   0 is running on CPU   0
    Thread   1 is running on CPU   1
    Thread   3 is running on CPU   3
    Thread   2 is running on CPU   2
    From initialization to finished, use: 12ms
  #+END_SRC
** NUMA API
*** Introduction
- On an SMP(symmetric multiprocessing) system, all CPUs have equal access to the same shared memory controller that connects to all the memory chips (DIMMs). Communication between the CPUs also goes through this shared resource, which can become congested.
- The number of memory chips that can be managed by the single controller is also limited, which limit how much memory can be suppported by the system. In addition, the latency to access memory through this single traffic hub is relative high.
- The NUMA architecture was designed to surpass the scalability limits of the SMP architecture. Instead of having a single memory controller per computer, the system is split into multiple nodes.
  - Each node has processors and its own memory.
  - The processors have very fast access to the local memory in the node.
  - All the nodes in the system are connected using a fast interconnect.
- NUMA policy is concerned with putting memory allocations on specific nodes to let programs access them as quickly as possible.
  *The primary way to do this is to allocate memory for a thread on its local node and keep the thread running there (node affinity)*
  1) plan your thread execution, for which thread executing on which CPU 
  2) allocate memory ahead based on the node position of that CPU
- In addition to =numactl= library, =numastat= collect statistics about the memory allocation and =numademo= to show the effect of different policies on the system.

*** Policies
- Policies can be set process or per memory region.
  - Policies set per memory region, also called VMA policies3, allow a process to set a policy for a block of memory in its address space. Memory region policies have a higher priority than the process policy.

*** libnuma
- =nodemask_t= is a fixed size bit set of node numbers.
  #+BEGIN_SRC c
    nodemask_set(&mask, maxnode); /* set node highest */
    if (nodemask_isset(&mask, 1)) { /* is node 1 set? */
      ...
     }
    nodemask_clr(&mask, maxnode); /* clear highest node again */

  #+END_SRC
  
- allocation memory on node/set of nodes
  1) =void *numa_alloc_onnode(size_t size, int node);=
  2) =void *numa_alloc_interleaved_subset(size_t size, struct bitmask *nodemask);=

- libnuma process policy
  When existing code in a program cannot be modified to use the numa_alloc functions directly, it is sometimes useful to change the process policy in a program. This way, specific subfunctions can be run with a nondefault policy without actually modifying their code.
  - Each thread has a default memory policy inherited from its parent. Unless changed with numactl, this policy is normally used to allocate memory preferably on the current node.
  - =numa_set_interleave_mask= enables interleaving for the current thread. All future memory allocations allocate memory round robin interleaved over the nodemask specified.
  - Process policy can also be used to set a policy for a child process before starting it.
  - =numa_bind()= binds the current task and its children to the nodes specified in nodemask. They will only run on the CPUs of the specified nodes and only be able to allocate memory from them. This function is equivalent to calling =numa_run_on_node_mask(nodemask)= followed by =numa_set_membind(nodemask)=.
    - =numa_run_on_node_mask()= runs the current task and its children only on nodes specified in =nodemask=. They will not migrate to CPUs of other nodes until the node affinity is reset with a new call to =numa_run_on_node_mask()=. Passing =numa_all_nodes= permits the kernel to schedule on all nodes again.
    - =void numa_set_membind(struct bitmask *nodemask);= sets the memory allocation mask. The task will only allocate memory from the nodes set in nodemask.

- Changing the policy of existing memory areas
  When working with shared memory, it is often not possible to use the numa_alloc family of functions to allocate memory. The memory must be gotten from shmat() or mmap instead. To allow libnuma programs to set policy on such areas, there are additional functions for setting memory policy for already existing memory areas.
  These function only affect future allocation in the specified area.

- Except allocating memory on specific nodes, another part of NUMA policy is to run the thread on the CPUs of the correct node.
  A simple way to use libnuma is the =numa_bind= function. It binds both the CPU and the memory of the process allocated in the future to a specific nodemask.
  - Example of binding process CPU and memory allocation to node 1 using numa_bind:
    #+BEGIN_SRC c
      nodemask_t mask;
      nodemask_zero(&mask);
      nodemask_set(&mask 1);
      numa_bind(&mask);
    #+END_SRC

  - =numa_get_run_node_mask()= returns a mask of CPUs on which the current task is allowed to run. This can be used to save and restore the scheduler affinity state before running a child process or starting a thread.

- Most functions in this library are only concerned about numa nodes and their memory. But some function which deals with the CPUs associated with numa nodes.
  - =int numa_node_to_cpus(int node, struct bitmask *mask);= convert a node number to a bitmask of CPUs. 

- NUMA allocation statistics with numastat
  - The statistic info is for each node. It aggregates the results from all cores on a node to form a single result for the entire node.
  - It reports the following statistics:
    1) numa_hit
    2) numa_miss
    3) numa_foreign
    4) local_node
    5) interleave_hit
    6) other_node
       
    The difference between numa_miss and numa_hit and local_node and foreign_node is that the first two count hit or miss for the NUMA policy. The latter count if the allocation was on the same node as the requesting thread.
* Use pthread to do the experiment
** Test serial version of code for memory access time
*** code to measure access time
#+BEGIN_SRC c
  #include <iostream>
  #include <vector>
  #include <sched.h>
  #include <stdio.h>
  #include <numa.h>
  #include <stdlib.h>
  #include <string>
  #include <map>
  #include <pthread.h>
  #include <sys/types.h>
  #include <unistd.h>


  using namespace std;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse);

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  /**allocate and access on the same node*/
  int main(int argc, char* argv[]) {
    if (argc != 3) {
      printf("usage: <size> <remote=1 true, or 0 false>, exit...\n");
      exit(-1);
    }

    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else {
      numa_set_strict(1);
    }
    
    map<int, int> topology;
    map<int, vector<int> > topology_inverse;
    initMap(topology, topology_inverse);

    long mx_size = atol(argv[1]);
    int remote = atoi(argv[2]);
    int on_cpu = 0;
    int on_node = 0;
    pid_t pid;
    pthread_t tid;

    pid = getpid();
    tid = pthread_self();
    printf("main thread: pid %lu tid %lu, on cpu: %d\n", (unsigned long)pid, (unsigned long)tid, sched_getcpu());
    
    cpu_set_t cpuset;
    cpu_set_t allcpuset;

    CPU_ZERO(&cpuset);
    CPU_SET(on_cpu, &cpuset);
    pthread_setaffinity_np(tid, sizeof(cpu_set_t), &cpuset);
    int cpu_num = sched_getcpu();
    printf("Now, main thread tid = %lu, is executing on cpu: %d, on node %d\n",pthread_self(), cpu_num, topology[cpu_num]);

    float* mx = NULL;
    
    if (remote) {
      on_node = 7;
    }
    printf("will allocate memory on node %d\n", on_node);
    mx = (float*)numa_alloc_onnode(mx_size * mx_size * sizeof(float), on_node);
    if (mx == NULL) {
      printf("could not allocate memory on node %d, exit...\n", on_node);
    }
    
    auto start = tick();
    for (long i = 0; i < mx_size * mx_size; i++) {
      mx[i] = i + i * 0.5 + i / 2;
    }

    auto end = tick();
    if (remote && topology[sched_getcpu()] != on_node) {
      std::cout << "remote access, use: " << end - start << endl;
    } else if (!remote && topology[sched_getcpu()] == on_node) {
      std::cout << "local access, use: " << end - start << endl;
    }

    numa_free(mx, mx_size * mx_size * sizeof(float));

    return 0;
  }

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse) {

    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }
#+END_SRC

*** access time records
size = 12800
|   id |      remote |      local |
|    1 |  6190994967 | 5525219763 |
|    2 |  4853278945 | 4454001759 |
|    3 |  5865416519 | 4404425967 |
|    4 |  6259437334 | 5740065838 |
|    5 |  5452799794 | 5076688723 |
| mean | 5724385500. | 5040080410 |
|------+-------------+------------|
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)

It is clear to see the effect of remote access is slower.
  
** Test parallel version of code for memory access and multiplication with vector
*** code with parallel access and multiplication
#+BEGIN_SRC c
  //
  // Created by zwpdbh on 07/05/2018.
  //

  #include <pthread.h>
  #include <iostream>
  #include <sched.h>
  #include <sys/types.h>
  #include <unistd.h>
  #include <sched.h>
  #include <map>
  #include <vector>
  #include <numa.h>
  #include <chrono>

  using namespace std;

  long ROWS;
  long COLS;
  int NUM_THREADS = 64;
  int NUM_NODES = 8;

  float** mx = nullptr;
  float* v = nullptr;
  float* w = nullptr;
  long s = 0;
  int with_numa = 1;
  map<int, int> topology;
  map<int, vector<int> > topology_inverse;

  pthread_t main_thread;
  pthread_barrier_t b;

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  typedef struct pthreadInfo {
    int thread_id;
    long from;
    long job_size;
    long row_in_mx;
    int p;
    pthread_t thread;
  } pInfo;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void* thr_alloc_fn(void* arg);
  void* thr_access_fn(void* arg);
  void* thr_multiply_fn(void* arg);
  void* normal_alloc_fn(void* arg);
  void* normal_access_fn(void* arg);
  void* normal_multiply_fn(void* arg);
  void* alloc_fn(void* arg);
  void* access_fn(void* arg);
  void* multiply_fn(void* arg);
  void check_mx();
  void check_w();
  void construct_barrier(pInfo* pthreads, int n_threads);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse);
  void migrate_to_node(int node_index);


  int main(int argc, char* argv[]) {
    if (argc != 4) {
      printf("usage: ./bin/matrix_multiplication <rows> <cols> <with_numa>, exit...\n");
      printf("choose between -1, 0, 1, 2\n");
      exit(-1);
    }
    
    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else  {
      numa_set_strict(1);    
    }

    ROWS = atol(argv[1]);
    COLS = atol(argv[2]);
    with_numa = atoi(argv[3]);

    s = ROWS * COLS;
    printf("s = %ld\n", s);
    initMap(topology, topology_inverse);

    /**make sure the main thread is executed on a fixed cpu*/
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(0, &cpuset);
    main_thread = pthread_self();
    pthread_setaffinity_np(main_thread, sizeof(cpu_set_t), &cpuset);
    printf("fix main_thread: %lu on cpu0\n", main_thread);
    
    if (with_numa == -1) {
      auto started = std::chrono::high_resolution_clock::now();
      printf("===Allocate mx with %ld * %ld, WITHOUT NUMA awareness:\n", ROWS, COLS);
      mx = (float**)emalloc(sizeof(float*) * ROWS);
      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_THREADS);
      int err = 0;
      printf("allocate %ld by %ld matrix\n", ROWS, COLS);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        //      printf("thread_%d, is allocating mx[i] from %ld to %ld\n", pthreads[i].thread_id, pthreads[i].from, pthreads[i].from + pthreads[i].job_size);
        err = pthread_create(&pthreads[i].thread, NULL, normal_alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }

      construct_barrier(pthreads, NUM_THREADS);
      
      pthread_barrier_init(&b, NULL, NUM_THREADS+1);
      /**measure access time*/

      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, normal_access_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
      
      pthread_barrier_wait(&b);
      auto start = tick();      
      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      printf("normal parallel access matrix, use: %lld\n", end - start);
      
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }

      
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, normal_multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
      pthread_barrier_wait(&b);
      start = tick();
      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      printf("normal parallel multiplication use: %lld\n", end - start);

      for (int i = 0; i < ROWS; i++) {
        free(mx[i]);
      }
      free(pthreads);
      free(mx);
      free(v);
      free(w);
      pthread_barrier_destroy(&b);
      auto done = std::chrono::high_resolution_clock::now();
      cout << "total execution time: " << std::chrono::duration_cast<std::chrono::duration<double> >(done - started).count() << " seconds" << endl;
      exit(0);
    } else if (with_numa == 2) {
      auto started = std::chrono::high_resolution_clock::now();
      printf("===Allocate mx with %ld * %ld, WITH NUMA awareness:\n", ROWS, COLS);
      mx = (float**)emalloc(sizeof(float*) * ROWS);
      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_THREADS);
      int err = 0;
      printf("allocate %ld by %ld matrix\n", ROWS, COLS);

      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }
      construct_barrier(pthreads, NUM_THREADS);
      

      pthread_barrier_init(&b, NULL, NUM_THREADS+1);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, access_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
      pthread_barrier_wait(&b);
      auto start = tick();
      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      printf("parallel access matrix with shape ROWS * COLS, use: %lld\n", end - start);
      
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }

      for (int i = 0; i < NUM_THREADS; i++) {
        if (ROWS / NUM_THREADS == 0) {
          pthreads[i].from = i * 1;
          pthreads[i].job_size = 1;
        } else {
          pthreads[i].from  = i * (ROWS / NUM_THREADS);
          pthreads[i].job_size = ROWS / NUM_THREADS;
        }

        if (i == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }

      pthread_barrier_wait(&b);
      start = tick();
      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      printf("parallel multiplication with matrix shape ROWS * COLS, use: %lld\n", end - start);

      int cpu_num = sched_getcpu();
      if (cpu_num) {
        printf("main thread is not fixed on cpu0 \n");
      }

      for (long i = 0; i < ROWS; i++) {
        numa_free(mx[i], sizeof(float) * COLS);
      }
      free(mx);
      free(pthreads);
      free(v);
      free(w);
      pthread_barrier_destroy(&b);
      auto done = std::chrono::high_resolution_clock::now();
      cout << "total execution time: " << std::chrono::duration_cast<std::chrono::duration<double> >(done - started).count() << " seconds" << endl;
      exit(0);
    } else {
      auto started = std::chrono::high_resolution_clock::now();
      mx = (float**)emalloc(sizeof(float*) * NUM_NODES);

      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_NODES);
      pthread_t* thread; // array of pthread_t, for catching each created thread
      int err = 0;
      printf("===Allocate mx with %d * %ld, ", NUM_NODES, s / NUM_NODES);
      if (with_numa) {
        printf("WITH NUMA awareness\n");
      } else {
        printf("WITHOUT NUMA awareness\n");
      }
      /**Allocate m by n matrix with the shape NUM_NODES * some right size*/
      for (int i = 0; i < NUM_NODES; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].row_in_mx = i;
        pthreads[i].job_size = (s/NUM_NODES);
        if (pthreads[i].thread_id == NUM_NODES - 1) {
          pthreads[i].job_size += (s % NUM_NODES);
        }
        err = pthread_create(&pthreads[i].thread, NULL, thr_alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }

      /**manually created barrier*/
      construct_barrier(pthreads, NUM_NODES);
      //  check_mx();

      pthreads =(pInfo*)remalloc(pthreads, sizeof(*pthreads) * NUM_THREADS);
      
      pthread_barrier_init(&b, NULL, NUM_THREADS+1);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].row_in_mx = i / (NUM_THREADS / NUM_NODES);
        pthreads[i].p = i  - pthreads[i].row_in_mx * (NUM_THREADS / NUM_NODES);
        pthreads[i].job_size = s / NUM_THREADS;
        pthreads[i].from = pthreads[i].p * pthreads[i].job_size;
        if ((pthreads[i].p + 1) % (NUM_THREADS / NUM_NODES) == 0) {
          pthreads[i].job_size += ((s / NUM_NODES) % (NUM_THREADS / NUM_NODES));
        }
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size +=  (s % NUM_NODES);
        }

        err = pthread_create(&pthreads[i].thread, NULL, thr_access_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }

      pthread_barrier_wait(&b);
      auto start = tick();
      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      //  auto done = std::chrono::high_resolution_clock::now();
      printf("parallel access matrix, use: %lld\n", end - start);
      //  std::cout << "By c++, parallel access matrix, use: " << std::chrono::duration_cast<std::chrono::nanoseconds>(done-started).count() << endl;
      //  check_mx();


      /**do the multiplication with a vector*/
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }
    
    

      for (int i = 0; i < NUM_THREADS; i++) {
        if (ROWS / NUM_THREADS == 0) {
          pthreads[i].from = i * 1;
          pthreads[i].job_size = 1;
        } else {
          pthreads[i].from  = i * (ROWS / NUM_THREADS);
          pthreads[i].job_size = ROWS / NUM_THREADS;
        }

        if (i == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, thr_multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }

      pthread_barrier_wait(&b);
      start = tick();
      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      //  done = std::chrono::high_resolution_clock::now();
      printf("parallel multiplication, use: %lld\n", end - start);
      //  std::cout << "By c++, parallel multiplication, use: " << std::chrono::duration_cast<std::chrono::nanoseconds>(done-started).count() << endl;
      //  check_w();
    
      int cpu_num = sched_getcpu();
      if (cpu_num) {
        printf("main thread is not fixed on cpu0 \n");
      }
      for (int i = 0; i < NUM_NODES; i++) {
        if (i == NUM_NODES - 1) {
          numa_free(mx[i], (s / NUM_NODES + s % NUM_NODES) * sizeof(float));
        } else {
          numa_free(mx[i], (s / NUM_NODES) * sizeof(float));
        }
      }
      pthread_barrier_destroy(&b);
      free(mx);
      free(pthreads);
      free(v);
      free(w);
      auto done = std::chrono::high_resolution_clock::now();
      cout << "total execution time: " << std::chrono::duration_cast<std::chrono::duration<double> >(done - started).count() << " seconds" << endl;
      exit(0);
    } // end of else
  }

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  // thread function to use libnuma to allocate memory on specific node
  void* thr_alloc_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    //  printf("id: %d, thread: %lu is allocating %ld on node %d\n", info.thread_id, info.thread, info.job_size, info.row_in_mx);

    mx[info.thread_id] = (float*)numa_alloc_onnode(info.job_size * sizeof(float), info.row_in_mx);
    for (long k = 0; k < info.job_size; k++) {
      ,*(*(mx + info.row_in_mx) + k) = 0.0;
      //    mx[info.row_in_mx][k] = 0;
    }
    //  printf("thread %d  numa_alloc_onnode(%ld, %d), done\n", info.thread_id, info.job_size, info.row_in_mx);
    return (void *)0;
  }


  void* thr_access_fn(void* arg) {
    pid_t pid = getpid();
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];

    pInfo info = *(pInfo *)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      //    printf("thread_id = %d, on cpu %d, on node %d, should be on node%d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node, node_index);
      migrate_to_node(node_index);
      //    sched_cpu = sched_getcpu();
      //    current_on_node = topology[sched_cpu];
      //    printf("thread_id = %d, on cpu %d, on node %d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node);
    }

    pthread_barrier_wait(&b);
    //  printf("id: %d, thread: %lu, is accessing on cpu: %d, on node: %d\n", info.thread_id, info.thread, sched_cpu, current_on_node);
    for (long i = info.from; i < info.from + info.job_size; i++) {
      mx[info.row_in_mx][i] = 3.0;
    }
    return (void *)0;
  }

  void* thr_multiply_fn(void* arg) {
    pid_t pid = getpid();
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];

    pInfo info = *(pInfo*)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      //printf("thread_id = %d, on cpu %d, on node %d, should be on node%d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node, node_index);
      migrate_to_node(node_index);
    }
    long each_size = s / NUM_NODES;
    pthread_barrier_wait(&b);
    //  printf("id: %d, thread: %lu, is doing multiplication from cpu %d, on node %d\n", info.thread_id, info.thread, sched_cpu, current_on_node);
    for (long r = info.from; r < info.from  + info.job_size; ++r) {
      // get corresponding mx(i, j)
      for (long i = 0; i < COLS; i++) {
        long l = r * COLS + i;
        long row_in_mx = l / each_size - 1;
        if (row_in_mx < 0) {
          row_in_mx = 0;
        }
        long col_in_mx = l % each_size;
        w[r] += (*(*(mx + row_in_mx) + col_in_mx) * v[i]);
      }
    }

    return (void *)0;
  }

  void check_mx() {
    // check the content of mx
    long size_for_each_node = s / NUM_NODES;

    for(int i = 0; i < NUM_NODES; i++) {
      if (i == NUM_NODES -1) {
        size_for_each_node += s % NUM_NODES;
      }
      for (int j = 0; j < size_for_each_node; j++) {
        printf("%.f ", *(*(mx + i) + j));
      }
      printf("\n");
    }
  }

  void check_w() {
    printf("w = \n");
    for (long i = 0; i < ROWS; i++) {
      printf("%.f ", w[i]);
    }
    printf("\n");
  }


  void construct_barrier(pInfo* pthreads, int n_threads) {
    int err;
    void* status;
    for (int i = 0; i < n_threads; i++) {
      if (main_thread == pthreads[i].thread) {
        printf("one of the worker threads is main thread !!!\n\n\n");
      }
      err = pthread_join(pthreads[i].thread, &status);
      if (err) {
        printf("error, return code from pthread_join() is %d\n", *(int*)status);
      }
      //    printf("thread %d is joint\n", i);
    }
  }


  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse) {

    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }

  void* normal_alloc_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    for (long k = info.from; k < info.from + info.job_size; k++) {
      mx[k] = (float*)emalloc(sizeof(float) * COLS);
      for (long j = 0; j < COLS; j++){
        mx[k][j] = 0;
      }
    }
    //  printf("thread_%d, done alloc.\n", info.thread_id);
    return (void*) 0;
  }

  void* normal_access_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    pthread_barrier_wait(&b);
    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        mx[k][j] = 3;
      }
    }
  }


  void* normal_multiply_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    //  printf("thread_id = %d, from = %d, to = %d\n", info.thread_id, info.from, info.from + info.job_size);
    pthread_barrier_wait(&b);
    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        w[k] += (mx[k][j] * v[j]);
      }
    }
  }

  void* alloc_fn(void* arg) {
    pInfo info = *(pInfo *)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    
    for (long k = info.from; k < info.from + info.job_size; k++) {
      mx[k] = (float*)numa_alloc_onnode(sizeof(float) * COLS, node_index);
      for (long j = 0; j < COLS; j++){
        mx[k][j] = 0;
      }
    }
  }


  void* access_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      migrate_to_node(node_index);
    }

    pthread_barrier_wait(&b);
    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        mx[k][j] = 3;
      }
    }
    return (void *)0;
  }

  void* multiply_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];
    
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);

    if(with_numa && node_index != current_on_node) {
      migrate_to_node(node_index);
    }

    pthread_barrier_wait(&b);  
    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        w[k] += (mx[k][j] * v[j]);
        //      w[k] += (*(*(mx + k) +j) * v[j]);
      }
    }
    return (void *)0;
  }


  void migrate_to_node(int node_index) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    vector<int> cpus = topology_inverse[node_index];
    for (int i = 0; i < cpus.size(); i++) {
      CPU_SET(cpus[i], &cpuset);
    }
    pthread_t tid = pthread_self();
    pthread_setaffinity_np(tid, sizeof(cpu_set_t), &cpuset);
    //  int sched_cpu = sched_getcpu();
    //  int current_on_node = topology[sched_cpu];
    //  printf("thread_id = %d, on cpu %d, on node %d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node);
  }
#+END_SRC

*** Execution record
Fix main thread executing on CPU 0, matrix is 12800 * 12800
**** Allocate mx with 12800 * 12800, WITHOUT NUMA awareness
|   id |     access | multiplication | total execution time of program(s) |
|    1 |  430604149 |      935806714 |                            1.30271 |
|    2 |  428375815 |      937043212 |                            1.30204 |
|    3 |  429507786 |      936916666 |                            1.69266 |
|    4 |  428615820 |      937980358 |                            1.30382 |
|    5 |  428463239 |      937544182 |                            1.30078 |
| mean | 429113360. |     937058230. |                           1.380402 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)

**** Allocate mx with 8 * 20480000, WITHOUT NUMA awareness
|   id |     access | multiplication | total execution time of program(s) |
|    1 |  680044058 |     6681668916 |                            4.02796 |
|    2 |  682953049 |     6676504701 |                            4.02523 |
|    3 |  687956195 |     6677806353 |                            4.02215 |
|    4 |  685070622 |     6677521668 |                            4.01936 |
|    5 |  684318610 |     6677222508 |                            4.02701 |
| mean | 684068510. |    6678144800. |                           4.024342 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)

**** Allocate mx with 8 * 20480000, WITH NUMA awareness
|   id |   access | multiplication | total execution time of program(s) |
|    1 | 56052365 |      803617342 |                            1.19494 |
|    2 | 55887468 |      804099605 |                            1.20406 |
|    3 | 55422889 |      804668921 |                            1.19966 |
|    4 | 55422889 |      804668921 |                            1.19966 |
|    5 | 55915354 |      803988492 |                            1.19751 |
| mean | 55740193 |     804208660. |                           1.199166 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)

**** Allocate mx with 12800 * 12800, WITH NUMA awareness
|   id |     access | multiplication | total execution time of program |
|    1 |  162304794 |      153310561 |                         1.01839 |
|    2 |  162328640 |      148230344 |                         1.01127 |
|    3 |  162403576 |      153474228 |                         1.01854 |
|    4 |  161863161 |      154374771 |                         1.01873 |
|    5 |  161203875 |      153011596 |                         1.01798 |
| mean | 162020810. |      152480300 |                        1.016982 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)
*** Summary
  |--------------------------------------------------------+-------------+---------------------+----------------------------------|
  |                                                        | access time | multiplication time | total program execution time (s) |
  | Allocate mx with 12800 * 12800, WITHOUT NUMA awareness |   429113360 |           937058230 |                         1.380402 |
  | Allocate mx with 8 * 20480000, WITHOUT NUMA awareness  |   684068510 |          6678144800 |                         4.024342 |
  | Allocate mx with 8 * 20480000, WITH NUMA awareness     |    55740193 |           804208660 |                         1.199166 |
  | Allocate mx with 12800 * 12800, WITH NUMA awareness    |   162020810 |           152480300 |                         1.016982 |
  |--------------------------------------------------------+-------------+---------------------+----------------------------------|

  - Using pthread with NUMA awareness could improve the performance.
  - Allocate data structure according to NUMA nodes could improve the performance further in some cases. But it is not convonient to use it.
  - Since, we could just use normal way to allocate data, 
   
* Performance boost using OpenMP
** Code using openmp to do matrix time vector
#+BEGIN_SRC c
  #include <omp.h>
  #include <map>
  #include <pthread.h>
  #include <sys/types.h>
  #include <unistd.h>
  #include <numa.h>
  #include <sched.h>
  #include <vector>
  #include <chrono>
  #include <stdio.h>
  #include <iostream>

  using namespace std;

  long ROWS;
  long COLS;
  int NUM_THREADS = 64;
  int NUM_NODES = 8;
  pthread_t main_thread;
  int with_numa = 1;

  float** mx = nullptr;
  float* v = nullptr;
  float* w = nullptr;
  long s = 0;
  map<int, int> topology;
  map<int, vector<int> > topology_inverse;
  map<int, int> distribute_scheme;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse, map<int, int> &distribute_scheme);
  void check_w();

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  int main(int argc, char* argv[]) {
    if (argc != 4) {
      printf("usage: ./bin/xxx <rows> <cols> <1 or 0>, exit...\n");
      exit(-1);
    }
    
    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else  {
      numa_set_strict(1);    
    }

    printf("OMP_PLACES : %s\n", getenv("OMP_PLACES"));
    ROWS = atol(argv[1]);
    COLS = atol(argv[2]);
    with_numa = atoi(argv[3]);

    initMap(topology, topology_inverse, distribute_scheme);
    
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(0, &cpuset);
    main_thread = pthread_self();
    pthread_setaffinity_np(main_thread, sizeof(cpu_set_t), &cpuset);
    printf("fix main_thread: %lu on cpu0\n", main_thread);

    v = (float*)emalloc(sizeof(float) * COLS);
    w = (float*)emalloc(sizeof(float) * ROWS);
    mx = (float**)emalloc(sizeof(float*) * ROWS);

    long i = 0;
    long j = 0;

    for (j = 0; j < COLS; j++) {
      v[j] = 1;
    }
    for (i = 0; i < ROWS; i++) {
      w[i] = 0;
    }
    //  check_w();
    if (with_numa) {
      auto started = std::chrono::high_resolution_clock::now();
      printf("===WITH NUMA:\n");
      /**allocate mx among different nodes*/
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx, topology, ROWS, COLS, distribute_scheme) private(i, j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        if (topology[sched_cpu] != distribute_scheme[thread_id]) {
          printf("During allocation, threads are running wroing\n");
        }
  #pragma omp for ordered schedule (static)
        for(i = 0; i < ROWS; i++) {
          mx[i] = (float*)numa_alloc_onnode(sizeof(float) * COLS, 7);
        }
      }

      /**measure access time*/
      auto start = tick();
      printf("during access:\n");
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx,topology, ROWS, COLS, distribute_scheme) private(i,j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        if (topology[sched_cpu] != distribute_scheme[thread_id])  {
          printf("During access, threads are running wroing\n");
        }
  #pragma omp for schedule (static)
        for (i = 0; i < ROWS; i++) {
          for (j = 0; j < COLS; j++) {
            mx[i][j] = 3;
          }
        }
      }
      auto end = tick();
      printf("parallel access use: %lld\n", end - start);


      /**measure multiplication time*/
      start = tick();
      printf("during multiplication:\n");
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx, v, w, topology, ROWS, COLS, distribute_scheme) private(i, j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        if(topology[sched_cpu] != distribute_scheme[thread_id]) {
          printf("During multiplication, threads are running wrong\n");
        }
  #pragma omp for schedule (static)
        for (i = 0; i < ROWS; i++) {
          for(j = 0; j < COLS; j++) {
            w[i] += (mx[i][j] * v[j]);
          }
        }
      }
      if (sched_getcpu()) {
        printf("main thread is not fixed on CPU_01\n");
      }
      end = tick();
      printf("parallel matrix multiplication use: %lld\n", end - start);

      for(i = 0; i < ROWS; i++) {
        numa_free(mx[i], sizeof(float) * COLS);
      }
      free(mx);
      free(v);
      free(w);
      auto done = std::chrono::high_resolution_clock::now();
      cout << "total execution time: " << std::chrono::duration_cast<std::chrono::duration<double> >(done - started).count() << " seconds" << endl;
      exit(0);
    } else {


      auto started = std::chrono::high_resolution_clock::now();
      printf("===WITHOUT NUMA:\n");
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        mx[i] = (float*)emalloc(sizeof(float) * COLS);
      }

      auto start = tick();
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        for (j = 0; j < COLS; j++) {
          mx[i][j] = 3;
        }
      }
      auto end = tick();
      printf("parallel access use: %lld\n", end - start);

      start = tick();
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, v, w, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        for (j = 0; j < COLS; j++) {
          w[i] += (mx[i][j] * v[j]);
        }
      }
      end = tick();
      printf("parallel matrix multiplication use: %lld\n", end - start);

      for(i = 0; i < ROWS; i++) {
        free(mx[i]);
      }
      free(mx);
      free(v);
      free(w);
      auto done = std::chrono::high_resolution_clock::now();
      cout << "total execution time: " << std::chrono::duration_cast<std::chrono::duration<double> >(done - started).count() << " seconds" << endl;
      exit(0);
    }
  }



  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse, map<int, int> &distribute_scheme) {
    int thread_id = 0;
    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
        distribute_scheme.insert(make_pair(thread_id, node));
        thread_id += 1;
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }

  void check_w() {
    printf("w = \n");
    for (long k = 0; k < ROWS; k++) {
      printf("%.f ", w[k]);
    }
    printf("\n");
  }

#+END_SRC
** Experiments using OpenMP
*** Test if the OpenMP threads is generated according to NUMA node locality
- OpenMP in the parallel construct will generate 64 threads, the first 0~7 threads will allocate and access memory from NUMA node0, the next 8~15 threads will allocate and access memory from NUMA node1, and so on.
- According to the NUMA topology 
  | node 0 cpus: 0 4 8 12 16 20 24 28    |
  | node 1 cpus: 32 36 40 44 48 52 56 60 |
  | node 2 cpus: 2 6 10 14 18 22 26 30   |
  | node 3 cpus: 34 38 42 46 50 54 58 62 |
  | node 4 cpus: 3 7 11 15 19 23 27 31   |
  | node 5 cpus: 35 39 43 47 51 55 59 63 |
  | node 6 cpus: 1 5 9 13 17 21 25 29    |
  | node 7 cpus: 33 37 41 45 49 53 57 61 |
  |--------------------------------------|

  - I set the NUMA environment as:
    #+BEGIN_SRC sh
    export OMP_PLACES="{0}, {4}, {8}, {12}, {16}, {20}, {24}, {28}, {32}, {36}, {40}, {44}, {48}, {52}, {56}, {60}, {2}, {6}, {10}, {14}, {18}, {22}, {26}, {30}, {34}, {38}, {42}, {46}, {50}, {54}, {58}, {62}, {3}, {7}, {11}, {15}, {19}, {23}, {27}, {31}, {35}, {39}, {43}, {47}, {51}, {55}, {59}, {63}, {1}, {5}, {9}, {13}, {17}, {21}, {25}, {29}, {33}, {37}, {41}, {45}, {49}, {53}, {57}, {61}"
  #+END_SRC
  - Then use =#pragma omp parallel proc_bind(close)= to bind 64 threads according to =OMP_PLACES=.
  - During execution, use =omp_get_thread_num()= to get each thread_id, and use =sched_getcpu()= to get the current thread is executing on which CPU, to check if it is same as the predefine scheme.
  - After checking, each thread generated by OpenMP is executing according to the plan.
*** Use different compilier optimization level to see if the optimization is effect
|   id | access level0 | multiplication level0 | access level2 | multiplication level2 |
|    1 |     335230152 |              86450733 |     329282949 |              28731075 |
|    2 |     319493224 |              86753427 |     317015178 |              20604664 |
|    3 |     322924294 |              80855368 |     332274013 |              28996716 |
|    4 |     322924294 |              80855368 |     320535839 |              25889419 |
| mean |     325142991 |              83728724 |    324776990. |              26055469 |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)::@6$4=vmean(@2..@5)::@6$5=vmean(@2..@5)
From the comparision, we could see that the optimization level indeed take effect, especially on the multipcation case.

*** Compare NUMA vs non-NUMA using OpenMP
- both use optimization level 2
- time measurement
  |      id | non-NUMA access | non-NUMA multiplication | NUMA access | NUMA multiplication |
  |       1 |       262611629 |                25957307 |   319459787 |            27242749 |
  |       2 |       266202154 |                24971359 |   317124380 |            28645037 |
  |       3 |       269134031 |                24630409 |   308853972 |            26528531 |
  |       4 |       273664402 |                23336407 |   311153197 |            29279562 |
  |    mean |       267903054 |                24723871 |   314147834 |           27923970. |
  | pthread |       159383062 |               151926385 |             |                     |
  #+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)::@6$4=vmean(@2..@5)::@6$5=vmean(@2..@5)  label:openmp-performance-table-1

- summary 
  According to this measurement, you could found that NUMA version is worse than non-NUMA version. I think the reason to cause NUMA version is slower is because the OS has to schedule each thread on a specific CPU. That is a too strict to achieve NUMA locality since you actually need to schedule on a set of CPUs for one NUMA node instead of schedule on one particular CPU.

*** Change settings about =OMP_PLACES=
- Change =OMP_PLACES= to:
  #+BEGIN_SRC sh
    export OMP_PLACES="{0, 4, 8, 12, 16, 20, 24, 28}, {32, 36, 40, 44, 48, 52, 56, 60}, {2, 6, 10, 14, 18, 22, 26, 30}, {34, 38, 42, 46, 50, 54, 58, 62}, {3, 7, 11, 15, 19, 23, 27, 31}, {35, 39, 43, 47, 51, 55, 59, 63}, {1, 5, 9, 13, 17, 21, 25, 29}, {33, 37, 41, 45, 49, 53, 57, 61}"
  #+END_SRC

- time measurement, with optimization level = 2
  |   id | non-NUMA access | non-NUMA multiplication | NUMA access | NUMA multiplication |
  |    1 |       283806290 |                45075298 |   308910591 |            26020108 |
  |    2 |       292365042 |                66875479 |   313655646 |            27090225 |
  |    3 |       303580500 |                69254549 |   322728477 |            23495901 |
  |    4 |       280702041 |                39244008 |   329080558 |            28275037 |
  |    5 |       294252690 |                34725024 |   323499789 |            27878445 |
  |    6 |       279747220 |                37873401 |   341436966 |            30811872 |
  | mean |      289075630. |               48841293. |  323218670. |           27261931. |
  #+TBLFM: @8$2=vmean(@2..@7)::@8$3=vmean(@2..@7)::@8$4=vmean(@2..@7)::@8$5=vmean(@2..@7)

- summary
  - It seems my expectation for relaxing the schedule on set of CPUs instead of on one particular CPU doesn't work well as I expected. The reason I think is the OS scheduler could schedule multiple threads on the same CPU. So it actually make the performance worse than previous setting, for both NUMA and non-NUMA case.
  - Access time for non-NUMA is still faster than NUMA
  - Multiplication time for NUMA is better than non-NUMA, while it shows no advantage over non-NUMA while =OMP_PLACES= is configure to fix on each CPU.

*** Increase the size of data to make NUMA effect more important
- From previous experiments, it shows when =OMP_PLACES= is set to schedule on each CPU, the performance is better for both NUMA and non-NUMA; and non-NUMA is better than NUMA. In this experiment, continue to increase the size of data, to make NUMA effect more important to see OpenMP under NUMA awareness is whether better or worse comparing with non-NUMA.
- time measurement, 25600 * 25600, with optimization level 2
  |   id | non-NUMA access | non-NUMA multipcation | NUMA access | NUMA multipcation |
  |    1 |      1136666571 |              77452377 |  1124357282 |          78184265 |
  |    2 |      1036407809 |              85516505 |  1107078429 |          77917661 |
  |    3 |      1063054334 |              78347814 |  1101156621 |          79212596 |
  |    4 |      1035991932 |              77821935 |  1086195798 |          78019858 |
  |    5 |      1026371449 |              84138365 |  1071735039 |          78102814 |
  |    6 |      1030569386 |              79861620 |  1099029846 |          84898814 |
  | mean |     1054843600. |             80523103. | 1098258800. |         79389335. |
  #+TBLFM: @8$2=vmean(@2..@7)::@8$3=vmean(@2..@7)::@8$4=vmean(@2..@7)::@8$5=vmean(@2..@7)
- summary
  I expected the NUMA performance will be better than non-NUMA, but from the result, it doesn't shows an obvious advantage. There may be something wrong.

*** Configure OpenMP to access remote node to verfiy the NUMA awareness effect from negative point of view
- Since by configuring OpenMP with NUMA doesn't show advantage over non-NUMA configuration. I need to proof that from a negative point of view by configure NUMA access from remote nodes.
- Configure all memory allocate on NUMA node 7, and expected the performance become worse becuase 56/64 threads will access memory remotely
- time measurement, 12800 * 12800, with optimization level 2
  |   id | NUMA access | NUMA multiplication |
  |    1 |   306617492 |            32410022 |
  |    2 |   309534292 |            31684650 |
  |    3 |   298530840 |            24370234 |
  |    4 |   304718160 |            27047280 |
  |    5 |   312366607 |            27151744 |
  |    6 |   308791332 |            25503240 |
  |    7 |   308954756 |            26223124 |
  | mean |  307073350. |            27770042 |
  #+TBLFM: @9$2=vmean(@2..@8)::@9$3=vmean(@2..@8)

- summary
  compare it with table ref:openmp-performance-table-1, the result is similar...

** Performance comparision between pthread and OpenMP
*** Summary of Using OpenMP with NUMA
The result from fatest to slowest for multiplication time is:
1) the best general performance is under =OMP_PLACES= configured for each CPU, with non-NUMA awareness.
   Under such binding scheme, NUMA awareness make the performance become a little worse. I have tried to allocate all memory on NUMA node 7, which means around 56/64 threads will access memory remotely. That doesn't show performance becomes obviously worse and it is similar with the performance of accessing from NUMA locality.
2) When I configure =OMP_PLACES= to distribute threads across CPU set based on NUMA node, instead of fixing each thread on each CPU. The NUMA version is better than non-NUMA version. But it is useless to do so. Because its performance is still worse than non-NUMA version under previous =OMP_PLACES= configurations(fix each thread on each CPU).
3) OpenMP version's total execution time is worse than pthread version for all OpenMP configuration.
4) For access time, pthread version is faster than OpenMP version
5) For multiplication time, pthread version is much slower than OpenMP version. 
6) All use optimization level 2 and it has effect.

In general, I think OpenMP could combine with compiler to produce much efficient code for multiplication. pthread has little overhead and could be faster than OpenMP under NUMA consideration. If the task is much more complicated for each thread, then I think pthread version will win because I guess OpenMP and compiler will not be able to generate very efficient code.

*** Feed back from ZhiYi
- It is very unlikely that the compiler could generate such different code for doing multiplication between OpenMP and pthread.
- ask compiler to use FPU for pthread code, then do the test again
- What is FPU and how to set it so the compiler could use it?
  use =-mhard-float= compile option
- May have to go to assembly language to see the details
*** Measure the pthread execution time 
- before specify =-mhard-float=, means generate output containing floating point instructions, this is the default. 
  |   id |    access | multipcation |
  |    1 | 160274465 |    151846524 |
  |    2 | 157219317 |    151687159 |
  |    3 | 158095075 |    149357437 |
  |    4 | 158095075 |    149357437 |
  | mean | 158420983 |    150562140 |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)

- after specify =-mhard-float=
  |   id |    access | multiplication |
  |    1 | 161272758 |      158283874 |
  |    2 | 157998876 |      149994785 |
  |    3 | 156882903 |      154323816 |
  |    4 | 152328315 |      154846556 |
  | mean | 157120713 |      154362260 |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)
- summary, from the documentation, =-mhard-float= is indeed the default option, the performance doesn't show improvement.
  It seems I have to check the assembly code to figure out why the performance is so different between OpenMP and pthread
** Check assembly code for pthread and OpenMP parallel multiplication to exam why there is so much different on performance
*** Measure execution time for each thread
**** pthread
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/matrix_multiplication 12800 12800 2
  s = 163840000
  fix main_thread: 140666197047104 on cpu0
  ===Allocate mx with 12800 * 12800, WITH NUMA awareness:
  allocate 12800 by 12800 matrix
  parallel access matrix with shape ROWS * COLS, use: 158434856
  execution_time for thread_0 is: 25811707
  execution_time for thread_1 is: 39146625
  execution_time for thread_2 is: 38689711
  execution_time for thread_3 is: 18968286
  execution_time for thread_4 is: 42242188
  execution_time for thread_5 is: 20329806
  execution_time for thread_6 is: 20096659
  execution_time for thread_7 is: 142304394
  execution_time for thread_8 is: 44232895
  execution_time for thread_9 is: 43858278
  execution_time for thread_10 is: 61079477
  execution_time for thread_11 is: 42888566
  execution_time for thread_12 is: 37178390
  execution_time for thread_13 is: 43499010
  execution_time for thread_14 is: 44232278
  execution_time for thread_15 is: 59066298
  execution_time for thread_16 is: 36302153
  execution_time for thread_17 is: 34433373
  execution_time for thread_18 is: 33450582
  execution_time for thread_19 is: 36574305
  execution_time for thread_20 is: 35585001
  execution_time for thread_21 is: 56079130
  execution_time for thread_22 is: 31475274
  execution_time for thread_23 is: 29475119
  execution_time for thread_24 is: 51665701
  execution_time for thread_25 is: 73907170
  execution_time for thread_26 is: 60759266
  execution_time for thread_27 is: 66223746
  execution_time for thread_28 is: 84355017
  execution_time for thread_29 is: 48460394
  execution_time for thread_30 is: 63947405
  execution_time for thread_31 is: 89287052
  execution_time for thread_32 is: 59692195
  execution_time for thread_33 is: 56029106
  execution_time for thread_34 is: 42633811
  execution_time for thread_35 is: 42159121
  execution_time for thread_36 is: 44311916
  execution_time for thread_37 is: 44220044
  execution_time for thread_38 is: 34822759
  execution_time for thread_39 is: 56633364
  execution_time for thread_40 is: 89097401
  execution_time for thread_41 is: 86921945
  execution_time for thread_42 is: 72168548
  execution_time for thread_43 is: 89003931
  execution_time for thread_44 is: 88814848
  execution_time for thread_45 is: 45535213
  execution_time for thread_46 is: 89028603
  execution_time for thread_47 is: 71954688
  execution_time for thread_48 is: 37697252
  execution_time for thread_49 is: 43018927
  execution_time for thread_50 is: 36570570
  execution_time for thread_51 is: 55947573
  execution_time for thread_52 is: 43071152
  execution_time for thread_53 is: 49220455
  execution_time for thread_54 is: 38225214
  execution_time for thread_55 is: 37081349
  execution_time for thread_56 is: 56987272
  execution_time for thread_57 is: 86720271
  execution_time for thread_58 is: 84175740
  execution_time for thread_59 is: 86107602
  execution_time for thread_60 is: 86571010
  execution_time for thread_61 is: 80477377
  execution_time for thread_62 is: 67635127
  execution_time for thread_63 is: 67629714
  the max_execution time is from thread_7, it is: 142304394
  parallel multiplication with matrix shape ROWS * COLS, use: 142304394
#+END_SRC
**** OpenMP
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/openmp_matrix_multiplication 12800 12800 1
  OMP_PLACES : {0}, {4}, {8}, {12}, {16}, {20}, {24}, {28}, {32}, {36}, {40}, {44}, {48}, {52}, {56}, {60}, {2}, {6}, {10}, {14}, {18}, {22}, {26}, {30}, {34}, {38}, {42}, {46}, {50}, {54}, {58}, {62}, {3}, {7}, {11}, {15}, {19}, {23}, {27}, {31}, {35}, {39}, {43}, {47}, {51}, {55}, {59}, {63}, {1}, {5}, {9}, {13}, {17}, {21}, {25}, {29}, {33}, {37}, {41}, {45}, {49}, {53}, {57}, {61}
  fix main_thread: 140272493819840 on cpu0
  ===WITH NUMA:
  during access:
  parallel access use: 347065654
  during multiplication:
  execution_time for thread_0 is: 23263891
  execution_time for thread_1 is: 23250446
  execution_time for thread_2 is: 23245685
  execution_time for thread_3 is: 23245589
  execution_time for thread_4 is: 23244663
  execution_time for thread_5 is: 23243710
  execution_time for thread_6 is: 23245227
  execution_time for thread_7 is: 23245203
  execution_time for thread_8 is: 23245845
  execution_time for thread_9 is: 23245991
  execution_time for thread_10 is: 23251517
  execution_time for thread_11 is: 23251514
  execution_time for thread_12 is: 23245732
  execution_time for thread_13 is: 23245763
  execution_time for thread_14 is: 23253933
  execution_time for thread_15 is: 23253406
  execution_time for thread_16 is: 23242764
  execution_time for thread_17 is: 23242905
  execution_time for thread_18 is: 23246030
  execution_time for thread_19 is: 23245999
  execution_time for thread_20 is: 23247785
  execution_time for thread_21 is: 23246862
  execution_time for thread_22 is: 23239579
  execution_time for thread_23 is: 23239548
  execution_time for thread_24 is: 23217726
  execution_time for thread_25 is: 23217892
  execution_time for thread_26 is: 23216667
  execution_time for thread_27 is: 23216566
  execution_time for thread_28 is: 23217630
  execution_time for thread_29 is: 23217779
  execution_time for thread_30 is: 23216721
  execution_time for thread_31 is: 23222684
  execution_time for thread_32 is: 23245449
  execution_time for thread_33 is: 23245416
  execution_time for thread_34 is: 23242869
  execution_time for thread_35 is: 23242696
  execution_time for thread_36 is: 23244685
  execution_time for thread_37 is: 23242141
  execution_time for thread_38 is: 23251539
  execution_time for thread_39 is: 23252905
  execution_time for thread_40 is: 23229603
  execution_time for thread_41 is: 23229627
  execution_time for thread_42 is: 23230670
  execution_time for thread_43 is: 23230664
  execution_time for thread_44 is: 23228910
  execution_time for thread_45 is: 23228870
  execution_time for thread_46 is: 23230183
  execution_time for thread_47 is: 23230243
  execution_time for thread_48 is: 23243949
  execution_time for thread_49 is: 23240454
  execution_time for thread_50 is: 22956393
  execution_time for thread_51 is: 23240093
  execution_time for thread_52 is: 23245480
  execution_time for thread_53 is: 23245488
  execution_time for thread_54 is: 23236365
  execution_time for thread_55 is: 23236345
  execution_time for thread_56 is: 23135988
  execution_time for thread_57 is: 23135847
  execution_time for thread_58 is: 23138779
  execution_time for thread_59 is: 23138663
  execution_time for thread_60 is: 23128408
  execution_time for thread_61 is: 23139349
  execution_time for thread_62 is: 23137153
  execution_time for thread_63 is: 23137047
  the max execution time is in thread: 0, use: 23263891
  parallel matrix multiplication use: 23263891
  total execution time: 4.29773 seconds

#+END_SRC
*** generate assembly code for OpenMP code
**** command line
  #+BEGIN_SRC sh
    /home/wzhao/gcc-4.9.4/bin/g++ \
        -I/usr/local/include/ \
        -I/usr/include \
        -I/home/wzhao/gcc-4.9.4/include/c++/4.9.4/ \
        -L/home/wzhao/gcc-4.9.4/lib64 \
        -lnuma -lgomp \
        -fopenmp -O2 \
        -std=gnu++11 openmp_matrix_multiplication.cpp -S
  #+END_SRC
**** code block of assembly
#+BEGIN_SRC asm
          521         rdtsc
          522 # 0 "" 2
          523 #NO_APP
          524         salq    $32, %rdx
          525         movl    %eax, %eax
          526         movq    %rdx, %rbx
          527         orq     %rax, %rbx
          528         call    omp_get_num_threads
          529         movl    %eax, %ecx
          530         movl    %r12d, %eax
          531         cltd
          532         idivl   %ecx
          533         cmpl    %edx, %ebp
          534         jge     .L77
          535         addl    $1, %eax
          536         xorl    %edx, %edx
          537 .L77:
          538         movl    %eax, %ecx
          539         imull   %ebp, %ecx
          540         addl    %edx, %ecx
          541         addl    %ecx, %eax
          542         cmpl    %eax, %ecx
          543         jge     .L72
          544         movq    w(%rip), %rdi
          545         subl    %ecx, %eax
          546         movq    COLS(%rip), %rsi
          547         movslq  %ecx, %rcx
          548         movq    v(%rip), %rdx
          549         subl    $1, %eax
          550         addq    $1, %rax
          551         leaq    (%rdi,%rcx,4), %r8
          552         movq    mx(%rip), %rdi
          553         leaq    0(,%rsi,4), %r10
          554         leaq    (%rdi,%rcx,8), %r11
          555         xorl    %edi, %edi
          556         .p2align 4,,10
          557         .p2align 3
          558 .L74:
          559         testq   %rsi, %rsi
          560         jle     .L76
          561         movq    (%r11,%rdi,8), %r9
          562         movss   (%r8,%rdi,4), %xmm1
          563         xorl    %ecx, %ecx
          564         .p2align 4,,10
          565         .p2align 3
          566 .L75:
          567         movss   (%r9,%rcx), %xmm0
          568         mulss   (%rdx,%rcx), %xmm0
          569         addq    $4, %rcx
          570         cmpq    %r10, %rcx
          571         addss   %xmm0, %xmm1
          572         movss   %xmm1, (%r8,%rdi,4)
          573         jne     .L75
          574 .L76:
          575         addq    $1, %rdi
          576         cmpq    %rax, %rdi
          577         jne     .L74
          578 .L72:
          579         call    GOMP_barrier
          580         movl    $16, %ecx
          581         addq    $24, %rsp
          582         .cfi_remember_state
          583         .cfi_def_cfa_offset 40
          584         movl    %ebp, %esi
          585 #APP
          586 # 37 "openmp_matrix_multiplication.cpp" 1
          587         rdtsc

#+END_SRC

*** generate assembly code for pthread code
**** command line
#+BEGIN_SRC sh
  /home/wzhao/gcc-4.9.4/bin/g++ \
      -I/usr/local/include/ \
      -I/usr/include \
      -I/home/wzhao/gcc-4.9.4/include/c++/4.9.4/ \
      -L/home/wzhao/gcc-4.9.4/lib64 \
      -lnuma -lpthread \
      -pthread -mhard-float -O2 \
      -std=gnu++11 matrix_multiplication.cpp -S
#+END_SRC
**** code block of assembly
#+BEGIN_SRC asm
           828         rdtsc
           829 # 0 "" 2
           830 #NO_APP
           831         salq    $32, %rdx
           832         movl    %eax, %eax
           833         orq     %rax, %rdx
           834         cmpq    %r9, %rbx
           835         movq    %rdx, %r11
           836         jge     .L107
           837         movq    COLS(%rip), %rsi
           838         movq    w(%rip), %rdi
           839         movq    mx(%rip), %r10
           840         movq    v(%rip), %rax
           841         .p2align 4,,10
           842         .p2align 3
           843 .L108:
           844         testq   %rsi, %rsi
           845         jle     .L110
           846         movq    (%r10,%rbx,8), %r8
           847         movss   (%rdi,%rbx,4), %xmm1
           848         xorl    %ecx, %ecx
           849         .p2align 4,,10
           850         .p2align 3
           851 .L109:
           852         movss   (%r8,%rcx,4), %xmm0
           853         mulss   (%rax,%rcx,4), %xmm0
           854         addq    $1, %rcx
           855         cmpq    %rsi, %rcx
           856         addss   %xmm0, %xmm1
           857         movss   %xmm1, (%rdi,%rbx,4)
           858         jne     .L109
           859 .L110:
           860         addq    $1, %rbx
           861         cmpq    %r9, %rbx
           862         jne     .L108
           863 .L107:
           864         movl    $16, %ecx
           865 #APP
           866 # 37 "matrix_multiplication.cpp" 1
           867         rdtsc

#+END_SRC
* Troubleshooting
** GCC version is 4.8 not enough to support OpenMP 4.0 for proc_bind functionality
- download gcc7.2 
  wget [[http://ftp.tsukuba.wide.ad.jp/software/gcc/releases/gcc-7.2.0/gcc-7.2.0.tar.gz][gcc7.2.0]]
- tar zxvf and cd into directory
- =./contrib/download_prerequisites=
- cd ..
- mkdir objdir
- cd objdir
- =$PWD/../gcc-7.2.0/configure --prefix=$HOME/gcc-7.2.0 --disable-multilib=
  - =disable-multilib= means only support 64-bits version.
- make 
- make install 

Referenced from [[https://gcc.gnu.org/wiki/InstallingGCC][install gcc]].

** Error during executing compiled program 
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 8 8 8
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by ./bin/openmp_with_numa)
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libgomp.so.1: version `GOMP_4.0' not found (required by ./bin/openmp_with_numa)
#+END_SRC
- reason: the program is using the system default OpenMP
- solution:
  1) find the new OpenMP library
     #+BEGIN_SRC sh
        wzhao@r815:~/numa-knn$ find /usr find / name -name libgomp.so.1
        /usr/lib/x86_64-linux-gnu/libgomp.so.1
        /usr/local/lib32/libgomp.so.1
        /usr/local/lib64/libgomp.so.1
     #+END_SRC
  2) set the path to newer version in =LD_LIBRARY_PATH=.
     =export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/wzhao/gcc-4.9.4/lib64/=

** Execution record using OpenMP 
*** Experiment show different execution time on matrix time vector code for 
  1) aware of NUMA
     parallel 32 threads on 4 node, allocate memory on those 4 nodes
  2) unawere of NUMA
     parallel 32 threads

|   Index | unaware of NUMA | aware of NUMA |
|       1 | 12580ms         | 14579ms       |
|       2 | 13036ms         | 14709ms       |
|       3 | 12646ms         | 14922ms       |
| average | 12754 ms        | 14736.667 ms  |

#+TBLFM: @5$2=vmean(@2..@4)::@5$3=vmean(@2..@4
- the reason why NUMA aware version is worse than not using NUMA is the vector is allocated on a specific node, so even each row of matrix is allocated according to memory affinity, it is slower during matrix * vector. Because for most of the row, the memory on vector is on remote node.
- I need to allocate vector's memory according to memory affinity.

*** Use vector<float*> instead of vector<float>  to use libnuma to allocate sizeof(float) memory on specific node
table shows the experiment result, unit is ms.
- column 2 shows case of allocation according to memory affinity
- column 3 shows case of allocate vector<float*> on node 7
|   index | numa_openmp | anti_numa_openmp | openmp_without_numa |
|---------+-------------+------------------+---------------------|
|       1 |        1010 |             1023 |                 516 |
|       2 |        1014 |             1009 |                 523 |
|       3 |        1015 |             1010 |                 522 |
|       4 |        1013 |             1024 |                 523 |
|       5 |        1016 |             1008 |                 543 |
| Average |      1013.6 |           1014.8 |               525.4 |

#+TBLFM: $3=vmean(@2..@6)::@7$2=vmean(@2..@6)::@7$4=vmean(@2..@6)
- code block of using vector<float*> to represent vector and use it to compute matrix vector multiplication
#+BEGIN_SRC c
  vector<float*> a;
  vector<float*> b;
  vector<float*> c;


  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(m, n, a, b, c, topology) private(i, j, eachRow, a_each, c_each)
  {
    int thread_num = omp_get_thread_num();
    int cpu_num = sched_getcpu();

    printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);
      
  #pragma omp for ordered schedule (static)
    for (i = 0; i < m; i++) {

      eachRow = NULL;
      a_each = NULL;
      c_each = NULL;

      cpu_num = sched_getcpu();
      int which_node = topology[cpu_num];

      eachRow = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
      a_each = (float*)numa_alloc_onnode(sizeof(float), which_node);
      c_each = (float*)numa_alloc_onnode(sizeof(float), which_node);

      if (eachRow == NULL || a_each == NULL || c_each == NULL) {
        printf("error during allocation numa memory on node %d\n", cpu_num);
        exit(-1);
      }

      a_each[0] = 0.0;
      c_each[0] = 2.0;
      for (j = 0; j < n; j++) {
        eachRow[j] = 2.0;
      }

  #pragma omp ordered
      a.push_back(a_each);
  #pragma omp ordered
      b.push_back(eachRow);
  #pragma omp ordered
      c.push_back(c_each);
    }


    printf("Check if the thread is paralleled as planed\n");
    auto started = std::chrono::high_resolution_clock::now();
  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n, topology) private(i, j)
    {
      int thread_num = omp_get_thread_num();
      int cpu_num = sched_getcpu();

      printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);

  #pragma omp for schedule (static)
      for (i = 0; i < m; i++) {
        for (j = 0; j < n; j++) {
          a[i][0] += (b[i][j] * (*c[i]));
        }
      }  
    }

    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
#+END_SRC

*** To fully control the code behaviour, use C **float instead of C++ vector<float*>
- pointer as array, the following pairs of execution has the same effect
#+BEGIN_SRC c
  printf("the address of a is %ld\n", a);
  printf("*a = %ld\n", *a);
  printf("a[0] = %ld\n", a[0]);

  printf("*(a+1) = %ld\n", *(a+1));
  printf("a[1] = %ld\n", a[1]);
    
  printf("*(*(a+i) + j) = %f\n", *(*(a+1) + 0));
  printf("a[i][j] = %f\n", a[1][0]);

#+END_SRC

- code block which allocate memory, a, b, c are 2 dimensional array, as matrix
  #+BEGIN_SRC c
    float** a;
    float** b;
    float** c;
      
    b = (float**) emalloc(m * sizeof(float*));
    c = (float**) emalloc(m * sizeof(float*));
    a = (float**) emalloc(m * sizeof(float*));
  #+END_SRC

- code block which allocates numa memory on specific node
#+BEGIN_SRC c
  #pragma omp for ordered schedule (static)
  for (i = 0; i < m; i++) {

    cpu_num = sched_getcpu();
    int which_node = topology[cpu_num];
        
    b[i] = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
    a[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);
    c[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);

    if (b[i] == NULL || a[i] == NULL || c[i] == NULL) {
      printf("error during allocation numa memory on node %d\n", cpu_num);
      exit(-1);
    }

    a[i][0] = 0.0;
    c[i][0] = 2.0;
    for (j = 0; j < n; j++) {
      b[i][j] = 2.0;
    }
   }    
  }
#+END_SRC

- code block which does execution of matrix time vector and messures time
 #+BEGIN_SRC c
   #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n) private(i, j)
     {
   #pragma omp for schedule (static)
       for (i = 0; i < m; i++) {
         for (j = 0; j < n; j++) {
           ,*(*(a+i) + 0) += ( *(*(b+i) +j) * (*(*(c +j) +0)));
           //      a[i][0] += (b[i][j] * c[j][0]);
         }
       }  
     }

   auto done = std::chrono::high_resolution_clock::now();
   std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
 #+END_SRC

**** time mesurement 
#+BEGIN_SRC sh
  cat numa_openmp.sh
  export OMP_PLACES="{0, 4, 8, 12, 16, 20, 24, 28}, {32, 36, 40, 44, 48, 52, 56, 60}, {2, 6, 10, 14, 18, 22, 26, 30}, {34, 38, 42, 46, 50, 54, 58, 62}, {3, 7, 11, 15, 19, 23, 27, 31}, {35, 39, 43, 47, 51, 55, 59, 63}, {1, 5, 9, 13, 17, 21, 25, 29}, {33, 37, 41, 45, 49, 53, 57, 61}"
  ./bin/numa_openmp 16000 16000 8
  ./bin/anti_numa_openmp 16000 16000 8
  ./bin/openmp_without_numa 16000 16000 8
#+END_SRC
|   id | use libnuma(ms) | not use libnuma(ms) | use libnuma on remote node |
|    1 |            3880 |                 420 |                       3934 |
|    2 |            4034 |                 416 |                       3973 |
|    3 |            4020 |                 418 |                       3960 |
|    4 |            3975 |                 419 |                       3951 |
|    5 |            4033 |                 413 |                       3908 |
| mean |          3988.4 |               417.2 |                     3945.2 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)
- It seems this approach make the libnuma case becomes worse.
- The execution time of using libnuma and the execution time of using libnuma but allocated memory deliberately on remote node is also similar. This indicate I have lot of remote memory access cases.
- This whole program is wrong because each row of the matrix need to multiple very element of vector, there is no point of allocation vector across multiple nodes.

*** OpenMP is not a good choice for doing Matrix times Matrix with NUMA Awareness
**** The initial basic idea of using OpenMP with NUMA
- OpenMP is doing data parallelisation via =for= loop. OpenMP distributes the data across different threads and each thread works on one chunck of data and in the end merge the result.
- NUMA is about memory affinity. If the thread and the data it is accessing is located on the same node, then the accessing speed will be faster than remote accessing.
- Using OpenMP data parallelisation with NUMA awareness. My initial plan is:
  1) first configure OpenMP environment to control where and how OpenMP generate thread based on the =for= loop.
     This is the so called =proc_bind=, such as make sure generated: thread 1 is running on cpu01
     thread 2 is running on cpu02
  2) Based on NUMA topology, such cpu01 is on node01, cpu02 is on node02. Allocate data's memory among different threads, so data's memory is distributed allocated by different threads, which in turn allocated on different cpu ==> on different nodes.
  3) When I do the computation using data. I make sure use the same OpenMP parallel schema, such that the generate threads are as same as before. Thus, each thread will be able to read its part of data locally since the corresponding part of data memory has been allocated on the right node already.
  4) This idea need to make sure the size of different data matches each other.
- Case study why this doesn't work, on matrix time matrix:
  - suppose matrix m which is 2 * 5 , times matrix n which is 5 * 2.

  - allocate memory using OpenMP, in the loop, allocate each row of m on different threads while allocate each column of n on the same different threads. Then initalize each element.
  - For the result matrix w which 2 * 2. Then it has the following problem:
    | 00 | 01 |
    |----+----|
    | 10 | 11 | 
    - if w is allocated row by row, which means 00, 01 is on node0, 10 and 11 is on node1. Then during computation, remote access will happend on 01(on node0) since it is the result of the first row of m which is on node0 times the second column of n which is on node 1. Similar thing happened on 10.
    - In fact, only left to right diagonal element which is refered in double loop as i == j can avoid remote access.

In general, data parallelisation in the case of matrix time matrix could not be computed efficiently using OpenMP with NUMA awareness configuration. 

  Data parallelisation with NUMA awareness could be hard since any interaction part in the parallel region will require access from remote nodes(because you generate threads on different CPUs => different nodes). Thus, the result will be similar with averagely allocation memory across different nodes which is the default setting for NUMA. The more frequently the interaction is, the less effect of NUMA allocation will be. It is not about how to allocate memory with NUMA, it is about the interaction between different parts of data which is inherited from the algorithm.
  
  So, a simple "solution" will be make multiple copy of data and let each thread works on its own corresponding copy. That is a kind of  task parallelism and I doult about the practical usage of it. 
** Error from using pthread
- result for check the matrix which is allocated according to the topology of NUMA nodes:
#+BEGIN_SRC sh
  pid = 60287, tid = 123145488523264, for i = 0
  pid = 60287, tid = 123145489059840, for i = 1
  pid = 60287, tid = 123145489596416, for i = 2
  pid = 60287, tid = 123145490132992, for i = 3
  pid = 60287, tid = 123145490669568, for i = 4
  pid = 60287, tid = 123145491206144, for i = 5
  pid = 60287, tid = 123145491742720, for i = 6
  pid = 60287, tid = 123145492279296, for i = 7
  for thread = 7, it gets extra job, size_for_each_node = 14
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 0 0 0 -76060119211228138797542729056256 0 
#+END_SRC
