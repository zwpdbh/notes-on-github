* Performance monitor on AMD


* Performance boost using OpenMP
** code for Matrix times Vector
#+BEGIN_SRC c++
  #include <omp.h>
  #include <iostream>
  #include <chrono>
  #include <vector>

  using namespace std;

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }


  int main(int argc, char* argv[]) {
    if (argc < 3) {
      cout << "Please specify m n on command line" << endl;
      exit(-1);
    }
    long m, n;
    
    m = stol(argv[1]);
    n = stol(argv[2]);
    int n_cpu = stoi(argv[3]);
    
    
    float* a = (float*)emalloc(n * sizeof(float));
    vector<float*> b;
    float* c = (float*)emalloc(n * sizeof(float));
    
    long i, j;
    
    auto started = std::chrono::high_resolution_clock::now();
    
    float* eachRow;
    
    
    for (i = 0; i < m; i++) {
      eachRow = (float*)emalloc(n * sizeof(float));
      for (j = 0; j < n; j++) {
        eachRow[j] = i;
      }
      b.push_back(eachRow);
    }
    
    
  #pragma omp parallel default(none) shared(c, n) private(i)
    {
  #pragma omp for
      for (i = 0; i < n; i++) {
        c[i] = 2.0;
      }
    }
    
  #pragma omp parallel default(none) shared(a,b,c,m,n) private(i, j) 
    {
      
  #pragma omp for
      for (i = 0; i < m; i++) {
        a[i] = 0.0;
        //    cout << "Thread " << omp_get_thread_num() << " is executing" << endl;
        for (j = 0; j < n; j++) {
          a[i] += (b[i][j] * c[j]);
        }
      }  
    }
    
    for (i = 0; i < m; i++) {
      free(b[i]);
    }
    
    
    free(c);
    free(a);
    
    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
    
    return 0;
  }
#+END_SRC
** executing result comparing between using OpenMP and without using OpenMP
- The time of executeing the program with OpenMP suppport is roughly half of the time of executing the program without OpenMP.
  Serial code execution records:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2655ms
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2384ms
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2589ms
  #+END_SRC

  Simple parallel version execution time
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1436ms
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1289ms
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1278ms
  #+END_SRC

- use =/usr/bin/time= to examine the detail of execution on both version
  - for using OpenMP
    #+BEGIN_SRC sh
      wzhao@r815:~/numa-knn$ /usr/bin/time -v ./bin/openmp_with_numa 10000 10000
      From initialization to finished, use: 1246ms
      Command being timed: "./bin/openmp_with_numa 10000 10000"
      User time (seconds): 4.41
      System time (seconds): 0.76
      Percent of CPU this job got: 399%
      Elapsed (wall clock) time (h:mm:ss or m:ss): 0:01.29
      Average shared text size (kbytes): 0
      Average unshared data size (kbytes): 0
      Average stack size (kbytes): 0
      Average total size (kbytes): 0
      Maximum resident set size (kbytes): 1570640
      Average resident set size (kbytes): 0
      Major (requiring I/O) page faults: 0
      Minor (reclaiming a frame) page faults: 98332
      Voluntary context switches: 119
      Involuntary context switches: 623
      Swaps: 0
      File system inputs: 0
      File system outputs: 0
      Socket messages sent: 0
      Socket messages received: 0
      Signals delivered: 0
      Page size (bytes): 4096
      Exit status: 0

    #+END_SRC

  - for not using OpenMP
    #+BEGIN_SRC sh
      wzhao@r815:~/numa-knn$ /usr/bin/time -v ./bin/no_openmp_with_numa 10000 10000
      From initialization to finished, use: 2592ms
      Command being timed: "./bin/no_openmp_with_numa 10000 10000"
      User time (seconds): 2.38
      System time (seconds): 0.24
      Percent of CPU this job got: 99%
      Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.63
      Average shared text size (kbytes): 0
      Average unshared data size (kbytes): 0
      Average stack size (kbytes): 0
      Average total size (kbytes): 0
      Maximum resident set size (kbytes): 1568208
      Average resident set size (kbytes): 0
      Major (requiring I/O) page faults: 0
      Minor (reclaiming a frame) page faults: 98110
      Voluntary context switches: 2
      Involuntary context switches: 264
      Swaps: 0
      File system inputs: 0
      File system outputs: 0
      Socket messages sent: 0
      Socket messages received: 0
      Signals delivered: 0
      Page size (bytes): 4096
      Exit status: 0
    #+END_SRC

** The result is not very impressive, further investigation.
- When I use set number of threads to 32, it is better than setting it to 64 which is the Maximum number of cpu on the machine.

** Question
- " From the software point of view, this remote memory can be used in the same way as local memory; it is fully cache coherent." What is cache coherent?
- The following code could compile but during execution, it produces memory error messages
  #+BEGIN_SRC c
    #pragma omp parallel default(none) shared(m, n, b) private(eachRow, i, j)
      {
    #pragma omp for
        for (i = 0; i < m; i++) {
          eachRow = (float*)emalloc(n * sizeof(float));
          for (j = 0; j < n; j++) {
            eachRow[j] = i;
          }
          b.push_back(eachRow);
        }
      }  
  #+END_SRC
- On SMP system, there is a common optimization called cache affinity that is a bit similar. Cache affinity tries to keep data in the cache of a CPU instead of frequently ìbouncingî it between processors. This is commonly done by a scheduler in the operating system, which tries to keep threads on a CPU for some time before scheduling it on another. However, there is an important difference from node affinity: When a thread on an SMP system moves between CPUs, its cache contents eventually move with it. Once a memory area is committed to a specific node on a NUMA system, it stays there. A thread running on a different node that accesses it always adds traffic to the interconnect and leads to higher latency. This is why NUMA systems need to try harder to archive node affinity than SMP systems.

  Why "When a thread on an SMP system moves between CPUs, its cache contents eventually move with it." ? How?

** NUMA API
*** Introduction
- On an SMP(symmetric multiprocessing) system, all CPUs have equal access to the same shared memory controller that connects to all the memory chips (DIMMs). Communication between the CPUs also goes through this shared resource, which can become congested.
- The number of memory chips that can be managed by the single controller is also limited, which limit how much memory can be suppported by the system. In addition, the latency to access memory through this single traffic hub is relative high.
- The NUMA architecture was designed to surpass the scalability limits of the SMP architecture. Instead of having a single memory controller per computer, the system is split into multiple nodes.
  - Each node has processors and its own memory.
  - The processors have very fast access to the local memory in the node.
  - All the nodes in the system are connected using a fast interconnect.
- NUMA policy is concerned with putting memory allocations on specific nodes to let programs access them as quickly as possible.
  *The primary way to do this is to allocate memory for a thread on its local node and keep the thread running there (node affinity)*
  1) plan your thread execution, for which thread executing on which CPU 
  2) allocate memory ahead based on the node position of that CPU
- In addition to =numactl= library, =numastat= collect statistics about the memory allocation and =numademo= to show the effect of different policies on the system.

*** Policies
- Policies can be set process or per memory region.
  - Policies set per memory region, also called VMA policies3, allow a process to set a policy for a block of memory in its address space. Memory region policies have a higher priority than the process policy.

*** libnuma
- =nodemask_t= is a fixed size bit set of node numbers.
  #+BEGIN_SRC c
    nodemask_set(&mask, maxnode); /* set node highest */
    if (nodemask_isset(&mask, 1)) { /* is node 1 set? */
      ...
     }
    nodemask_clr(&mask, maxnode); /* clear highest node again */

  #+END_SRC
  
- allocation memory on node/set of nodes
  1) =void *numa_alloc_onnode(size_t size, int node);=
  2) =void *numa_alloc_interleaved_subset(size_t size, struct bitmask *nodemask);=

- libnuma process policy
  When existing code in a program cannot be modified to use the numa_alloc functions directly, it is sometimes useful to change the process policy in a program. This way, specific subfunctions can be run with a nondefault policy without actually modifying their code.
  - Each thread has a default memory policy inherited from its parent. Unless changed with numactl, this policy is normally used to allocate memory preferably on the current node.
  - =numa_set_interleave_mask= enables interleaving for the current thread. All future memory allocations allocate memory round robin interleaved over the nodemask specified.
  - Process policy can also be used to set a policy for a child process before starting it.
  - =numa_bind()= binds the current task and its children to the nodes specified in nodemask. They will only run on the CPUs of the specified nodes and only be able to allocate memory from them. This function is equivalent to calling =numa_run_on_node_mask(nodemask)= followed by =numa_set_membind(nodemask)=.
    - =numa_run_on_node_mask()= runs the current task and its children only on nodes specified in =nodemask=. They will not migrate to CPUs of other nodes until the node affinity is reset with a new call to =numa_run_on_node_mask()=. Passing =numa_all_nodes= permits the kernel to schedule on all nodes again.
    - =void numa_set_membind(struct bitmask *nodemask);= sets the memory allocation mask. The task will only allocate memory from the nodes set in nodemask.

- Changing the policy of existing memory areas
  When working with shared memory, it is often not possible to use the numa_alloc family of functions to allocate memory. The memory must be gotten from shmat() or mmap instead. To allow libnuma programs to set policy on such areas, there are additional functions for setting memory policy for already existing memory areas.
  These function only affect future allocation in the specified area.

- Except allocating memory on specific nodes, another part of NUMA policy is to run the thread on the CPUs of the correct node.
  A simple way to use libnuma is the =numa_bind= function. It binds both the CPU and the memory of the process allocated in the future to a specific nodemask.
  - Example of binding process CPU and memory allocation to node 1 using numa_bind:
    #+BEGIN_SRC c
      nodemask_t mask;
      nodemask_zero(&mask);
      nodemask_set(&mask 1);
      numa_bind(&mask);
    #+END_SRC

  - =numa_get_run_node_mask()= returns a mask of CPUs on which the current task is allowed to run. This can be used to save and restore the scheduler affinity state before running a child process or starting a thread.

- Most functions in this library are only concerned about numa nodes and their memory. But some function which deals with the CPUs associated with numa nodes.
  - =int numa_node_to_cpus(int node, struct bitmask *mask);= convert a node number to a bitmask of CPUs. 

- NUMA allocation statistics with numastat
  - The statistic info is for each node. It aggregates the results from all cores on a node to form a single result for the entire node.
  - It reports the following statistics:
    1) numa_hit
    2) numa_miss
    3) numa_foreign
    4) local_node
    5) interleave_hit
    6) other_node
       
    The difference between numa_miss and numa_hit and local_node and foreign_node is that the first two count hit or miss for the NUMA policy. The latter count if the allocation was on the same node as the requesting thread.

** How to use libnuma to boost?
- check numa architectures on Linux
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ numactl --hardware
    available: 8 nodes (0-7)
    node 0 cpus: 0 4 8 12 16 20 24 28        === 
    node 0 size: 65526 MB
    node 0 free: 62754 MB
    node 1 cpus: 32 36 40 44 48 52 56 60     === 
    node 1 size: 65536 MB
    node 1 free: 60707 MB
    node 2 cpus: 2 6 10 14 18 22 26 30      ===
    node 2 size: 65536 MB
    node 2 free: 63404 MB
    node 3 cpus: 34 38 42 46 50 54 58 62   ===
    node 3 size: 65536 MB
    node 3 free: 63005 MB
    node 4 cpus: 3 7 11 15 19 23 27 31
    node 4 size: 65536 MB
    node 4 free: 62909 MB
    node 5 cpus: 35 39 43 47 51 55 59 63
    node 5 size: 65536 MB
    node 5 free: 63202 MB
    node 6 cpus: 1 5 9 13 17 21 25 29
    node 6 size: 65536 MB
    node 6 free: 11 MB
    node 7 cpus: 33 37 41 45 49 53 57 61
    node 7 size: 65520 MB
    node 7 free: 51391 MB
    node distances:
    node   0   1   2   3   4   5   6   7 
    0:  10  16  16  22  16  22  16  22 
    1:  16  10  22  16  16  22  22  16 
    2:  16  22  10  16  16  16  16  16 
    3:  22  16  16  10  16  16  22  22 
    4:  16  16  16  16  10  16  16  22 
    5:  22  22  16  16  16  10  22  16 
    6:  16  22  16  22  16  22  10  16 
    7:  22  16  16  22  22  16  16  10 
  #+END_SRC
  - There are seven processors, each has 7 cores
    
*** How to retrieve the core id in which a thread is running? 
  Use =sched_getcpu()=
  #+BEGIN_SRC c
    #include <stdio.h>
    #include <sched.h>
    #include <omp.h>

    int main() {
    #pragma omp parallel
      {
        int thread_num = omp_get_thread_num();
        int cpu_num = sched_getcpu();
        printf("Thread %3d is running on CPU %3d\n", thread_num, cpu_num);
      }

      return 0;
    }
  #+END_SRC
  
  Use command line to check the affinity and execution result:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ GOMP_CPU_AFFINITY='0,1,2,3' ./bin/openmp_with_numa 10 10 4
    The number of highest possible node in the system is: 63
    Thread   0 is running on CPU   0
    Thread   1 is running on CPU   1
    Thread   3 is running on CPU   3
    Thread   2 is running on CPU   2
    From initialization to finished, use: 12ms
  #+END_SRC

** measurement time 
- 
* Troubleshooting
** GCC version is 4.8 not enough to support OpenMP 4.0 for proc_bind functionality
- download gcc7.2 
  wget [[http://ftp.tsukuba.wide.ad.jp/software/gcc/releases/gcc-7.2.0/gcc-7.2.0.tar.gz][gcc7.2.0]]
- tar zxvf and cd into directory
- =./contrib/download_prerequisites=
- cd ..
- mkdir objdir
- cd objdir
- =$PWD/../gcc-7.2.0/configure --prefix=$HOME/gcc-7.2.0 --disable-multilib=
  - =disable-multilib= means only support 64-bits version.
- make 
- make install 

Referenced from [[https://gcc.gnu.org/wiki/InstallingGCC][install gcc]].

** Error during executing compiled program 
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 8 8 8
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by ./bin/openmp_with_numa)
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libgomp.so.1: version `GOMP_4.0' not found (required by ./bin/openmp_with_numa)
#+END_SRC
- reason: the program is using the system default OpenMP
- solution:
  1) find the new OpenMP library
     #+BEGIN_SRC sh
        wzhao@r815:~/numa-knn$ find /usr find / name -name libgomp.so.1
        /usr/lib/x86_64-linux-gnu/libgomp.so.1
        /usr/local/lib32/libgomp.so.1
        /usr/local/lib64/libgomp.so.1
     #+END_SRC
  2) set the path to newer version in =LD_LIBRARY_PATH=.
     =export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/wzhao/gcc-4.9.4/lib64/=

** Execution record 
*** experiment show different execution time on the matrix assignment code for 1) from local node 2) from remote node
|   Index | serial on local node | serial on remote node |
|       1 | 24243ms              | 29185ms               |
|       2 | 26409ms              | 28327ms               |
|       3 | 24598ms              | 28099ms               |
|       4 | 25857ms              | 27667ms               |
|       5 | 26112ms              | 27980ms               |
|       6 | 24398ms              | 29055ms               |
|       7 | 25489ms              | 28016ms               |
|       8 | 25214ms              | 28432ms               |
|       9 | 25606ms              | 28209ms               |
|      10 | 25341ms              | 28076ms               |
| Average | 25326.7 ms           | 28304.6 ms            |



*** experiment show different execution time on matrix time vector code for 
  1) aware of NUMA
     parallel 32 threads on 4 node, allocate memory on those 4 nodes
  2) unawere of NUMA
     parallel 32 threads

|   Index | unaware of NUMA | aware of NUMA |
|       1 | 12580ms         | 14579ms       |
|       2 | 13036ms         | 14709ms       |
|       3 | 12646ms         | 14922ms       |
| average | 12754 ms        | 14736.667 ms  |

#+TBLFM: @5$2=vmean(@2..@4)::@5$3=vmean(@2..@4
- the reason why NUMA aware version is worse than not using NUMA is the vector is allocated on a specific node, so even each row of matrix is allocated according to memory affinity, it is slower during matrix * vector. Because for most of the row, the memory on vector is on remote node.
- I need to allocate vector's memory according to memory affinity.

*** Use vector<float*> instead of vector<float>  to use libnuma to allocate sizeof(float) memory on specific node
table shows the experiment result, unit is ms.
- column 2 shows case of allocation according to memory affinity
- column 3 shows case of allocate vector<float*> on node 7
|   index | numa_openmp | anti_numa_openmp | openmp_without_numa |
|---------+-------------+------------------+---------------------|
|       1 |        1010 |             1023 |                 516 |
|       2 |        1014 |             1009 |                 523 |
|       3 |        1015 |             1010 |                 522 |
|       4 |        1013 |             1024 |                 523 |
|       5 |        1016 |             1008 |                 543 |
| Average |      1013.6 |           1014.8 |               525.4 |

#+TBLFM: $3=vmean(@2..@6)::@7$2=vmean(@2..@6)::@7$4=vmean(@2..@6)
- code block of using vector<float*> to represent vector and use it to compute matrix vector multiplication
#+BEGIN_SRC c
  vector<float*> a;
  vector<float*> b;
  vector<float*> c;


  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(m, n, a, b, c, topology) private(i, j, eachRow, a_each, c_each)
  {
    int thread_num = omp_get_thread_num();
    int cpu_num = sched_getcpu();

    printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);
      
  #pragma omp for ordered schedule (static)
    for (i = 0; i < m; i++) {

      eachRow = NULL;
      a_each = NULL;
      c_each = NULL;

      cpu_num = sched_getcpu();
      int which_node = topology[cpu_num];

      eachRow = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
      a_each = (float*)numa_alloc_onnode(sizeof(float), which_node);
      c_each = (float*)numa_alloc_onnode(sizeof(float), which_node);

      if (eachRow == NULL || a_each == NULL || c_each == NULL) {
        printf("error during allocation numa memory on node %d\n", cpu_num);
        exit(-1);
      }

      a_each[0] = 0.0;
      c_each[0] = 2.0;
      for (j = 0; j < n; j++) {
        eachRow[j] = 2.0;
      }

  #pragma omp ordered
      a.push_back(a_each);
  #pragma omp ordered
      b.push_back(eachRow);
  #pragma omp ordered
      c.push_back(c_each);
    }


    printf("Check if the thread is paralleled as planed\n");
    auto started = std::chrono::high_resolution_clock::now();
  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n, topology) private(i, j)
    {
      int thread_num = omp_get_thread_num();
      int cpu_num = sched_getcpu();

      printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);

  #pragma omp for schedule (static)
      for (i = 0; i < m; i++) {
        for (j = 0; j < n; j++) {
          a[i][0] += (b[i][j] * (*c[i]));
        }
      }  
    }

    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
#+END_SRC

