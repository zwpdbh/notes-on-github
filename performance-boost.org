* Performance monitor on AMD
** What criteria do I need to measure to identify the performance bottleneck?
** How to monitor each of those criteria?
* Performance bost using libnuma
** How to use libnuma to boost?
- check numa architectures on Linux
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ numactl --hardware
    available: 8 nodes (0-7)
    node 0 cpus: 0 4 8 12 16 20 24 28        
    node 0 size: 65526 MB
    node 0 free: 62754 MB
    node 1 cpus: 32 36 40 44 48 52 56 60     
    node 1 size: 65536 MB
    node 1 free: 60707 MB
    node 2 cpus: 2 6 10 14 18 22 26 30      
    node 2 size: 65536 MB
    node 2 free: 63404 MB
    node 3 cpus: 34 38 42 46 50 54 58 62   
    node 3 size: 65536 MB
    node 3 free: 63005 MB
    node 4 cpus: 3 7 11 15 19 23 27 31
    node 4 size: 65536 MB
    node 4 free: 62909 MB
    node 5 cpus: 35 39 43 47 51 55 59 63
    node 5 size: 65536 MB
    node 5 free: 63202 MB
    node 6 cpus: 1 5 9 13 17 21 25 29
    node 6 size: 65536 MB
    node 6 free: 11 MB
    node 7 cpus: 33 37 41 45 49 53 57 61
    node 7 size: 65520 MB
    node 7 free: 51391 MB
    node distances:
    node   0   1   2   3   4   5   6   7 
    0:  10  16  16  22  16  22  16  22 
    1:  16  10  22  16  16  22  22  16 
    2:  16  22  10  16  16  16  16  16 
    3:  22  16  16  10  16  16  22  22 
    4:  16  16  16  16  10  16  16  22 
    5:  22  22  16  16  16  10  22  16 
    6:  16  22  16  22  16  22  10  16 
    7:  22  16  16  22  22  16  16  10 
  #+END_SRC
  - There are seven processors, each has 7 cores
    
*** How to retrieve the core id in which a thread is running? 
  Use =sched_getcpu()=
  #+BEGIN_SRC c
    #include <stdio.h>
    #include <sched.h>
    #include <omp.h>

    int main() {
    #pragma omp parallel
      {
        int thread_num = omp_get_thread_num();
        int cpu_num = sched_getcpu();
        printf("Thread %3d is running on CPU %3d\n", thread_num, cpu_num);
      }

      return 0;
    }
  #+END_SRC
  
  Use command line to check the affinity and execution result:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ GOMP_CPU_AFFINITY='0,1,2,3' ./bin/openmp_with_numa 10 10 4
    The number of highest possible node in the system is: 63
    Thread   0 is running on CPU   0
    Thread   1 is running on CPU   1
    Thread   3 is running on CPU   3
    Thread   2 is running on CPU   2
    From initialization to finished, use: 12ms
  #+END_SRC
** NUMA API
*** Introduction
- On an SMP(symmetric multiprocessing) system, all CPUs have equal access to the same shared memory controller that connects to all the memory chips (DIMMs). Communication between the CPUs also goes through this shared resource, which can become congested.
- The number of memory chips that can be managed by the single controller is also limited, which limit how much memory can be suppported by the system. In addition, the latency to access memory through this single traffic hub is relative high.
- The NUMA architecture was designed to surpass the scalability limits of the SMP architecture. Instead of having a single memory controller per computer, the system is split into multiple nodes.
  - Each node has processors and its own memory.
  - The processors have very fast access to the local memory in the node.
  - All the nodes in the system are connected using a fast interconnect.
- NUMA policy is concerned with putting memory allocations on specific nodes to let programs access them as quickly as possible.
  *The primary way to do this is to allocate memory for a thread on its local node and keep the thread running there (node affinity)*
  1) plan your thread execution, for which thread executing on which CPU 
  2) allocate memory ahead based on the node position of that CPU
- In addition to =numactl= library, =numastat= collect statistics about the memory allocation and =numademo= to show the effect of different policies on the system.

*** Policies
- Policies can be set process or per memory region.
  - Policies set per memory region, also called VMA policies3, allow a process to set a policy for a block of memory in its address space. Memory region policies have a higher priority than the process policy.

*** libnuma
- =nodemask_t= is a fixed size bit set of node numbers.
  #+BEGIN_SRC c
    nodemask_set(&mask, maxnode); /* set node highest */
    if (nodemask_isset(&mask, 1)) { /* is node 1 set? */
      ...
     }
    nodemask_clr(&mask, maxnode); /* clear highest node again */

  #+END_SRC
  
- allocation memory on node/set of nodes
  1) =void *numa_alloc_onnode(size_t size, int node);=
  2) =void *numa_alloc_interleaved_subset(size_t size, struct bitmask *nodemask);=

- libnuma process policy
  When existing code in a program cannot be modified to use the numa_alloc functions directly, it is sometimes useful to change the process policy in a program. This way, specific subfunctions can be run with a nondefault policy without actually modifying their code.
  - Each thread has a default memory policy inherited from its parent. Unless changed with numactl, this policy is normally used to allocate memory preferably on the current node.
  - =numa_set_interleave_mask= enables interleaving for the current thread. All future memory allocations allocate memory round robin interleaved over the nodemask specified.
  - Process policy can also be used to set a policy for a child process before starting it.
  - =numa_bind()= binds the current task and its children to the nodes specified in nodemask. They will only run on the CPUs of the specified nodes and only be able to allocate memory from them. This function is equivalent to calling =numa_run_on_node_mask(nodemask)= followed by =numa_set_membind(nodemask)=.
    - =numa_run_on_node_mask()= runs the current task and its children only on nodes specified in =nodemask=. They will not migrate to CPUs of other nodes until the node affinity is reset with a new call to =numa_run_on_node_mask()=. Passing =numa_all_nodes= permits the kernel to schedule on all nodes again.
    - =void numa_set_membind(struct bitmask *nodemask);= sets the memory allocation mask. The task will only allocate memory from the nodes set in nodemask.

- Changing the policy of existing memory areas
  When working with shared memory, it is often not possible to use the numa_alloc family of functions to allocate memory. The memory must be gotten from shmat() or mmap instead. To allow libnuma programs to set policy on such areas, there are additional functions for setting memory policy for already existing memory areas.
  These function only affect future allocation in the specified area.

- Except allocating memory on specific nodes, another part of NUMA policy is to run the thread on the CPUs of the correct node.
  A simple way to use libnuma is the =numa_bind= function. It binds both the CPU and the memory of the process allocated in the future to a specific nodemask.
  - Example of binding process CPU and memory allocation to node 1 using numa_bind:
    #+BEGIN_SRC c
      nodemask_t mask;
      nodemask_zero(&mask);
      nodemask_set(&mask 1);
      numa_bind(&mask);
    #+END_SRC

  - =numa_get_run_node_mask()= returns a mask of CPUs on which the current task is allowed to run. This can be used to save and restore the scheduler affinity state before running a child process or starting a thread.

- Most functions in this library are only concerned about numa nodes and their memory. But some function which deals with the CPUs associated with numa nodes.
  - =int numa_node_to_cpus(int node, struct bitmask *mask);= convert a node number to a bitmask of CPUs. 

- NUMA allocation statistics with numastat
  - The statistic info is for each node. It aggregates the results from all cores on a node to form a single result for the entire node.
  - It reports the following statistics:
    1) numa_hit
    2) numa_miss
    3) numa_foreign
    4) local_node
    5) interleave_hit
    6) other_node
       
    The difference between numa_miss and numa_hit and local_node and foreign_node is that the first two count hit or miss for the NUMA policy. The latter count if the allocation was on the same node as the requesting thread.
* Use pthread to do the experiment
To eliminate other errors which could be caused by using OpenMP, I use pthread to do the test. Also, using OpenMP is not every convonient to test different situations since you need to set OpenMP environment and remember to use the correct in different program.
- also use tick() to measure time
#+BEGIN_SRC c
  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  start = tick();
  // do something
  end = tick();
  printf("%llu ", end - start);

#+END_SRC
** Test serial version of code for memory access time
*** code to measure access time
#+BEGIN_SRC c
  #include <iostream>
  #include <vector>
  #include <sched.h>
  #include <stdio.h>
  #include <numa.h>
  #include <stdlib.h>
  #include <string>
  #include <map>
  #include <pthread.h>
  #include <sys/types.h>
  #include <unistd.h>


  using namespace std;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse);

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  /**allocate and access on the same node*/
  int main(int argc, char* argv[]) {
    if (argc != 3) {
      printf("usage: <size> <remote=1 true, or 0 false>, exit...\n");
      exit(-1);
    }

    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else {
      numa_set_strict(1);
    }
    
    map<int, int> topology;
    map<int, vector<int> > topology_inverse;
    initMap(topology, topology_inverse);

    long mx_size = atol(argv[1]);
    int remote = atoi(argv[2]);
    int on_cpu = 0;
    int on_node = 0;
    pid_t pid;
    pthread_t tid;

    pid = getpid();
    tid = pthread_self();
    printf("main thread: pid %lu tid %lu, on cpu: %d\n", (unsigned long)pid, (unsigned long)tid, sched_getcpu());
    
    cpu_set_t cpuset;
    cpu_set_t allcpuset;

    CPU_ZERO(&cpuset);
    CPU_SET(on_cpu, &cpuset);
    pthread_setaffinity_np(tid, sizeof(cpu_set_t), &cpuset);
    int cpu_num = sched_getcpu();
    printf("Now, main thread tid = %lu, is executing on cpu: %d, on node %d\n",pthread_self(), cpu_num, topology[cpu_num]);

    float* mx = NULL;
    
    if (remote) {
      on_node = 7;
    }
    printf("will allocate memory on node %d\n", on_node);
    mx = (float*)numa_alloc_onnode(mx_size * mx_size * sizeof(float), on_node);
    if (mx == NULL) {
      printf("could not allocate memory on node %d, exit...\n", on_node);
    }
    
    auto start = tick();
    for (long i = 0; i < mx_size * mx_size; i++) {
      mx[i] = i + i * 0.5 + i / 2;
    }

    auto end = tick();
    if (remote && topology[sched_getcpu()] != on_node) {
      std::cout << "remote access, use: " << end - start << endl;
    } else if (!remote && topology[sched_getcpu()] == on_node) {
      std::cout << "local access, use: " << end - start << endl;
    }

    numa_free(mx, mx_size * mx_size * sizeof(float));

    return 0;
  }

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse) {

    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }
#+END_SRC


*** access time records
size = 12800
|   id |      remote |      local |
|    1 |  6190994967 | 5525219763 |
|    2 |  4853278945 | 4454001759 |
|    3 |  5865416519 | 4404425967 |
|    4 |  6259437334 | 5740065838 |
|    5 |  5452799794 | 5076688723 |
| mean | 5724385500. | 5040080410 |
|------+-------------+------------|
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)

It is clear to see the effect of remote access is slower.
  
** Test parallel version of code for memory access and multiplication with vector
*** code with parallel access and multiplication
#+BEGIN_SRC c
  #include <pthread.h>
  #include <iostream>
  #include <sched.h>
  #include <sys/types.h>
  #include <unistd.h>
  #include <sched.h>
  #include <map>
  #include <vector>
  #include <numa.h>
  #include <chrono>

  using namespace std;

  long ROWS;
  long COLS;
  int NUM_THREADS = 64;
  int NUM_NODES = 8;

  float** mx = nullptr;
  float* v = nullptr;
  float* w = nullptr;
  long s = 0;
  int with_numa = 1;
  map<int, int> topology;
  map<int, vector<int> > topology_inverse;

  long acc_time_from_mx = 0;
  long acc_time_from_v = 0;

  pthread_t main_thread;

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  typedef struct pthreadInfo {
    int thread_id;
    long from;
    long job_size;
    long row_in_mx;
    int p;
    pthread_t thread;
  } pInfo;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void* thr_alloc_fn(void* arg);
  void* thr_access_fn(void* arg);
  void* thr_multiply_fn(void* arg);
  void* normal_alloc_fn(void* arg);
  void* normal_access_fn(void* arg);
  void* normal_multiply_fn(void* arg);
  void* alloc_fn(void* arg);
  void* access_fn(void* arg);
  void* multiply_fn(void* arg);
  void check_mx();
  void check_w();
  void construct_barrier(pInfo* pthreads, int n_threads);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse);
  void migrate_to_node(int node_index);


  int main(int argc, char* argv[]) {
    if (argc != 4) {
      printf("usage: ./bin/matrix_multiplication <rows> <cols> <with_numa>, exit...\n");
      printf("choose between -1, 0, 1, 2\n");
      exit(-1);
    }
    
    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else  {
      numa_set_strict(1);    
    }

    ROWS = atol(argv[1]);
    COLS = atol(argv[2]);
    with_numa = atoi(argv[3]);

    s = ROWS * COLS;
    printf("s = %ld\n", s);
    initMap(topology, topology_inverse);

    /**make sure the main thread is executed on a fixed cpu*/
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(0, &cpuset);
    main_thread = pthread_self();
    pthread_setaffinity_np(main_thread, sizeof(cpu_set_t), &cpuset);
    printf("fix main_thread: %lu on cpu0\n", main_thread);
    
    if (with_numa == -1) {
      printf("===Allocate mx with %ld * %ld, WITHOUT NUMA awareness:\n", ROWS, COLS);
      mx = (float**)emalloc(sizeof(float*) * ROWS);
      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_THREADS);
      int err = 0;
      printf("allocate %ld by %ld matrix\n", ROWS, COLS);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        //      printf("thread_%d, is allocating mx[i] from %ld to %ld\n", pthreads[i].thread_id, pthreads[i].from, pthreads[i].from + pthreads[i].job_size);
        err = pthread_create(&pthreads[i].thread, NULL, normal_alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }

      construct_barrier(pthreads, NUM_THREADS);

      /**measure access time*/
      auto start = tick();
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, normal_access_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
        
      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      printf("normal parallel access matrix, use: %lld\n", end - start);
      
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }

      start = tick();
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, normal_multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      printf("normal parallel multiplication use: %lld\n", end - start);

      for (int i = 0; i < ROWS; i++) {
        free(mx[i]);
      }
      free(pthreads);
      free(mx);
      free(v);
      free(w);
      exit(0);
    } else if (with_numa == 2) {
      printf("===Allocate mx with %ld * %ld, WITH NUMA awareness:\n", ROWS, COLS);
      mx = (float**)emalloc(sizeof(float*) * ROWS);
      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_THREADS);
      int err = 0;
      printf("allocate %ld by %ld matrix\n", ROWS, COLS);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS -1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }
      construct_barrier(pthreads, NUM_THREADS);
      
      auto start = tick();
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].job_size = ROWS / NUM_THREADS;
        pthreads[i].from = i * (ROWS / NUM_THREADS);
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, access_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }
      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      printf("parallel access matrix with shape ROWS * COLS, use: %lld\n", end - start);
      
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }

      start = tick();
      for (int i = 0; i < NUM_THREADS; i++) {
        if (ROWS / NUM_THREADS == 0) {
          pthreads[i].from = i * 1;
          pthreads[i].job_size = 1;
        } else {
          pthreads[i].from  = i * (ROWS / NUM_THREADS);
          pthreads[i].job_size = ROWS / NUM_THREADS;
        }

        if (i == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }

      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      printf("parallel multiplication with matrix shape ROWS * COLS, use: %lld\n", end - start);

      int cpu_num = sched_getcpu();
      if (cpu_num) {
        printf("main thread is not fixed on cpu0 \n");
      }

      for (long i = 0; i < ROWS; i++) {
        numa_free(mx[i], sizeof(float) * COLS);
      }
      free(mx);
      free(pthreads);
      free(v);
      free(w);
      exit(0);
    } else {
      mx = (float**)emalloc(sizeof(float*) * NUM_NODES);

      pInfo* pthreads = (pInfo*) emalloc(sizeof(*pthreads) * NUM_NODES);
      pthread_t* thread; // array of pthread_t, for catching each created thread
      int err = 0;
      printf("===Allocate mx with %d * %ld, ", NUM_NODES, s / NUM_NODES);
      if (with_numa) {
        printf("WITH NUMA awareness\n");
      } else {
        printf("WITHOUT NUMA awareness\n");
      }
      /**Allocate m by n matrix with the shape NUM_NODES * some right size*/
      for (int i = 0; i < NUM_NODES; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].row_in_mx = i;
        pthreads[i].job_size = (s/NUM_NODES);
        if (pthreads[i].thread_id == NUM_NODES - 1) {
          pthreads[i].job_size += (s % NUM_NODES);
        }
        err = pthread_create(&pthreads[i].thread, NULL, thr_alloc_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit..\n");
          exit(-1);
        }
      }

      /**manually created barrier*/
      construct_barrier(pthreads, NUM_NODES);
      //  check_mx();

      auto start = tick();
      //  auto started = std::chrono::high_resolution_clock::now();
      pthreads =(pInfo*)remalloc(pthreads, sizeof(*pthreads) * NUM_THREADS);
      for (int i = 0; i < NUM_THREADS; i++) {
        pthreads[i].thread_id = i;
        pthreads[i].row_in_mx = i / (NUM_THREADS / NUM_NODES);
        pthreads[i].p = i  - pthreads[i].row_in_mx * (NUM_THREADS / NUM_NODES);
        pthreads[i].job_size = s / NUM_THREADS;
        pthreads[i].from = pthreads[i].p * pthreads[i].job_size;
        if ((pthreads[i].p + 1) % (NUM_THREADS / NUM_NODES) == 0) {
          pthreads[i].job_size += ((s / NUM_NODES) % (NUM_THREADS / NUM_NODES));
        }
        if (pthreads[i].thread_id == NUM_THREADS - 1) {
          pthreads[i].job_size +=  (s % NUM_NODES);
        }

        err = pthread_create(&pthreads[i].thread, NULL, thr_access_fn, (void *)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }

      construct_barrier(pthreads, NUM_THREADS);
      auto end = tick();
      //  auto done = std::chrono::high_resolution_clock::now();
      printf("parallel access matrix, use: %lld\n", end - start);
      //  std::cout << "By c++, parallel access matrix, use: " << std::chrono::duration_cast<std::chrono::nanoseconds>(done-started).count() << endl;
      //  check_mx();


      /**do the multiplication with a vector*/
      v = (float*)emalloc(sizeof(float) * COLS);
      w = (float*)emalloc(sizeof(float) * ROWS);
      for (int i = 0; i < COLS; i++) {
        v[i] = 1;
      }
      for (int i = 0; i < ROWS; i++) {
        w[i] = 0;
      }
    
    
      start = tick();
      for (int i = 0; i < NUM_THREADS; i++) {
        if (ROWS / NUM_THREADS == 0) {
          pthreads[i].from = i * 1;
          pthreads[i].job_size = 1;
        } else {
          pthreads[i].from  = i * (ROWS / NUM_THREADS);
          pthreads[i].job_size = ROWS / NUM_THREADS;
        }

        if (i == NUM_THREADS - 1) {
          pthreads[i].job_size += (ROWS % NUM_THREADS);
        }
        err = pthread_create(&pthreads[i].thread, NULL, thr_multiply_fn, (void*)&pthreads[i]);
        if (err != 0) {
          printf("error during thread creation, exit...\n");
          exit(-1);
        }
      }


      construct_barrier(pthreads, NUM_THREADS);
      end = tick();
      //  done = std::chrono::high_resolution_clock::now();
      printf("parallel multiplication, use: %lld\n", end - start);
      //  std::cout << "By c++, parallel multiplication, use: " << std::chrono::duration_cast<std::chrono::nanoseconds>(done-started).count() << endl;
      //  check_w();
    
      int cpu_num = sched_getcpu();
      if (cpu_num) {
        printf("main thread is not fixed on cpu0 \n");
      }
      for (int i = 0; i < NUM_NODES; i++) {
        if (i == NUM_NODES - 1) {
          numa_free(mx[i], (s / NUM_NODES + s % NUM_NODES) * sizeof(float));
        } else {
          numa_free(mx[i], (s / NUM_NODES) * sizeof(float));
        }
      }

      free(mx);
      free(pthreads);
      free(v);
      free(w);
      exit(0);
    } // end of else
  }

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  // thread function to use libnuma to allocate memory on specific node
  void* thr_alloc_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    //  printf("id: %d, thread: %lu is allocating %ld on node %d\n", info.thread_id, info.thread, info.job_size, info.row_in_mx);

    mx[info.thread_id] = (float*)numa_alloc_onnode(info.job_size * sizeof(float), info.row_in_mx);
    for (long k = 0; k < info.job_size; k++) {
      ,*(*(mx + info.row_in_mx) + k) = 0.0;
      //    mx[info.row_in_mx][k] = 0;
    }
    //  printf("thread %d  numa_alloc_onnode(%ld, %d), done\n", info.thread_id, info.job_size, info.row_in_mx);
    return (void *)0;
  }


  void* thr_access_fn(void* arg) {
    pid_t pid = getpid();
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];

    pInfo info = *(pInfo *)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      //    printf("thread_id = %d, on cpu %d, on node %d, should be on node%d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node, node_index);
      migrate_to_node(node_index);
      //    sched_cpu = sched_getcpu();
      //    current_on_node = topology[sched_cpu];
      //    printf("thread_id = %d, on cpu %d, on node %d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node);
    }
    //  printf("id: %d, thread: %lu, is accessing on cpu: %d, on node: %d\n", info.thread_id, info.thread, sched_cpu, current_on_node);
    for (long i = info.from; i < info.from + info.job_size; i++) {
      mx[info.row_in_mx][i] = 3.0;
    }
    return (void *)0;
  }

  void* thr_multiply_fn(void* arg) {
    pid_t pid = getpid();
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];

    pInfo info = *(pInfo*)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      //printf("thread_id = %d, on cpu %d, on node %d, should be on node%d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node, node_index);
      migrate_to_node(node_index);
    }
    //  printf("id: %d, thread: %lu, is doing multiplication from cpu %d, on node %d\n", info.thread_id, info.thread, sched_cpu, current_on_node);
    for (long r = info.from; r < info.from  + info.job_size; ++r) {
      // get corresponding mx(i, j)
      for (long i = 0; i < COLS; i++) {
        long l = r * COLS + i;
        long row_in_mx = l / (s / NUM_NODES) - 1;
        if (row_in_mx < 0) {
          row_in_mx = 0;
        }
        long col_in_mx = l % (s / NUM_NODES);
        w[r] += (*(*(mx + row_in_mx) + col_in_mx) * v[i]);
      }
    }

    return (void *)0;
  }

  void check_mx() {
    // check the content of mx
    long size_for_each_node = s / NUM_NODES;

    for(int i = 0; i < NUM_NODES; i++) {
      if (i == NUM_NODES -1) {
        size_for_each_node += s % NUM_NODES;
      }
      for (int j = 0; j < size_for_each_node; j++) {
        printf("%.f ", *(*(mx + i) + j));
      }
      printf("\n");
    }
  }

  void check_w() {
    printf("w = \n");
    for (long i = 0; i < ROWS; i++) {
      printf("%.f ", w[i]);
    }
    printf("\n");
  }


  void construct_barrier(pInfo* pthreads, int n_threads) {
    int err;
    void* status;
    for (int i = 0; i < n_threads; i++) {
      if (main_thread == pthreads[i].thread) {
        printf("one of the worker threads is main thread !!!\n\n\n");
      }
      err = pthread_join(pthreads[i].thread, &status);
      if (err) {
        printf("error, return code from pthread_join() is %d\n", *(int*)status);
      }
      //    printf("thread %d is joint\n", i);
    }
  }


  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse) {

    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }

  void* normal_alloc_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    for (long k = info.from; k < info.from + info.job_size; k++) {
      mx[k] = (float*)emalloc(sizeof(float) * COLS);
      for (long j = 0; j < COLS; j++){
        mx[k][j] = 0;
      }
    }
    //  printf("thread_%d, done alloc.\n", info.thread_id);
    return (void*) 0;
  }

  void* normal_access_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        mx[k][j] = 3;
      }
    }
  }


  void* normal_multiply_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    //  printf("thread_id = %d, from = %d, to = %d\n", info.thread_id, info.from, info.from + info.job_size);
    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        w[k] += (mx[k][j] * v[j]);
      }
    }
  }

  void* alloc_fn(void* arg) {
    pInfo info = *(pInfo *)arg;
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    
    for (long k = info.from; k < info.from + info.job_size; k++) {
      mx[k] = (float*)numa_alloc_onnode(sizeof(float) * COLS, node_index);
      for (long j = 0; j < COLS; j++){
        mx[k][j] = 0;
      }
    }
  }


  void* access_fn(void* arg) {
    pInfo info = *(pInfo*)arg;
    
    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);
    if(with_numa && node_index != current_on_node) {
      migrate_to_node(node_index);
    }

    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        mx[k][j] = 3;
      }
    }
  }

  void* multiply_fn(void* arg) {
    pInfo info = *(pInfo*)arg;

    int sched_cpu = sched_getcpu();
    int current_on_node = topology[sched_cpu];
    
    int node_index = info.thread_id / (NUM_THREADS / NUM_NODES);

    if(with_numa && node_index != current_on_node) {
      migrate_to_node(node_index);
    }

    for (long k = info.from; k < info.from + info.job_size; k++) {
      for (long j = 0; j < COLS; j++) {
        w[k] += (mx[k][j] * v[j]);
      }
    }
  }


  void migrate_to_node(int node_index) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    vector<int> cpus = topology_inverse[node_index];
    for (int i = 0; i < cpus.size(); i++) {
      CPU_SET(cpus[i], &cpuset);
    }
    pthread_t tid = pthread_self();
    pthread_setaffinity_np(tid, sizeof(cpu_set_t), &cpuset);
    //  int sched_cpu = sched_getcpu();
    //  int current_on_node = topology[sched_cpu];
    //  printf("thread_id = %d, on cpu %d, on node %d\n", (unsigned long)info.thread_id, sched_cpu, current_on_node);
  }

#+END_SRC

*** Execution record
**** Fix main thread executing on CPU 0, use 
./bin/matrix_multiplication 12800 12800
***** Use C++ nanoseconds to measure
| id | WITH access | WITH multiplication | WITHOUT access | WITHOUT multiplication |
|  1 |   166326074 |          1510417312 |     1301943431 |            12556594665 |
|  2 |   166366766 |          1510256587 |     1302406072 |            12558704630 |
|  3 |   166237962 |          1508920121 |     1302070425 |            12558016977 |
|  4 |   166215395 |          1509373984 |     1299333226 |            12556759414 |

***** Use tick() to measure
| id | WITH access | WITH multiplication | WITHOUT access | WITHOUT multiplication |
|  1 |   350128990 |          3492185360 |     2753145226 |            28683960961 |
|  2 |   349595320 |          3495013569 |     2749643354 |            28687094593 |
|  3 |   349548272 |          3494935935 |     2751425583 |            28685699069 |
|  4 |   349219292 |          3493437642 |     2751906677 |            28688361121 |

***** compare normal situation with NUMA
|                        id | WITH access | WITH multiplication | WITHOUT access | WITHOUT multiplication | NORMAL access | NORMAL multiplication |
|                         1 |   345545088 |          3491522057 |     2691950679 |            28953121579 |    1946171831 |            4201424415 |
|                         2 |   345787749 |          3490512751 |     2691251738 |            28937151565 |    1945789503 |            4196188061 |
|                         3 |   345435510 |          3492186245 |     2691130047 |            28946658459 |    1945328493 |            4200917062 |
|                         4 |   345432426 |          3490595989 |     2692585216 |            28950542474 |    1945922490 |            4200946452 |
|                      mean |   345550190 |          3491204300 |     2691729420 |            28946869000 |    1945803100 |            4199869000 |
|---------------------------+-------------+---------------------+----------------+------------------------+---------------+-----------------------|
| without compare with NUMA |  1945803100 |          4199869000 |                |                        |               |                       |
|  normal compare with NUMA |  2691729420 |         28946869000 |                |                        |               |                       |

***** comparison between NUMA with different allocation struction
- Allocate mx with 12800 * 12800, WITH NUMA awareness
  |   id | access_time | multiplication_time |
  |    1 |   272672319 |           531647736 |
  |    2 |   269100843 |           532553171 |
  |    3 |   270184762 |           532270269 |
  |    4 |   268955594 |           531988564 |
  | mean |  270228380. |           532114935 |

#+TBLFM: $2=vmean(@2..@5)::@6$3=vmean(@2..@5)

- Allocate mx with 8 * 20480000, WITH NUMA awareness
  |   id | access_time | multiplication_time |
  |    1 |   320727622 |          3476584573 |
  |    2 |   321024532 |          3472887618 |
  |    3 |   322771861 |          3473999289 |
  |    4 |   320801179 |          3473729126 |
  | mean |  321331300. |         3474300200. |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)

- Allocate mx with 8 * 20480000, WITHOUT NUMA
  |   id | access_time | multiplication_time |
  |    1 |  2546591700 |         28718785908 |
  |    2 |  2561287436 |         28719224914 |
  |    3 |  2566636553 |         28717831502 |
  |    4 |  2553683247 |         28722415729 |
  | mean |  2557049734 |        28719565000. |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)

- Allocate mx with 12800 * 12800, WITHOUT NUMA awareness
  |   id | access_time | multiplication_time |
  |    1 |  1985376933 |          4197224163 |
  |    2 |  1986976911 |          4191556113 |
  |    3 |  1984342423 |          4213631400 |
  |    4 |  1984112597 |          4193804496 |
  | mean |  1985202216 |          4199054043 |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)

*** Summary
  |                                                        | access_time | multiplication_time |
  | Allocate mx with 12800 * 12800, WITHOUT NUMA awareness |  1985202216 |          4199054043 |
  | Allocate mx with 8 * 20480000, WITHOUT NUMA awareness  |  2557049734 |         28719565000 |
  | Allocate mx with 8 * 20480000, WITH NUMA awareness     |   321331300 |          3474300200 |
  | Allocate mx with 12800 * 12800, WITH NUMA awareness    |   270228380 |           532114935 |
  |--------------------------------------------------------+-------------+---------------------|

  - currently, when use NUMA, the not natural way to allocation need carefully allocation of memory, it could be faster, if the code for computing the correct index is more efficient.
* Performance boost using OpenMP
** code using openmp to do matrix time vector
#+BEGIN_SRC c
  #include <omp.h>
  #include <map>
  #include <pthread.h>
  #include <sys/types.h>
  #include <unistd.h>
  #include <numa.h>
  #include <sched.h>
  #include <vector>

  using namespace std;

  long ROWS;
  long COLS;
  int NUM_THREADS = 64;
  int NUM_NODES = 8;
  pthread_t main_thread;
  int with_numa = 1;

  float** mx = nullptr;
  float* v = nullptr;
  float* w = nullptr;
  long s = 0;
  map<int, int> topology;
  map<int, vector<int> > topology_inverse;
  map<int, int> distribute_record;

  void *emalloc(size_t s);
  void *remalloc(void *p, size_t s);
  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse);
  int is_distribute_record_not_consisted(int thread_id, int sched_cpu);
  void check_w();

  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }


  int main(int argc, char* argv[]) {
    if (argc != 4) {
      printf("usage: ./bin/xxx <rows> <cols> <1 or 0>, exit...\n");
      exit(-1);
    }
    
    if(numa_available() == -1) {
      printf("no libnuma support\n");
    } else  {
      numa_set_strict(1);    
    }

    ROWS = atol(argv[1]);
    COLS = atol(argv[2]);
    with_numa = atoi(argv[3]);

    initMap(topology, topology_inverse);
    
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(0, &cpuset);
    main_thread = pthread_self();
    pthread_setaffinity_np(main_thread, sizeof(cpu_set_t), &cpuset);
    printf("fix main_thread: %lu on cpu0\n", main_thread);

    v = (float*)emalloc(sizeof(float) * COLS);
    w = (float*)emalloc(sizeof(float) * ROWS);
    mx = (float**)emalloc(sizeof(float*) * ROWS);

    long i = 0;
    long j = 0;

    for (j = 0; j < COLS; j++) {
      v[j] = 1;
    }
    for (i = 0; i < ROWS; i++) {
      w[i] = 0;
    }
    //  check_w();
    if (with_numa) {
      printf("===WITH NUMA:\n");
      /**allocate mx among different nodes*/
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx, topology, ROWS, COLS, distribute_record) private(i, j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        distribute_record.insert(make_pair(thread_id, topology[sched_cpu]));

  #pragma omp for ordered schedule (static)
        for(i = 0; i < ROWS; i++) {
          sched_cpu = sched_getcpu();
          int node_index = topology[sched_cpu];
          mx[i] = (float*)numa_alloc_onnode(sizeof(float) * COLS, node_index);
        }
      }

      /**measure access time*/
      auto start = tick();
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx,topology, ROWS, COLS, distribute_record) private(i,j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        is_distribute_record_not_consisted(thread_id, sched_cpu);

  #pragma omp for schedule (static)
        for (i = 0; i < ROWS; i++) {
          for (j = 0; j < COLS; j++) {
            mx[i][j] = 3;
          }
        }
      }
      auto end = tick();
      printf("parallel access use: %lld\n", end - start);


      /**measure multiplication time*/
      start = tick();
  #pragma omp parallel proc_bind(close) num_threads(NUM_THREADS) default(none) shared(mx, v, w, topology, ROWS, COLS, distribute_record) private(i, j)
      {
        int sched_cpu = sched_getcpu();
        int thread_id = omp_get_thread_num();
        //    is_distribute_record_not_consisted(thread_id, sched_cpu);

  #pragma omp for schedule (static)
        for (i = 0; i < ROWS; i++) {
          for(j = 0; j < COLS; j++) {
            w[i] += (mx[i][j] * v[j]);
          }
        }
      }
      end = tick();
      printf("parallel matrix multiplication use: %lld\n", end - start);

      for(i = 0; i < ROWS; i++) {
        numa_free(mx[i], sizeof(float) * COLS);
      }
      free(mx);
      free(v);
      free(w);

      exit(0);
    } else {
      printf("===WITHOUT NUMA:\n");
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        mx[i] = (float*)emalloc(sizeof(float) * COLS);
      }

      auto start = tick();
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        for (j = 0; j < COLS; j++) {
          mx[i][j] = 3;
        }
      }
      auto end = tick();
      printf("parallel access use: %lld\n", end - start);

      start = tick();
  #pragma omp parallel for num_threads(NUM_THREADS) default(none) shared(mx, v, w, ROWS, COLS) private(i, j)
      for (i = 0; i < ROWS; i++) {
        for (j = 0; j < COLS; j++) {
          w[i] += (mx[i][j] * v[j]);
        }
      }
      end = tick();
      printf("parallel matrix multiplication use: %lld\n", end - start);

      for(i = 0; i < ROWS; i++) {
        free(mx[i]);
      }
      free(mx);
      free(v);
      free(w);
      exit(0);
    }
  }



  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == nullptr) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void *remalloc(void *p, size_t s) {
    void *result = realloc(p, s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void initMap(map<int, int> &topology, map<int, vector<int> > &topology_inverse) {

    vector<int> index_set = {0, 32, 2, 34, 3, 35, 1, 33};
    for (int node = 0 ; node < index_set.size(); node++) {
      vector<int> cpus_on_node;
      for (int cpu = index_set[node]; cpu < index_set[node] + 32; cpu+=4) {
        //      printf("make pair: %d <=> %d\n", node, cpu);
        topology.insert(make_pair(cpu, node));
        cpus_on_node.push_back(cpu);
      }
      topology_inverse.insert(make_pair(node, cpus_on_node));
    }
  }

  void check_w() {
    printf("w = \n");
    for (long k = 0; k < ROWS; k++) {
      printf("%.f ", w[k]);
    }
    printf("\n");
  }


  int is_distribute_record_not_consisted(int thread_id, int sched_cpu) {
    if (distribute_record.find(thread_id)->second != topology[sched_cpu]) {
      //    printf("previous record: thread_%d <=> node_%d\n", thread_id, distribute_record.find(thread_id)->second);
      //    printf("current record: thread_%d <=> node_%d\n", thread_id, topology[sched_cpu]);
      return -1;
    }
    
    return 0;
  }

#+END_SRC
** problem of using openmp
- To use OpenMP to allocate and access matrix element locally, I need to ensure the openmp execute each thread consistedly during allocation and accessing. But it does not do so. If I use ordered clause to make it so, it becomes serial execution which is no point to use OpenMP
- execution result
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ ./bin/openmp_matrix_multiplication 64 64
    fix main_thread: 140012401936320 on cpu0
    previous record: thread_53 <=> node_0
    previous record: thread_22 <=> node_0
    previous record: thread_40 <=> node_0
    previous record: thread_15 <=> node_0
    previous record: thread_23 <=> node_0
    previous record: thread_13 <=> node_0
    previous record: thread_59 <=> node_0
    previous record: thread_24 <=> node_0
    current record: thread_24 <=> node_3
    previous record: thread_43 <=> node_0
    current record: thread_43 <=> node_5
    previous record: thread_10 <=> node_0
    current record: thread_10 <=> node_1
    previous record: thread_33 <=> node_0
    current record: thread_33 <=> node_4
    previous record: thread_29 <=> node_0
    current record: thread_29 <=> node_3
    previous record: thread_48 <=> node_0
    current record: thread_48 <=> node_6
    previous record: thread_28 <=> node_0
    current record: thread_28 <=> node_3
    previous record: thread_47 <=> node_0
    current record: thread_47 <=> node_5
    previous record: thread_16 <=> node_0
    current record: thread_16 <=> node_2
    previous record: thread_11 <=> node_0
    current record: thread_11 <=> node_1
    previous record: thread_31 <=> node_0
    previous record: thread_34 <=> node_0
    current record: thread_34 <=> node_4
    previous record: thread_55 <=> node_0
    current record: thread_55 <=> node_6
    previous record: thread_41 <=> node_0
    current record: thread_41 <=> node_5
    previous record: thread_20 <=> node_0
    current record: thread_20 <=> node_2
    previous record: thread_51 <=> node_0
    current record: thread_51 <=> node_6
    previous record: thread_18 <=> node_0
    current record: thread_18 <=> node_2
    previous record: thread_17 <=> node_0
    current record: thread_17 <=> node_2
    previous record: thread_32 <=> node_0
    current record: thread_32 <=> node_4
    previous record: thread_44 <=> node_0
    current record: thread_44 <=> node_5
    previous record: thread_56 <=> node_0
    current record: thread_56 <=> node_7
    previous record: thread_49 <=> node_0
    current record: thread_49 <=> node_6
    previous record: thread_63 <=> node_0
    current record: thread_63 <=> node_7
    previous record: thread_54 <=> node_0
    current record: thread_54 <=> node_6
    previous record: thread_61 <=> node_0
    current record: thread_61 <=> node_7
    previous record: thread_26 <=> node_0
    current record: thread_26 <=> node_3
    previous record: thread_42 <=> node_0
    current record: thread_42 <=> node_5
    current record: thread_53 <=> node_6
    previous record: thread_57 <=> node_0
    current record: thread_57 <=> node_7
    previous record: thread_9 <=> node_0
    current record: thread_9 <=> node_1
    previous record: thread_52 <=> node_0
    current record: thread_52 <=> node_6
    previous record: thread_50 <=> node_0
    current record: thread_50 <=> node_6
    previous record: thread_46 <=> node_0
    current record: thread_46 <=> node_5
    current record: thread_22 <=> node_2
    previous record: thread_35 <=> node_0
    current record: thread_35 <=> node_4
    previous record: thread_36 <=> node_0
    current record: thread_36 <=> node_4
    previous record: thread_25 <=> node_0
    current record: thread_25 <=> node_3
    previous record: thread_38 <=> node_0
    current record: thread_38 <=> node_4
    previous record: thread_37 <=> node_0
    current record: thread_37 <=> node_4
    current record: thread_40 <=> node_5
    previous record: thread_58 <=> node_0
    current record: thread_58 <=> node_7
    previous record: thread_62 <=> node_0
    current record: thread_62 <=> node_7
    current record: thread_15 <=> node_1
    previous record: thread_12 <=> node_0
    current record: thread_12 <=> node_1
    current record: thread_23 <=> node_2
    previous record: thread_45 <=> node_0
    current record: thread_45 <=> node_5
    current record: thread_13 <=> node_1
    current record: thread_59 <=> node_7
    previous record: thread_19 <=> node_0
    current record: thread_19 <=> node_2
    previous record: thread_30 <=> node_0
    current record: thread_30 <=> node_3
    previous record: thread_27 <=> node_0
    current record: thread_27 <=> node_3
    previous record: thread_21 <=> node_0
    current record: thread_21 <=> node_2
    current record: thread_31 <=> node_3
    previous record: thread_60 <=> node_0
    current record: thread_60 <=> node_7
    previous record: thread_39 <=> node_0
    current record: thread_39 <=> node_4
    previous record: thread_14 <=> node_0
    current record: thread_14 <=> node_1
    parallel access use: 107051367
    parallel matrix multiplication use: 17470602
  #+END_SRC
- Time record
  |        id |    access | multiplication | without_access | without_multiplication |
  |         1 | 333679058 |       86703288 |      307299765 |               98623590 |
  |         2 | 322496449 |       81689395 |      299924519 |               85783732 |
  |         3 | 346066176 |       81845200 |      319220052 |              100671163 |
  |         4 | 350297098 |       79371861 |      303946232 |               85174860 |
  |      mean | 338134700 |       82402436 |      307597642 |               92563336 |
  |-----------+-----------+----------------+----------------+------------------------|
  | pthread_1 | 270228380 |      532114935 |                |                        |
  | pthread_2 | 321331300 |     3474300200 |                |                        |
#+TBLFM: @6$2=vmean(@2..@5)::@6$3=vmean(@2..@5)::@6$4=vmean(@2..@5)::@6$5=vmean(@2..@5)
* Troubleshooting
** GCC version is 4.8 not enough to support OpenMP 4.0 for proc_bind functionality
- download gcc7.2 
  wget [[http://ftp.tsukuba.wide.ad.jp/software/gcc/releases/gcc-7.2.0/gcc-7.2.0.tar.gz][gcc7.2.0]]
- tar zxvf and cd into directory
- =./contrib/download_prerequisites=
- cd ..
- mkdir objdir
- cd objdir
- =$PWD/../gcc-7.2.0/configure --prefix=$HOME/gcc-7.2.0 --disable-multilib=
  - =disable-multilib= means only support 64-bits version.
- make 
- make install 

Referenced from [[https://gcc.gnu.org/wiki/InstallingGCC][install gcc]].

** Error during executing compiled program 
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 8 8 8
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by ./bin/openmp_with_numa)
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libgomp.so.1: version `GOMP_4.0' not found (required by ./bin/openmp_with_numa)
#+END_SRC
- reason: the program is using the system default OpenMP
- solution:
  1) find the new OpenMP library
     #+BEGIN_SRC sh
        wzhao@r815:~/numa-knn$ find /usr find / name -name libgomp.so.1
        /usr/lib/x86_64-linux-gnu/libgomp.so.1
        /usr/local/lib32/libgomp.so.1
        /usr/local/lib64/libgomp.so.1
     #+END_SRC
  2) set the path to newer version in =LD_LIBRARY_PATH=.
     =export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/wzhao/gcc-4.9.4/lib64/=

** Execution record using OpenMP 
*** Experiment show different execution time on matrix time vector code for 
  1) aware of NUMA
     parallel 32 threads on 4 node, allocate memory on those 4 nodes
  2) unawere of NUMA
     parallel 32 threads

|   Index | unaware of NUMA | aware of NUMA |
|       1 | 12580ms         | 14579ms       |
|       2 | 13036ms         | 14709ms       |
|       3 | 12646ms         | 14922ms       |
| average | 12754 ms        | 14736.667 ms  |

#+TBLFM: @5$2=vmean(@2..@4)::@5$3=vmean(@2..@4
- the reason why NUMA aware version is worse than not using NUMA is the vector is allocated on a specific node, so even each row of matrix is allocated according to memory affinity, it is slower during matrix * vector. Because for most of the row, the memory on vector is on remote node.
- I need to allocate vector's memory according to memory affinity.

*** Use vector<float*> instead of vector<float>  to use libnuma to allocate sizeof(float) memory on specific node
table shows the experiment result, unit is ms.
- column 2 shows case of allocation according to memory affinity
- column 3 shows case of allocate vector<float*> on node 7
|   index | numa_openmp | anti_numa_openmp | openmp_without_numa |
|---------+-------------+------------------+---------------------|
|       1 |        1010 |             1023 |                 516 |
|       2 |        1014 |             1009 |                 523 |
|       3 |        1015 |             1010 |                 522 |
|       4 |        1013 |             1024 |                 523 |
|       5 |        1016 |             1008 |                 543 |
| Average |      1013.6 |           1014.8 |               525.4 |

#+TBLFM: $3=vmean(@2..@6)::@7$2=vmean(@2..@6)::@7$4=vmean(@2..@6)
- code block of using vector<float*> to represent vector and use it to compute matrix vector multiplication
#+BEGIN_SRC c
  vector<float*> a;
  vector<float*> b;
  vector<float*> c;


  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(m, n, a, b, c, topology) private(i, j, eachRow, a_each, c_each)
  {
    int thread_num = omp_get_thread_num();
    int cpu_num = sched_getcpu();

    printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);
      
  #pragma omp for ordered schedule (static)
    for (i = 0; i < m; i++) {

      eachRow = NULL;
      a_each = NULL;
      c_each = NULL;

      cpu_num = sched_getcpu();
      int which_node = topology[cpu_num];

      eachRow = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
      a_each = (float*)numa_alloc_onnode(sizeof(float), which_node);
      c_each = (float*)numa_alloc_onnode(sizeof(float), which_node);

      if (eachRow == NULL || a_each == NULL || c_each == NULL) {
        printf("error during allocation numa memory on node %d\n", cpu_num);
        exit(-1);
      }

      a_each[0] = 0.0;
      c_each[0] = 2.0;
      for (j = 0; j < n; j++) {
        eachRow[j] = 2.0;
      }

  #pragma omp ordered
      a.push_back(a_each);
  #pragma omp ordered
      b.push_back(eachRow);
  #pragma omp ordered
      c.push_back(c_each);
    }


    printf("Check if the thread is paralleled as planed\n");
    auto started = std::chrono::high_resolution_clock::now();
  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n, topology) private(i, j)
    {
      int thread_num = omp_get_thread_num();
      int cpu_num = sched_getcpu();

      printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);

  #pragma omp for schedule (static)
      for (i = 0; i < m; i++) {
        for (j = 0; j < n; j++) {
          a[i][0] += (b[i][j] * (*c[i]));
        }
      }  
    }

    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
#+END_SRC

*** To fully control the code behaviour, use C **float instead of C++ vector<float*>
- pointer as array, the following pairs of execution has the same effect
#+BEGIN_SRC c
  printf("the address of a is %ld\n", a);
  printf("*a = %ld\n", *a);
  printf("a[0] = %ld\n", a[0]);

  printf("*(a+1) = %ld\n", *(a+1));
  printf("a[1] = %ld\n", a[1]);
    
  printf("*(*(a+i) + j) = %f\n", *(*(a+1) + 0));
  printf("a[i][j] = %f\n", a[1][0]);

#+END_SRC

- code block which allocate memory, a, b, c are 2 dimensional array, as matrix
  #+BEGIN_SRC c
    float** a;
    float** b;
    float** c;
      
    b = (float**) emalloc(m * sizeof(float*));
    c = (float**) emalloc(m * sizeof(float*));
    a = (float**) emalloc(m * sizeof(float*));
  #+END_SRC

- code block which allocates numa memory on specific node
#+BEGIN_SRC c
  #pragma omp for ordered schedule (static)
  for (i = 0; i < m; i++) {

    cpu_num = sched_getcpu();
    int which_node = topology[cpu_num];
        
    b[i] = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
    a[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);
    c[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);

    if (b[i] == NULL || a[i] == NULL || c[i] == NULL) {
      printf("error during allocation numa memory on node %d\n", cpu_num);
      exit(-1);
    }

    a[i][0] = 0.0;
    c[i][0] = 2.0;
    for (j = 0; j < n; j++) {
      b[i][j] = 2.0;
    }
   }    
  }
#+END_SRC

- code block which does execution of matrix time vector and messures time
 #+BEGIN_SRC c
   #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n) private(i, j)
     {
   #pragma omp for schedule (static)
       for (i = 0; i < m; i++) {
         for (j = 0; j < n; j++) {
           ,*(*(a+i) + 0) += ( *(*(b+i) +j) * (*(*(c +j) +0)));
           //      a[i][0] += (b[i][j] * c[j][0]);
         }
       }  
     }

   auto done = std::chrono::high_resolution_clock::now();
   std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
 #+END_SRC

**** time mesurement 
#+BEGIN_SRC sh
  cat numa_openmp.sh
  export OMP_PLACES="{0, 4, 8, 12, 16, 20, 24, 28}, {32, 36, 40, 44, 48, 52, 56, 60}, {2, 6, 10, 14, 18, 22, 26, 30}, {34, 38, 42, 46, 50, 54, 58, 62}, {3, 7, 11, 15, 19, 23, 27, 31}, {35, 39, 43, 47, 51, 55, 59, 63}, {1, 5, 9, 13, 17, 21, 25, 29}, {33, 37, 41, 45, 49, 53, 57, 61}"
  ./bin/numa_openmp 16000 16000 8
  ./bin/anti_numa_openmp 16000 16000 8
  ./bin/openmp_without_numa 16000 16000 8
#+END_SRC
|   id | use libnuma(ms) | not use libnuma(ms) | use libnuma on remote node |
|    1 |            3880 |                 420 |                       3934 |
|    2 |            4034 |                 416 |                       3973 |
|    3 |            4020 |                 418 |                       3960 |
|    4 |            3975 |                 419 |                       3951 |
|    5 |            4033 |                 413 |                       3908 |
| mean |          3988.4 |               417.2 |                     3945.2 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)
- It seems this approach make the libnuma case becomes worse.
- The execution time of using libnuma and the execution time of using libnuma but allocated memory deliberately on remote node is also similar. This indicate I have lot of remote memory access cases.
- This whole program is wrong because each row of the matrix need to multiple very element of vector, there is no point of allocation vector across multiple nodes.

*** OpenMP is not a good choice for doing Matrix times Matrix with NUMA Awareness
**** The initial basic idea of using OpenMP with NUMA
- OpenMP is doing data parallelisation via =for= loop. OpenMP distributes the data across different threads and each thread works on one chunck of data and in the end merge the result.
- NUMA is about memory affinity. If the thread and the data it is accessing is located on the same node, then the accessing speed will be faster than remote accessing.
- Using OpenMP data parallelisation with NUMA awareness. My initial plan is:
  1) first configure OpenMP environment to control where and how OpenMP generate thread based on the =for= loop.
     This is the so called =proc_bind=, such as make sure generated: thread 1 is running on cpu01
     thread 2 is running on cpu02
  2) Based on NUMA topology, such cpu01 is on node01, cpu02 is on node02. Allocate data's memory among different threads, so data's memory is distributed allocated by different threads, which in turn allocated on different cpu ==> on different nodes.
  3) When I do the computation using data. I make sure use the same OpenMP parallel schema, such that the generate threads are as same as before. Thus, each thread will be able to read its part of data locally since the corresponding part of data memory has been allocated on the right node already.
  4) This idea need to make sure the size of different data matches each other.
- Case study why this doesn't work, on matrix time matrix:
  - suppose matrix m which is 2 * 5 , times matrix n which is 5 * 2.

  - allocate memory using OpenMP, in the loop, allocate each row of m on different threads while allocate each column of n on the same different threads. Then initalize each element.
  - For the result matrix w which 2 * 2. Then it has the following problem:
    | 00 | 01 |
    |----+----|
    | 10 | 11 | 
    - if w is allocated row by row, which means 00, 01 is on node0, 10 and 11 is on node1. Then during computation, remote access will happend on 01(on node0) since it is the result of the first row of m which is on node0 times the second column of n which is on node 1. Similar thing happened on 10.
    - In fact, only left to right diagonal element which is refered in double loop as i == j can avoid remote access.

In general, data parallelisation in the case of matrix time matrix could not be computed efficiently using OpenMP with NUMA awareness configuration. 

  Data parallelisation with NUMA awareness could be hard since any interaction part in the parallel region will require access from remote nodes(because you generate threads on different CPUs => different nodes). Thus, the result will be similar with averagely allocation memory across different nodes which is the default setting for NUMA. The more frequently the interaction is, the less effect of NUMA allocation will be. It is not about how to allocate memory with NUMA, it is about the interaction between different parts of data which is inherited from the algorithm.
  
  So, a simple "solution" will be make multiple copy of data and let each thread works on its own corresponding copy. That is a kind of  task parallelism and I doult about the practical usage of it. 
** Error from using pthread
- result for check the matrix which is allocated according to the topology of NUMA nodes:
#+BEGIN_SRC sh
  pid = 60287, tid = 123145488523264, for i = 0
  pid = 60287, tid = 123145489059840, for i = 1
  pid = 60287, tid = 123145489596416, for i = 2
  pid = 60287, tid = 123145490132992, for i = 3
  pid = 60287, tid = 123145490669568, for i = 4
  pid = 60287, tid = 123145491206144, for i = 5
  pid = 60287, tid = 123145491742720, for i = 6
  pid = 60287, tid = 123145492279296, for i = 7
  for thread = 7, it gets extra job, size_for_each_node = 14
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 
      0 0 0 0 0 0 0 0 0 0 0 0 -76060119211228138797542729056256 0 
#+END_SRC
