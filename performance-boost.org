* Performance monitor on AMD
** What criteria do I need to measure to identify the performance bottleneck?
** How to monitor each of those criteria?
* Use pthread to do the experiment
To eliminate other errors which could be caused by using OpenMP, I use pthread to do the test. Also, using OpenMP is not every convonient to test different situations since you need to set OpenMP environment and remember to use the correct in different program.
- also use tick() to measure time
#+BEGIN_SRC c
  inline uint64_t tick() {
    uint32_t tmp[2];
    __asm__ ("rdtsc" : "=a" (tmp[1]), "=d" (tmp[0]) : "c" (0x10) );
    return (((uint64_t) tmp[0]) << 32) | tmp[1];
  }

  start = tick();
  // do something
  end = tick();
  printf("%llu ", end - start);

#+END_SRC
** Test serial version of code for memory access time
*** code to measure access time
#+BEGIN_SRC c
  #include <iostream>
  #include <chrono>
  #include <vector>
  #include <sched.h>
  #include <stdio.h>
  #include <numa.h>
  #include <stdlib.h>
  #include <string>
  #include <map>
  #include <pthread.h>
  #include <sys/types.h>
  #include <unistd.h>


  using namespace std;

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }

  void initMap(map<int, int> &topology) {
    // node 0 cpus: 0 4 8 12 16 20 24 28
    for (int i = 0; i <=28; i+=4) {
      topology.insert(make_pair(i, 0));
    }

    // node 1 cpus: 32 36 40 44 48 52 56 60
    for (int i = 32; i <=60; i+=4) {
      topology.insert(make_pair(i, 1));
    }

    // node 2 cpus: 2 6 10 14 18 22 26 30
    for (int i = 2; i <= 30; i+=4) {
      topology.insert(make_pair(i, 2));
    }

    // node 3 cpus: 34 38 42 46 50 54 58 62
    for (int i = 34; i <= 62; i+=4) {
      topology.insert(make_pair(i, 3));
    }
    
    // node 4 cpus: 3 7 11 15 19 23 27 31
    for (int i = 3; i <= 31; i+=4) {
      topology.insert(make_pair(i, 4));
    }

    // node 5 cpus: 35 39 43 47 51 55 59 63
    for (int i = 35; i <= 63; i+=4) {
      topology.insert(make_pair(i, 5));
    }

    // node 6 cpus: 1 5 9 13 17 21 25 29
    for (int i = 1; i <= 29; i+=4) {
      topology.insert(make_pair(i, 6));
    }

    // node 7 cpus: 33 37 41 45 49 53 57 61
    for (int i = 33; i <= 61; i+=4) {
      topology.insert(make_pair(i, 7));
    }
  }

  /**allocate and access on the same node*/
  int main(int argc, char* argv[]) {
    if (argc != 3) {
      printf("usage: specify the size of n by n matrix, exit...\n");
      exit(-1);
    }

    if(numa_available() == -1) {
      printf("no libnuma support\n");
    }
    
    map<int, int> topology;
    initMap(topology);

    long mx_size = atol(argv[1]);
    int remote = atoi(argv[2]);
    int on_cpu = 0;
    int on_node = 0;
    pid_t pid;
    pthread_t tid;

    pid = getpid();
    tid = pthread_self();
    printf("main thread: pid %lu tid %lu, on cpu: %d\n", (unsigned long)pid, (unsigned long)tid, sched_getcpu());
    
    cpu_set_t cpuset;
    cpu_set_t allcpuset;

    CPU_ZERO(&cpuset);
    CPU_SET(on_cpu, &cpuset);
    pthread_setaffinity_np(tid, sizeof(cpu_set_t), &cpuset);
    int cpu_num = sched_getcpu();
    printf("Now, main thread tid = %lu, is executing on cpu: %d, on node %d\n",pthread_self(), cpu_num, topology[cpu_num]);

    float* mx = NULL;
    numa_set_strict(1);
    
    if (remote) {
      on_node = 7;
    }
    printf("will allocate memory on node %d\n", on_node);
    mx = (float*)numa_alloc_onnode(mx_size * mx_size * sizeof(float), on_node);
    if (mx == NULL) {
      printf("could not allocate memory on node %d, exit...\n", on_node);
    }
    
    cpu_num = sched_getcpu();
    printf("Make sure, main thread tid = %lu, is executing on cpu: %d, on node %d\n",pthread_self(), cpu_num, topology[cpu_num]);
    auto started = std::chrono::high_resolution_clock::now();
    printf("Accessing matrix from cpu %d on node %d, to access memory allocated on node %d.\n", cpu_num, topology[cpu_num], on_node);
    for (long i = 0; i < mx_size * mx_size; i++) {
      mx[i] = i + i * 0.5 + i / 2;
    }

    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::nanoseconds>(done-started).count() << "ns" << endl;

    numa_free(mx, mx_size * mx_size * sizeof(float));

    return 0;
  }
#+END_SRC

***** =numa_set_strict(1);= must be set !
**** time execution record
| ./bin/memory_access_without_parallel 100000 1 (remote case) | ./bin/memory_access_without_parallel 100000 0 (local) |
|                                                162444187364 |                                          138574271370 |
|                                                155324195985 |                                          149659914995 |
|                                                159565283709 |                                          148881298203 |
|                                               159111220000. |                                         145705160000. |
#+TBLFM: @5$1=vmean(@2..@4)::@5$2=vmean(@2..@4)


| ./bin/memory_access_without_parallel 10000 1 (remote) | ./bin/memory_access_without_parallel 10000 0 (local) |
|                                            1403278413 |                                           1262754231 |
|                                            1635843573 |                                           1428285921 |
|                                            1661301586 |                                           1483400262 |
|                                            1530844828 |                                           1355476917 |
|                                            1557817100 |                                          1382479300. |
#+TBLFM: @6$1=vmean(@2..@5)::@6$2=vmean(@2..@5)
***** It is clearly to see that the time execution on remote case is longer than local access case.

** Test parallel version of code for memory access time
**** code with parallel 
**** time execution record
* Performance bost using libnuma
** How to use libnuma to boost?
- check numa architectures on Linux
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ numactl --hardware
    available: 8 nodes (0-7)
    node 0 cpus: 0 4 8 12 16 20 24 28        === 
    node 0 size: 65526 MB
    node 0 free: 62754 MB
    node 1 cpus: 32 36 40 44 48 52 56 60     === 
    node 1 size: 65536 MB
    node 1 free: 60707 MB
    node 2 cpus: 2 6 10 14 18 22 26 30      ===
    node 2 size: 65536 MB
    node 2 free: 63404 MB
    node 3 cpus: 34 38 42 46 50 54 58 62   ===
    node 3 size: 65536 MB
    node 3 free: 63005 MB
    node 4 cpus: 3 7 11 15 19 23 27 31
    node 4 size: 65536 MB
    node 4 free: 62909 MB
    node 5 cpus: 35 39 43 47 51 55 59 63
    node 5 size: 65536 MB
    node 5 free: 63202 MB
    node 6 cpus: 1 5 9 13 17 21 25 29
    node 6 size: 65536 MB
    node 6 free: 11 MB
    node 7 cpus: 33 37 41 45 49 53 57 61
    node 7 size: 65520 MB
    node 7 free: 51391 MB
    node distances:
    node   0   1   2   3   4   5   6   7 
    0:  10  16  16  22  16  22  16  22 
    1:  16  10  22  16  16  22  22  16 
    2:  16  22  10  16  16  16  16  16 
    3:  22  16  16  10  16  16  22  22 
    4:  16  16  16  16  10  16  16  22 
    5:  22  22  16  16  16  10  22  16 
    6:  16  22  16  22  16  22  10  16 
    7:  22  16  16  22  22  16  16  10 
  #+END_SRC
  - There are seven processors, each has 7 cores
    
*** How to retrieve the core id in which a thread is running? 
  Use =sched_getcpu()=
  #+BEGIN_SRC c
    #include <stdio.h>
    #include <sched.h>
    #include <omp.h>

    int main() {
    #pragma omp parallel
      {
        int thread_num = omp_get_thread_num();
        int cpu_num = sched_getcpu();
        printf("Thread %3d is running on CPU %3d\n", thread_num, cpu_num);
      }

      return 0;
    }
  #+END_SRC
  
  Use command line to check the affinity and execution result:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ GOMP_CPU_AFFINITY='0,1,2,3' ./bin/openmp_with_numa 10 10 4
    The number of highest possible node in the system is: 63
    Thread   0 is running on CPU   0
    Thread   1 is running on CPU   1
    Thread   3 is running on CPU   3
    Thread   2 is running on CPU   2
    From initialization to finished, use: 12ms
  #+END_SRC
** NUMA API
*** Introduction
- On an SMP(symmetric multiprocessing) system, all CPUs have equal access to the same shared memory controller that connects to all the memory chips (DIMMs). Communication between the CPUs also goes through this shared resource, which can become congested.
- The number of memory chips that can be managed by the single controller is also limited, which limit how much memory can be suppported by the system. In addition, the latency to access memory through this single traffic hub is relative high.
- The NUMA architecture was designed to surpass the scalability limits of the SMP architecture. Instead of having a single memory controller per computer, the system is split into multiple nodes.
  - Each node has processors and its own memory.
  - The processors have very fast access to the local memory in the node.
  - All the nodes in the system are connected using a fast interconnect.
- NUMA policy is concerned with putting memory allocations on specific nodes to let programs access them as quickly as possible.
  *The primary way to do this is to allocate memory for a thread on its local node and keep the thread running there (node affinity)*
  1) plan your thread execution, for which thread executing on which CPU 
  2) allocate memory ahead based on the node position of that CPU
- In addition to =numactl= library, =numastat= collect statistics about the memory allocation and =numademo= to show the effect of different policies on the system.

*** Policies
- Policies can be set process or per memory region.
  - Policies set per memory region, also called VMA policies3, allow a process to set a policy for a block of memory in its address space. Memory region policies have a higher priority than the process policy.

*** libnuma
- =nodemask_t= is a fixed size bit set of node numbers.
  #+BEGIN_SRC c
    nodemask_set(&mask, maxnode); /* set node highest */
    if (nodemask_isset(&mask, 1)) { /* is node 1 set? */
      ...
     }
    nodemask_clr(&mask, maxnode); /* clear highest node again */

  #+END_SRC
  
- allocation memory on node/set of nodes
  1) =void *numa_alloc_onnode(size_t size, int node);=
  2) =void *numa_alloc_interleaved_subset(size_t size, struct bitmask *nodemask);=

- libnuma process policy
  When existing code in a program cannot be modified to use the numa_alloc functions directly, it is sometimes useful to change the process policy in a program. This way, specific subfunctions can be run with a nondefault policy without actually modifying their code.
  - Each thread has a default memory policy inherited from its parent. Unless changed with numactl, this policy is normally used to allocate memory preferably on the current node.
  - =numa_set_interleave_mask= enables interleaving for the current thread. All future memory allocations allocate memory round robin interleaved over the nodemask specified.
  - Process policy can also be used to set a policy for a child process before starting it.
  - =numa_bind()= binds the current task and its children to the nodes specified in nodemask. They will only run on the CPUs of the specified nodes and only be able to allocate memory from them. This function is equivalent to calling =numa_run_on_node_mask(nodemask)= followed by =numa_set_membind(nodemask)=.
    - =numa_run_on_node_mask()= runs the current task and its children only on nodes specified in =nodemask=. They will not migrate to CPUs of other nodes until the node affinity is reset with a new call to =numa_run_on_node_mask()=. Passing =numa_all_nodes= permits the kernel to schedule on all nodes again.
    - =void numa_set_membind(struct bitmask *nodemask);= sets the memory allocation mask. The task will only allocate memory from the nodes set in nodemask.

- Changing the policy of existing memory areas
  When working with shared memory, it is often not possible to use the numa_alloc family of functions to allocate memory. The memory must be gotten from shmat() or mmap instead. To allow libnuma programs to set policy on such areas, there are additional functions for setting memory policy for already existing memory areas.
  These function only affect future allocation in the specified area.

- Except allocating memory on specific nodes, another part of NUMA policy is to run the thread on the CPUs of the correct node.
  A simple way to use libnuma is the =numa_bind= function. It binds both the CPU and the memory of the process allocated in the future to a specific nodemask.
  - Example of binding process CPU and memory allocation to node 1 using numa_bind:
    #+BEGIN_SRC c
      nodemask_t mask;
      nodemask_zero(&mask);
      nodemask_set(&mask 1);
      numa_bind(&mask);
    #+END_SRC

  - =numa_get_run_node_mask()= returns a mask of CPUs on which the current task is allowed to run. This can be used to save and restore the scheduler affinity state before running a child process or starting a thread.

- Most functions in this library are only concerned about numa nodes and their memory. But some function which deals with the CPUs associated with numa nodes.
  - =int numa_node_to_cpus(int node, struct bitmask *mask);= convert a node number to a bitmask of CPUs. 

- NUMA allocation statistics with numastat
  - The statistic info is for each node. It aggregates the results from all cores on a node to form a single result for the entire node.
  - It reports the following statistics:
    1) numa_hit
    2) numa_miss
    3) numa_foreign
    4) local_node
    5) interleave_hit
    6) other_node
       
    The difference between numa_miss and numa_hit and local_node and foreign_node is that the first two count hit or miss for the NUMA policy. The latter count if the allocation was on the same node as the requesting thread.
* Performance boost using OpenMP
** code for Matrix times Vector
#+BEGIN_SRC c++
  #include <omp.h>
  #include <iostream>
  #include <chrono>
  #include <vector>

  using namespace std;

  void *emalloc(size_t s) {
    void *result = malloc(s);
    if (result == NULL) {
      fprintf(stderr, "memory allocation failed");
      exit(EXIT_FAILURE);
    }
    return result;
  }


  int main(int argc, char* argv[]) {
    if (argc < 3) {
      cout << "Please specify m n on command line" << endl;
      exit(-1);
    }
    long m, n;
    
    m = stol(argv[1]);
    n = stol(argv[2]);
    int n_cpu = stoi(argv[3]);
    
    
    float* a = (float*)emalloc(n * sizeof(float));
    vector<float*> b;
    float* c = (float*)emalloc(n * sizeof(float));
    
    long i, j;
    
    auto started = std::chrono::high_resolution_clock::now();
    
    float* eachRow;
    
    
    for (i = 0; i < m; i++) {
      eachRow = (float*)emalloc(n * sizeof(float));
      for (j = 0; j < n; j++) {
        eachRow[j] = i;
      }
      b.push_back(eachRow);
    }
    
    
  #pragma omp parallel default(none) shared(c, n) private(i)
    {
  #pragma omp for
      for (i = 0; i < n; i++) {
        c[i] = 2.0;
      }
    }
    
  #pragma omp parallel default(none) shared(a,b,c,m,n) private(i, j) 
    {
      
  #pragma omp for
      for (i = 0; i < m; i++) {
        a[i] = 0.0;
        //    cout << "Thread " << omp_get_thread_num() << " is executing" << endl;
        for (j = 0; j < n; j++) {
          a[i] += (b[i][j] * c[j]);
        }
      }  
    }
    
    for (i = 0; i < m; i++) {
      free(b[i]);
    }
    
    
    free(c);
    free(a);
    
    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
    
    return 0;
  }
#+END_SRC
** executing result comparing between using OpenMP and without using OpenMP
- The time of executeing the program with OpenMP suppport is roughly half of the time of executing the program without OpenMP.
  Serial code execution records:
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2655ms
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2384ms
    wzhao@r815:~/numa-knn$ ./bin/no_openmp_with_numa 10000 10000
    From initialization to finished, use: 2589ms
  #+END_SRC

  Simple parallel version execution time
  #+BEGIN_SRC sh
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1436ms
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1289ms
    wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 10000 10000
    From initialization to finished, use: 1278ms
  #+END_SRC

- use =/usr/bin/time= to examine the detail of execution on both version
  - for using OpenMP
    #+BEGIN_SRC sh
      wzhao@r815:~/numa-knn$ /usr/bin/time -v ./bin/openmp_with_numa 10000 10000
      From initialization to finished, use: 1246ms
      Command being timed: "./bin/openmp_with_numa 10000 10000"
      User time (seconds): 4.41
      System time (seconds): 0.76
      Percent of CPU this job got: 399%
      Elapsed (wall clock) time (h:mm:ss or m:ss): 0:01.29
      Average shared text size (kbytes): 0
      Average unshared data size (kbytes): 0
      Average stack size (kbytes): 0
      Average total size (kbytes): 0
      Maximum resident set size (kbytes): 1570640
      Average resident set size (kbytes): 0
      Major (requiring I/O) page faults: 0
      Minor (reclaiming a frame) page faults: 98332
      Voluntary context switches: 119
      Involuntary context switches: 623
      Swaps: 0
      File system inputs: 0
      File system outputs: 0
      Socket messages sent: 0
      Socket messages received: 0
      Signals delivered: 0
      Page size (bytes): 4096
      Exit status: 0

    #+END_SRC

  - for not using OpenMP
    #+BEGIN_SRC sh
      wzhao@r815:~/numa-knn$ /usr/bin/time -v ./bin/no_openmp_with_numa 10000 10000
      From initialization to finished, use: 2592ms
      Command being timed: "./bin/no_openmp_with_numa 10000 10000"
      User time (seconds): 2.38
      System time (seconds): 0.24
      Percent of CPU this job got: 99%
      Elapsed (wall clock) time (h:mm:ss or m:ss): 0:02.63
      Average shared text size (kbytes): 0
      Average unshared data size (kbytes): 0
      Average stack size (kbytes): 0
      Average total size (kbytes): 0
      Maximum resident set size (kbytes): 1568208
      Average resident set size (kbytes): 0
      Major (requiring I/O) page faults: 0
      Minor (reclaiming a frame) page faults: 98110
      Voluntary context switches: 2
      Involuntary context switches: 264
      Swaps: 0
      File system inputs: 0
      File system outputs: 0
      Socket messages sent: 0
      Socket messages received: 0
      Signals delivered: 0
      Page size (bytes): 4096
      Exit status: 0
    #+END_SRC

** The result is not very impressive, further investigation.
- When I use set number of threads to 32, it is better than setting it to 64 which is the Maximum number of cpu on the machine.

** Question
- " From the software point of view, this remote memory can be used in the same way as local memory; it is fully cache coherent." What is cache coherent?
- The following code could compile but during execution, it produces memory error messages
  #+BEGIN_SRC c
    #pragma omp parallel default(none) shared(m, n, b) private(eachRow, i, j)
      {
    #pragma omp for
        for (i = 0; i < m; i++) {
          eachRow = (float*)emalloc(n * sizeof(float));
          for (j = 0; j < n; j++) {
            eachRow[j] = i;
          }
          b.push_back(eachRow);
        }
      }  
  #+END_SRC
- On SMP system, there is a common optimization called cache affinity that is a bit similar. Cache affinity tries to keep data in the cache of a CPU instead of frequently ìbouncingî it between processors. This is commonly done by a scheduler in the operating system, which tries to keep threads on a CPU for some time before scheduling it on another. However, there is an important difference from node affinity: When a thread on an SMP system moves between CPUs, its cache contents eventually move with it. Once a memory area is committed to a specific node on a NUMA system, it stays there. A thread running on a different node that accesses it always adds traffic to the interconnect and leads to higher latency. This is why NUMA systems need to try harder to archive node affinity than SMP systems.

  Why "When a thread on an SMP system moves between CPUs, its cache contents eventually move with it." ? How?
* Troubleshooting
** GCC version is 4.8 not enough to support OpenMP 4.0 for proc_bind functionality
- download gcc7.2 
  wget [[http://ftp.tsukuba.wide.ad.jp/software/gcc/releases/gcc-7.2.0/gcc-7.2.0.tar.gz][gcc7.2.0]]
- tar zxvf and cd into directory
- =./contrib/download_prerequisites=
- cd ..
- mkdir objdir
- cd objdir
- =$PWD/../gcc-7.2.0/configure --prefix=$HOME/gcc-7.2.0 --disable-multilib=
  - =disable-multilib= means only support 64-bits version.
- make 
- make install 

Referenced from [[https://gcc.gnu.org/wiki/InstallingGCC][install gcc]].

** Error during executing compiled program 
#+BEGIN_SRC sh
  wzhao@r815:~/numa-knn$ ./bin/openmp_with_numa 8 8 8
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by ./bin/openmp_with_numa)
  ./bin/openmp_with_numa: /usr/lib/x86_64-linux-gnu/libgomp.so.1: version `GOMP_4.0' not found (required by ./bin/openmp_with_numa)
#+END_SRC
- reason: the program is using the system default OpenMP
- solution:
  1) find the new OpenMP library
     #+BEGIN_SRC sh
        wzhao@r815:~/numa-knn$ find /usr find / name -name libgomp.so.1
        /usr/lib/x86_64-linux-gnu/libgomp.so.1
        /usr/local/lib32/libgomp.so.1
        /usr/local/lib64/libgomp.so.1
     #+END_SRC
  2) set the path to newer version in =LD_LIBRARY_PATH=.
     =export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/wzhao/gcc-4.9.4/lib64/=

** Execution record 
*** Experiment show different execution time on matrix time vector code for 
  1) aware of NUMA
     parallel 32 threads on 4 node, allocate memory on those 4 nodes
  2) unawere of NUMA
     parallel 32 threads

|   Index | unaware of NUMA | aware of NUMA |
|       1 | 12580ms         | 14579ms       |
|       2 | 13036ms         | 14709ms       |
|       3 | 12646ms         | 14922ms       |
| average | 12754 ms        | 14736.667 ms  |

#+TBLFM: @5$2=vmean(@2..@4)::@5$3=vmean(@2..@4
- the reason why NUMA aware version is worse than not using NUMA is the vector is allocated on a specific node, so even each row of matrix is allocated according to memory affinity, it is slower during matrix * vector. Because for most of the row, the memory on vector is on remote node.
- I need to allocate vector's memory according to memory affinity.

*** Use vector<float*> instead of vector<float>  to use libnuma to allocate sizeof(float) memory on specific node
table shows the experiment result, unit is ms.
- column 2 shows case of allocation according to memory affinity
- column 3 shows case of allocate vector<float*> on node 7
|   index | numa_openmp | anti_numa_openmp | openmp_without_numa |
|---------+-------------+------------------+---------------------|
|       1 |        1010 |             1023 |                 516 |
|       2 |        1014 |             1009 |                 523 |
|       3 |        1015 |             1010 |                 522 |
|       4 |        1013 |             1024 |                 523 |
|       5 |        1016 |             1008 |                 543 |
| Average |      1013.6 |           1014.8 |               525.4 |

#+TBLFM: $3=vmean(@2..@6)::@7$2=vmean(@2..@6)::@7$4=vmean(@2..@6)
- code block of using vector<float*> to represent vector and use it to compute matrix vector multiplication
#+BEGIN_SRC c
  vector<float*> a;
  vector<float*> b;
  vector<float*> c;


  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(m, n, a, b, c, topology) private(i, j, eachRow, a_each, c_each)
  {
    int thread_num = omp_get_thread_num();
    int cpu_num = sched_getcpu();

    printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);
      
  #pragma omp for ordered schedule (static)
    for (i = 0; i < m; i++) {

      eachRow = NULL;
      a_each = NULL;
      c_each = NULL;

      cpu_num = sched_getcpu();
      int which_node = topology[cpu_num];

      eachRow = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
      a_each = (float*)numa_alloc_onnode(sizeof(float), which_node);
      c_each = (float*)numa_alloc_onnode(sizeof(float), which_node);

      if (eachRow == NULL || a_each == NULL || c_each == NULL) {
        printf("error during allocation numa memory on node %d\n", cpu_num);
        exit(-1);
      }

      a_each[0] = 0.0;
      c_each[0] = 2.0;
      for (j = 0; j < n; j++) {
        eachRow[j] = 2.0;
      }

  #pragma omp ordered
      a.push_back(a_each);
  #pragma omp ordered
      b.push_back(eachRow);
  #pragma omp ordered
      c.push_back(c_each);
    }


    printf("Check if the thread is paralleled as planed\n");
    auto started = std::chrono::high_resolution_clock::now();
  #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n, topology) private(i, j)
    {
      int thread_num = omp_get_thread_num();
      int cpu_num = sched_getcpu();

      printf("Thread %3d is running on CPU %3d, on node %d\n", thread_num, cpu_num, topology[cpu_num]);

  #pragma omp for schedule (static)
      for (i = 0; i < m; i++) {
        for (j = 0; j < n; j++) {
          a[i][0] += (b[i][j] * (*c[i]));
        }
      }  
    }

    auto done = std::chrono::high_resolution_clock::now();
    std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
#+END_SRC

*** To fully control the code behaviour, use C **float instead of C++ vector<float*>
- pointer as array, the following pairs of execution has the same effect
#+BEGIN_SRC c
  printf("the address of a is %ld\n", a);
  printf("*a = %ld\n", *a);
  printf("a[0] = %ld\n", a[0]);

  printf("*(a+1) = %ld\n", *(a+1));
  printf("a[1] = %ld\n", a[1]);
    
  printf("*(*(a+i) + j) = %f\n", *(*(a+1) + 0));
  printf("a[i][j] = %f\n", a[1][0]);

#+END_SRC

- code block which allocate memory, a, b, c are 2 dimensional array, as matrix
  #+BEGIN_SRC c
    float** a;
    float** b;
    float** c;
      
    b = (float**) emalloc(m * sizeof(float*));
    c = (float**) emalloc(m * sizeof(float*));
    a = (float**) emalloc(m * sizeof(float*));
  #+END_SRC

- code block which allocates numa memory on specific node
#+BEGIN_SRC c
  #pragma omp for ordered schedule (static)
  for (i = 0; i < m; i++) {

    cpu_num = sched_getcpu();
    int which_node = topology[cpu_num];
        
    b[i] = (float*)numa_alloc_onnode(n * sizeof(float), which_node);
    a[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);
    c[i] = (float*)numa_alloc_onnode(sizeof(float), which_node);

    if (b[i] == NULL || a[i] == NULL || c[i] == NULL) {
      printf("error during allocation numa memory on node %d\n", cpu_num);
      exit(-1);
    }

    a[i][0] = 0.0;
    c[i][0] = 2.0;
    for (j = 0; j < n; j++) {
      b[i][j] = 2.0;
    }
   }    
  }
#+END_SRC

- code block which does execution of matrix time vector and messures time
 #+BEGIN_SRC c
   #pragma omp parallel proc_bind(close) num_threads(n_cpu) default(none) shared(a,b,c,m,n) private(i, j)
     {
   #pragma omp for schedule (static)
       for (i = 0; i < m; i++) {
         for (j = 0; j < n; j++) {
           ,*(*(a+i) + 0) += ( *(*(b+i) +j) * (*(*(c +j) +0)));
           //      a[i][0] += (b[i][j] * c[j][0]);
         }
       }  
     }

   auto done = std::chrono::high_resolution_clock::now();
   std::cout << "From initialization to finished, use: " << std::chrono::duration_cast<std::chrono::milliseconds>(done-started).count() << "ms" << endl;
 #+END_SRC

**** time mesurement 
#+BEGIN_SRC sh
  cat numa_openmp.sh
  export OMP_PLACES="{0, 4, 8, 12, 16, 20, 24, 28}, {32, 36, 40, 44, 48, 52, 56, 60}, {2, 6, 10, 14, 18, 22, 26, 30}, {34, 38, 42, 46, 50, 54, 58, 62}, {3, 7, 11, 15, 19, 23, 27, 31}, {35, 39, 43, 47, 51, 55, 59, 63}, {1, 5, 9, 13, 17, 21, 25, 29}, {33, 37, 41, 45, 49, 53, 57, 61}"
  ./bin/numa_openmp 16000 16000 8
  ./bin/anti_numa_openmp 16000 16000 8
  ./bin/openmp_without_numa 16000 16000 8
#+END_SRC
|   id | use libnuma(ms) | not use libnuma(ms) | use libnuma on remote node |
|    1 |            3880 |                 420 |                       3934 |
|    2 |            4034 |                 416 |                       3973 |
|    3 |            4020 |                 418 |                       3960 |
|    4 |            3975 |                 419 |                       3951 |
|    5 |            4033 |                 413 |                       3908 |
| mean |          3988.4 |               417.2 |                     3945.2 |
#+TBLFM: @7$2=vmean(@2..@6)::@7$3=vmean(@2..@6)::@7$4=vmean(@2..@6)
- It seems this approach make the libnuma case becomes worse.
- The execution time of using libnuma and the execution time of using libnuma but allocated memory deliberately on remote node is also similar. This indicate I have lot of remote memory access cases.
- This whole program is wrong because each row of the matrix need to multiple very element of vector, there is no point of allocation vector across multiple nodes.

*** OpenMP is not a good choice for doing Matrix times Matrix with NUMA Awareness
**** The initial basic idea of using OpenMP with NUMA
- OpenMP is doing data parallelisation via =for= loop. OpenMP distributes the data across different threads and each thread works on one chunck of data and in the end merge the result.
- NUMA is about memory affinity. If the thread and the data it is accessing is located on the same node, then the accessing speed will be faster than remote accessing.
- Using OpenMP data parallelisation with NUMA awareness. My initial plan is:
  1) first configure OpenMP environment to control where and how OpenMP generate thread based on the =for= loop.
     This is the so called =proc_bind=, such as make sure generated: thread 1 is running on cpu01
     thread 2 is running on cpu02
  2) Based on NUMA topology, such cpu01 is on node01, cpu02 is on node02. Allocate data's memory among different threads, so data's memory is distributed allocated by different threads, which in turn allocated on different cpu ==> on different nodes.
  3) When I do the computation using data. I make sure use the same OpenMP parallel schema, such that the generate threads are as same as before. Thus, each thread will be able to read its part of data locally since the corresponding part of data memory has been allocated on the right node already.
  4) This idea need to make sure the size of different data matches each other.
- Case study why this doesn't work, on matrix time matrix:
  - suppose matrix m which is 2 * 5 , times matrix n which is 5 * 2.

  - allocate memory using OpenMP, in the loop, allocate each row of m on different threads while allocate each column of n on the same different threads. Then initalize each element.
  - For the result matrix w which 2 * 2. Then it has the following problem:
    | 00 | 01 |
    |----+----|
    | 10 | 11 | 
    - if w is allocated row by row, which means 00, 01 is on node0, 10 and 11 is on node1. Then during computation, remote access will happend on 01(on node0) since it is the result of the first row of m which is on node0 times the second column of n which is on node 1. Similar thing happened on 10.
    - In fact, only left to right diagonal element which is refered in double loop as i == j can avoid remote access.

In general, data parallelisation in the case of matrix time matrix could not be computed efficiently using OpenMP with NUMA awareness configuration. 

  Data parallelisation with NUMA awareness could be hard since any interaction part in the parallel region will require access from remote nodes(because you generate threads on different CPUs => different nodes). Thus, the result will be similar with averagely allocation memory across different nodes which is the default setting for NUMA. The more frequently the interaction is, the less effect of NUMA allocation will be. It is not about how to allocate memory with NUMA, it is about the interaction between different parts of data which is inherited from the algorithm.
  
  So, a simple "solution" will be make multiple copy of data and let each thread works on its own corresponding copy. That is a kind of  task parallelism and I doult about the practical usage of it. 
