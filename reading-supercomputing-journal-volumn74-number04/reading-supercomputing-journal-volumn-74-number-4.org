#+title: reading notes for supercomputing journal value 74 no. 4
#+latex_header: \usepackage{hyperref}
#+latex_header: \usepackage{cleveref}
#+latex_header: \usepackage{xcolor}
#+latex_header: \usepackage{amsmath}
#+latex_header: \hypersetup{colorlinks=true}


* Parallel implementations of the 3D fast wavelet transform on a Raspberry Pi 2 cluster cite:bernabe16_paral_implem_fast_wavel_trans
Present and evaluate 3 parallelization strategies of the 3d fast wavelet transform on a cluster of Raspberry Pi 2 SDCs. Using booth pthread and MPI.

** Introduction
- In this work, they study the parallelization of the 3d fast wavelet transform(3D-FWT) on a cluster of raspberry pi 2 SDC(single-board computer)s. Using pthread and MPI for evaluating 3 different parallelization strategies on a cluster of 4 Pis.
- Pthread version are restricted to runs on a single boards, and MPI version can span to several Pis.
- Performance drops when all MPI processes spread to several boards due to the the limited bandwidth of the onboard LAN port.
- Overall, they have shown Raspberry Pi 2 SDC reveals as an appealing platform for giving support to 3D-FWT-based applications with low-cost and energy efficiency requirements.
* A mathematical model to calculate real cost/performance in software distributed shared memory on computing environments cite:khaneghah17_mathem_model_to_calcul_real
** Introduction
- cost/performance ratio is an important factors in HPC(high-performance computing), which acts as an economic justification for running scientific program on HPC system.
- This ratio parameter specifies the type of scientific program that can run in HPC systems like Cluster, Grid, and Peer-to-Peer (P2P) cite:thackston15_perfor_low_cost_commer_cloud.
- cost calculate of a system includes: the execution of applications in HPC system, the mechanisms used to calculate the cost(two method).
  1) The first solution is the mathematical model of calculating cost and efficiency of each scientific program or computing system management application or any special feature of the computing system.

     The most important feature of the minor fee calculation is the process of using program execution for the calculation of cost and the cost/performance ratio.
     The most important advantage of this method is the feature of proposing the exact cost of a program execution in certain computing systems.

  2) The second solution is using a public pattern to calculate the cost and mentioned coefficient in computing systems.
- One of the most important questions that should be answered while calculating the inter-process cost in computing systems is the cost calculation pattern.
- IPC (inter-process communication)
- DSM (distributed shared memory)
** Related work
- SMP (symmetric multi-parallel)
*** Review of distributed shared memory based on system approach
*** General parameters of cost in DSM system based on system approach

(...boring topic for now)

* An efficient anonymous authentication protocol in multiple server communication networks (EAAM)  cite:braeken17_effic_anony_authen_protoc_multip
Propose an efficient anonymous authentication protocal in multiple server communication networks, called EAAM protocal, which is able to establish user anonymity, mutual authentication, and resistance against know security attacks.

THe novelty of the proposed scheme is that it does not require a secure channel during the registration between the user and the registration center and is resistant to a curious but honest registration system.

** Conclusion
The main novelty of the protocol is that it provides resistance against a honest-but-curious RC(registration center).

(not very relavent to parallel computing)


* Strategy for data-flow synchronizations in stencil parallel computations on multi-/manycore systems cite:szustak18_strat_data_flow_synch_stenc
An innovative strategy for the data-flow synchronization in shared-memory system is proposed. This trategy assumes to synchronize only interdependent threads instead of using the barrier approach.
Their proposed approache is evaluated for various Intel microarchitectures and done comparision with OpenMP barrier. It show it is better for 1.3 times.

** Introduction
The main idea of this strategy is to synchronize only interdependent threads instead of using the barrier approach that—in contrast to our approach—synchronize all threads. An inseparable part of this strategy is the scheme of thread interrelationships for a given application. In fact, the data dependencies, workload distribution, way of parallelization and inter-thread data traffic play a key role in the effective adaptation of this strategy to a given application.

- the state-of-the-art synchronization alogrithms differ in trade-off between communication complexity, length of the critical path and memory footprint cite:braeken17_effic_anony_authen_protoc_multip. 
- The barriers are an essential sychronization approach for parallel models of many shared-memory programming languages such as OpenMP, OpenCL or Cilk. They can be grouped into three categories: centralized, tree and butterfly.
- Each synchronization algorithm features its own set of trade-off, where the areal profit is largely dependent on the structure of a computing system.

The main aim of this work is to avoid global barriers: the synchronization process should proceed only between carefully selected threads that depend on each others. An justification and study of related work meeting this challenge can be found in work cite:bhatti13_effic_synch_stenc_comput_using.
- what is dynamic task graph?

- An other synchronization strategie: Data-flow communication layers are very popular in distributed-memory programming standards, including MPI or hStreams programming library 
- In both cases, the synchronization between the interdepen- dent processing elements is explicitly defined according to communication flows of data, using the specific commands such as MPI_Send and MPI_Recv in the case of MPI. 

So, there are two strategies for synchronizations:
1) barrier based, used mainly in shared-memory model 
2) data flow based, used mainly in distributed-memory model 

The author use data flow based, in shared-memory model.

* Language-based vectorization and parallelization using intrinsics, OpenMP, TBB and Cilk Plus  cite:stpiczynski18_languag_based_vector_paral_using

This paper evaluate OpenMP, TBB and Cilk Plus as basic language-based tools for simple and efficient parallelization of recursively defined computatinal problems and other problems that need both task and data parallelization techniques.

Show how to use these models to utilize multiple cores of modern processes.
- tuning data structures for better utilization of vector extensions of modern processors.
- Manual vectorization techniques based on Cilk
- Intel SIMD Data Layout Template containers

** Introduction
Intel C/C++ compilers and development tools offer many language-based extensions that can be used to simplify the process of developing high-performance parallel programs.
- OpenMP
- Threading Building BLocks (TBB)
- Cilk Plus
- /intrinsics/, which all to utilize Intel Advanced Vector Extensions explicitly
- SDLT template library can be applied to introduce SIMD-friendly memory layout transparently

** Short overview of selected language-based tools
- OpenMP
- [[https://www.threadingbuildingblocks.org][TBB]] is a C+++ template library supporting task parallelism on Intel multicore platformcs.
- [[https://www.cilkplus.org][Cilk Plus]] adds simple language extensions to the C and C++languages to express task and data parallelism.
- Intrinsics for SIMD instructions allow to take full advantage of Intel Advanced Vector Extensions what cannot always be easily achieved due to limitations of programming languages and compilers. They all programmers to write constructs that look like C/C++ functions calls corresponding to actual SIMD instructions.
- SDLT (SIMD Data Layout Template) is a C++11 template library which provides containers with SIMD-friendly data layouts.
** Conclusion
- They compare the speedup of the different implementation against sequential version of code. Depend on whether it is data parallel or task parallel, the performance varys be
* Parallelization of stochastic bounds for Markov chains on multicore and manycore platforms cite:bylina18_paral_stoch_bound_markov_chain
Demonstrates the methodology for parallelizing of finding stochastic bounds for Markov chains on multicore and manycore platformcs
- involve a lot of irregular memory access.
- using OpenMP(for loop parallelism) and Cilk Plus (for task-based parallelism)
- compare the execution time and scalability

** Experimental results
- time
- speedup
** Conclusion
This paper presents the strength of the OpenMP standard for parallelizing with the use of ~#pragma omp parallel for~ which is data parallelism in OpenMP.

** Things to do:
need to state clearly what is the main differences between task parallelism and loop (data) parallelism.

* A taxonomy of task-based parallel programming technologies for high-performance computing cite:thoman18_taxon_task_based_paral_progr

Provide an initial task-focused taxonomy for HPC technologies, which covers programming interface and runtime mechanisms.

** Introduction
- In HPC domain, loop-based and message-passing paradigms are dominant. We specifically aim to categorize task-based parallelism technologies which are in use in HPC
- Definition of task: a sequence of instructions within a program that can be processed concurrently with other tasks in the same program.
- Several languages are common in HPC domain:
  1) Cilk 
  2) OpenMP 
  3) TBB 
  4) Qthreads
  5) Argobots
  6) StarGPU
  7) Chapel
  8) X10
  9) HPX
  10) Charm++

- Each task-based environment has two central components:
  1) programming interface
  2) runtime system




* Hybridworkstealingoflocality-flexibleandcancelabletasksfortheAPGAS library  cite:posner18_hybrid_work_steal_local_flexib
** Abstract
- parallel programs should be albe to deal with both shared memory and distributed memory
- propose a hybrid work stealing scheme, which combines the lifeline-based variant of distributed task pools with the node-internal load balancing of Java's Fork/Join framework.
- [[http://x10-lang.org/releases/apgas-release-100.html][APGAS]] library for Java, which is a branch of the [[http://x10-lang.org][X10]] project.
* Pythonacceleratorsforhigh-performancecomputing cite:marowka17_python_accel_high_perfor_comput
** Abstract
- python is popular and is slow; python community drive the effort to improve the performance of it.
- focus on specific promised solution that aim to provide high-performance and performance portability for python applications, specially [[http://numba.pydata.org][Numba]].
** Python accelerators
A few popular solutions that enhance python's performance
- Numpy
- SciPy, extends the functionality of NumPy
- PyPy, a just-in-time compiler and interpreter for Python. It aims to provide faster efficient and compatible alternative implementation of Python language.
- Cython, enabling decarations of static typing to functions, variables, and classes, allows C code to be generated once and then compiles with C/C++ compilers to produce efficient C code.
- Numexpr, a module which accelerate evaluations of a numerical expression operation on NumPy.

** Numba in a nutshell
** Test case: matrix-matrix multiplication

* Actor model of Anemone functional language cite:batko18_actor_model_anemon_funct_languag 
** Abstract
not be able to download yet
* A process calculus for parallel and distributed programming in Haskell cite:bloecker18_pardis
** Abstract
- parallel programming and distributed programming is hard to implement, result in non-deterministic program behaviour.
- gap between model and implementation
- propose a fully determinisitc process calculus for parallel and distributed programming and implement it as a domain-specific language in Haskell to address these problems.
- achieve correctness guarantees regarding process composition at compile time through Haskell's type system.
- Their result could be used as a high-level tool to implement parallel and distributed programs.

(to read)
* Function portability of molecular dynamics on heterogeneous parallel architectures with OpenCL  cite:halver18_funct_portab_molec_dynam_heter
** Abstract
- evaluate latency, data transfer, memory access characterisitcs of parallel compute intense work
- data layout, for which the access of structure-of-arrays shows best performance in most cases.
- performance portability is a problem since various architectures strongly depend on specific vectorization optimization.

** Conclusion
- One of the main goals of the present work was to investigate the performance character- istics of a function portable cell-based MD program on a set of different architectures.
- OpenCL has been chosen because it allows for interoperability on different types of architectures. Without any changes of the code it was possible to execute a benchmark on several multi- and many-core systems. Compared severl features:
  1) memory bandwidth
  2) core speed 
  3) effects of data structure layout (an important issue for GPU architectures)
     - arrays-of-structures
     - structure-of-arrays 
- Current their research focus on function portability, A desirable feature would be performance portability

* Bibliography
bibliographystyle:abbrv
bibliography:../parallel-numa.bib 



