% Created 2018-04-17 Tue 13:01
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{xcolor}
\usepackage{amsmath}
\hypersetup{colorlinks=true}
\author{zwpdbh}
\date{\today}
\title{2018-04-12 reading notes for OpenMP NUMA}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.3.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents


\section{OpenMP on NUMA architectures}
\label{sec-1}
\subsection{Investigating NUMA topologies}
\label{sec-1-1}
\begin{itemize}
\item \verb~numactl --hardware~ print information on NUMA nodes in the system
\end{itemize}
\begin{verbatim}
wzhao@r815:~$ numactl --hardware
available: 8 nodes (0-7)
node 0 cpus: 0 4 8 12 16 20 24 28
node 0 size: 65526 MB
node 0 free: 63750 MB
node 1 cpus: 32 36 40 44 48 52 56 60
node 1 size: 65536 MB
node 1 free: 63869 MB
node 2 cpus: 2 6 10 14 18 22 26 30
node 2 size: 65536 MB
node 2 free: 63860 MB
node 3 cpus: 34 38 42 46 50 54 58 62
node 3 size: 65536 MB
node 3 free: 63861 MB
node 4 cpus: 3 7 11 15 19 23 27 31
node 4 size: 65536 MB
node 4 free: 63863 MB
node 5 cpus: 35 39 43 47 51 55 59 63
node 5 size: 65536 MB
node 5 free: 63869 MB
node 6 cpus: 1 5 9 13 17 21 25 29
node 6 size: 65536 MB
node 6 free: 63871 MB
node 7 cpus: 33 37 41 45 49 53 57 61
node 7 size: 65520 MB
node 7 free: 63836 MB
node distances:
node   0   1   2   3   4   5   6   7 
0:  10  16  16  22  16  22  16  22 
1:  16  10  22  16  16  22  22  16 
2:  16  22  10  16  16  16  16  16 
3:  22  16  16  10  16  16  22  22 
4:  16  16  16  16  10  16  16  22 
5:  22  22  16  16  16  10  22  16 
6:  16  22  16  22  16  22  10  16 
7:  22  16  16  22  22  16  16  10
\end{verbatim}
\begin{itemize}
\item \verb~numactl --show~ prints information on available resources for the process
\end{itemize}
\begin{verbatim}
wzhao@r815:~$ numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 
cpubind: 0 1 2 3 4 5 6 7 
nodebind: 0 1 2 3 4 5 6 7 
membind: 0 1 2 3 4 5 6 7
\end{verbatim}

\begin{itemize}
\item use \verb~lstopo~ show the system topology
\end{itemize}
\subsection{Optimizing NUMA accesses}
\label{sec-1-2}
Goal: minimize the number of remote memory accesses as much as possible.
\begin{itemize}
\item how are threads distributed on the system?
\item how is the data distributed on the system?
\item how is work distributed across thread?
\end{itemize}

\subsection{Thread placement in OpenMP}
\label{sec-1-3}
\subsubsection{Selecting the right binding strategy depends not only on the topology, but also on the characteristics of the application.}
\label{sec-1-3-1}
\begin{enumerate}
\item Putting threads far part, on different sockets
\label{sec-1-3-1-1}
\begin{itemize}
\item May improve the aggregated memory bandwidth available to your application.
\item May improve the combined cache size available to your application.
\item May decrease performance of synchronization constructs
\end{itemize}
\item Putting threads close together, i.e. on two adjacent cores which possibly shared some caches
\label{sec-1-3-1-2}
\begin{itemize}
\item May improve performance of synchronization constructs
\item May decrease the available memory bandwidth and cache size
\end{itemize}
\end{enumerate}
\subsubsection{Available strategies}
\label{sec-1-3-2}
\begin{itemize}
\item close: put threads close together on the system
\item spread: place threads far apart from each other
\item master: run on the same place as the master thread
\end{itemize}

Assume the following machine:
\begin{figure}[htb]
\centering
\includegraphics[width=2.0in]{/Users/zw/Documents/screenshots/numa-nodes.png}
\caption{\label{fig:numa-example-01}numa-example, 2 sockets, 4 cores per socket, 4 hyper-threads per core}
\end{figure}
\subsubsection{Abstract names for  \texttt{OMP\_PLACES}}
\label{sec-1-3-3}
\begin{itemize}
\item threads: each place corresponds to a single hardware thread on the target machine.
\item cores: each place corresponds to a single core (having one or more hardware threads) on the target machine.
\item sockets: each place corresponds to a single socket (consisting of one or more cores) on the target machine.
\end{itemize}

\begin{enumerate}
\item Example's objective
\label{sec-1-3-3-1}
\begin{itemize}
\item Separate cores for outer loop and near cores for inner loop
\end{itemize}
\begin{verbatim}
OMP_PLACES={0,1,2,3}, {4,5,6,7}, ... = {0:4}:8:4
#pragma omp parallel proc_bind(spread)
#pragma omp parallel proc_bind(close)
\end{verbatim}
\begin{itemize}
\item \texttt{OMP\_PLACES}
\item \texttt{proc\_bind}
\end{itemize}
\end{enumerate}
\subsection{Data Placement}
\label{sec-1-4}
\begin{itemize}
\item OpenMP does not provide support for cc-NUMA
\item Placement comes from the operating system (Operating system dependent)
\item By default, use the "First Touch" placement policy
\end{itemize}

\subsubsection{First touch in action}
\label{sec-1-4-1}
\begin{enumerate}
\item In serial code, all array element are allocated in the memory of the NUMA node containing the core executing this thread
\label{sec-1-4-1-1}
\begin{verbatim}
double* A;
A = (double*) malloc(N * sizeof(double));
for (int i = 0; i < N; i++) {
  A[i] = 0.0;
 }
\end{verbatim}

\begin{figure}[htb]
\centering
\includegraphics[width=.9\linewidth]{/Users/zw/Documents/screenshots/numa-serial-init.png}
\caption{\label{fig:numa-serial-init}serial code first toucle example}
\end{figure}

\item In parallel code
\label{sec-1-4-1-2}
\begin{verbatim}
double* A;
A = (double*)malloc(N * sizeof(double));
omp_set_num_threads(2);
#pragma omp parallel for
for (int i = 0; i < N; i++) {
  A[i] = 0.0;
 }
\end{verbatim}

\begin{figure}[htb]
\centering
\includegraphics[width=.9\linewidth]{/Users/zw/Documents/screenshots/numa-parallel-init.png}
\caption{\label{fig:numa-parallel-init}parallel code first touch example}
\end{figure}
\end{enumerate}

\subsection{Memory and thread placement in Linux}
\label{sec-1-5}
\verb~numactl~ command line tool to investigate and handle NUMA under Linux
\begin{itemize}
\item \verb~numactl --cpunodebind 0,1,2 ./a.out~, only use cores of NUMA node 0-2 to execute a.out
\item \verb~numactl --physcpubind 0-17 ./a.out~, only use cores 0-17 to execute a.out.
\item \verb~numactl --membind 0,3 ./a.out~, only use memory of NUMA node 0 and 3 to execute a.out.
\item \verb~numactl --interleave 0-3 ./a.out~, distribute memory pages on NUMA nodes 0-3 in a round-robin fashion, overwrite first-touch policy.
\end{itemize}

\verb~libnuma~ library of NUMA control
\begin{itemize}
\item \texttt{void *numa\_alloc\_local(size\_t size)}, allocate memory on the local NUMA node.
\item \texttt{void *numa\_alloc\_onnode(size\_t size, int node)}, allocate memory on NUMA node node.
\item \texttt{void *numa\_alloc\_interleaved(size\_t size)} allocate memory distributed round-robin on all NUMA nodes.
\item \textasciitilde{}int numa$_{\text{move}}$$_{\text{pages}}$(int pid, unsigned long count, void **pages, const int *nodes, int *status, int flags)=, migrate memory pages at runtime to different NUMA nodes.
\end{itemize}
\section{OpenMP overview}
\label{sec-2}
\subsection{OpenMp Compilation process}
\label{sec-2-1}
\begin{itemize}
\item Annotated source code -> OpenMP compiler -> parallel object code
\item Compiler can also generate sequential object cde
\item compiler front end: parse OpenMP directives, correctness checks
\item compiler back end: replace constructs by calls to runtime library, change structure of program
\end{itemize}
\subsection{Notation}
\label{sec-2-2}
\subsubsection{Syntax}
\label{sec-2-2-1}
\begin{itemize}
\item directive: \verb~pragma~ statement
\item runtime library routine: function defined in \verb~omp.h~
\item structured block: simgle statement or compound statement with a single entry at the top and a single exit at the bottom.
\item clause: modifies a directive's behavior
\item Directives combined with code form a construct, which is a pattern to accomplish something. Such as a parallel construct is a \verb~parallel~ directive, with its optional clauses, and any code to be executed.
\item Environment variable: defined outside the program
\end{itemize}
\subsubsection{Notation (OpenMP)}
\label{sec-2-2-2}
\begin{itemize}
\item master thread: original thread
\item slave thread: all additional threads
\item team: master thread + slave thread
\end{itemize}
\subsubsection{Scope of Variables}
\label{sec-2-2-3}
\begin{itemize}
\item shared scope
variable can be accessed by all threads in team, variables declared outside a structured block following a parallel directive
\item private scope
variable can be accessed by a single thread, variable declared inside a structured block following a parallel directive.
\end{itemize}
\begin{enumerate}
\item Handout only: scope of variables
\label{sec-2-2-3-1}
\begin{itemize}
\item private variables are uninitialized
\item initialize variables with value from master thread: \verb~firstprivate~
\item \textasciitilde{}default(none)= requires programmer to specify visibility for all variables implicitly, good practice.
\end{itemize}
\end{enumerate}
\subsubsection{\texttt{parallel for} directive}
\label{sec-2-2-4}
\begin{itemize}
\item run loop interations in parallel
\item shortcut: =\#pragma omp parallel for\textasciitilde{}
\item loop iterations must be data-independent
\item OpenMP must be able to determine the number of iterations before the loop is executed.
\item Mapping of iterations to threads controlled by \verb~schedule~ clause
\begin{itemize}
\item schedule(static [, chunksize]): block of chunksize iterations statically assigned to thread
\item schedule(dynamic [, chunksize]): thread reserves chunksize iterations from queue
\item schedule(guided [, chunksize]): same as dynamnic, but chunk size starts big and gets smaller and smaller, until it reaches chunksize.
\item schedule(runtime): scheduling behavior determined by environment variable
\end{itemize}
\end{itemize}
\subsubsection{\texttt{reduce} clause}
\label{sec-2-2-5}
\subsubsection{\texttt{critical} sections}
\label{sec-2-2-6}
\subsubsection{\texttt{Atomic} statements}
\label{sec-2-2-7}
\subsubsection{More synchronization constructs}
\label{sec-2-2-8}
\begin{itemize}
\item \texttt{\#pragma omp barrier}: wait until all threads arrive
\item =\#pragma omp for nowait\textasciitilde{}: remove implicit barrier after for loop (also exists for other directives)
\item =\#pragma omp master\textasciitilde{}: only executed by master thread
\item =\#pragma omp single\textasciitilde{}: only executed by one thread
\item Sections: define a number of blocks, every thread executes one block
\item Locks: \texttt{omp\_init\_lock()}, \texttt{omp\_set\_lock()}, \texttt{omp\_unset\_lock()},\ldots{}
\end{itemize}

\subsection{ForestGOMP: NUMA with OpenMP}
\label{sec-2-3}
\begin{itemize}
\item Objectives and motivations
\begin{enumerate}
\item Keep buffer and threads operating on them on the same NUMA node (reducing contention)
\item Processor level: group threads sharing data intensively(improve cache usage)
\end{enumerate}
\item Triggers for scheduling
\begin{enumerate}
\item Allocation/deallocation of resources
\item Processor becomes idle
\item Change of hardware counters (e.g., cache miss, remote acess rate)
\end{enumerate}
\end{itemize}
\subsubsection{BubbleSched: hierarchical buble-based thread scheduler}
\label{sec-2-3-1}
\begin{itemize}
\item Runqueue for different hierarchical levels
\item Bubble: group of threads sharing data or heavy synchronization
\item Responsible for scheduling threads
\end{itemize}
\subsubsection{Mami: NUMA-aware memory manager}
\label{sec-2-3-2}
\begin{itemize}
\item API for memory allocation
\item Can migrate memory to a different NUMA node
\item Support next touch policy: migrate data to NUMA node of accessing thread.
\begin{itemize}
\item Buffers are marked as migrate-on-next-touch when a thread migration is expected
\item Buffer is relocated if thread thouches buffer that is not located on local node
\item Implemented in kernel mode
\end{itemize}
\item ForestGOMP: Mami-aware OpenMP Runtime
\begin{itemize}
\item Mami attaches memory hints: e.g., which regions are access frequently by a certain thread
\item Initial distribution: put thread and corresponding memory on same NUMA node (local accesses)
\item Handle idleness: steal threads from local core, then from different NUMA node (also migrates memory; prefers threads with less memory)
\item Two levels of distribution: memory-aware, then cache-aware
\end{itemize}
\end{itemize}
\section{ForestGOMP: an efficient OpenMP environment for NUMA architectures \cite{broquedis10_fores}}
\label{sec-3}
\section{OpenMP task scheduling strategies for multicore NUMA systems \cite{olivier12_openm_task_sched_strat_multic_numa_system}}
\label{sec-4}
\subsection{Abstract}
\label{sec-4-1}
\begin{itemize}
\item Efficient scheduling of tasks on modern multi-socket multicore shared memory system requires consideration of shared caches and NUMA characteristics.
\item They extendent the open source Qthreads threadling library to implement different scheduler designs, accepting OpenMP programs through the ROSE compiler.
\end{itemize}
\subsection{Introduction}
\label{sec-4-2}
\begin{itemize}
\item What is task-parallel programming models? what are the benefits?
\item Efficient task scheduler
\begin{itemize}
\item exploit cache and memory locality
\item maintain load balance
\item minimize overhead costs
\end{itemize}
Trade off between them: 
\begin{itemize}
\item However, load balancing operations can also contribute to overhead costs. Load balancing operations between sockets increase memory access time due to more cold cache misses and more high-latency remote memory accesses.
\end{itemize}
\end{itemize}
\subsubsection{Their contributions}
\label{sec-4-2-1}
\begin{enumerate}
\item A hierarchical scheduling strategy targeting modern multi-socket multicore shared memory systems.
\label{sec-4-2-1-1}
\begin{itemize}
\item NUMA architecture is not well supported by work-stealing scheduler with one queue per core or by centralized scheduler.
\item work-stealing, a scheduling strategy for multithreaded computer programs. It solved the problem of executing a dynamically multithreaded commputation, one that can "spawn" new threads of execution, on a statically multithreaded computer, with a fixed number of processors. 
See \cite{blumofe99_sched_multit_comput_by_work_steal}, a important paper.
Also, see paper \cite{beaumont06_centr} for comparsion.
\end{itemize}

\item A detailed performance study on a current generation multi-socket multicore Intel system
\label{sec-4-2-1-2}
\item Additional performance evaluation on a two-socket multicore AMD system and a 192-processor SGI Altix
\label{sec-4-2-1-3}
\end{enumerate}

\subsection{Background}
\label{sec-4-3}
\begin{itemize}
\item OpenMP 3.0 explicit task parallelism to complement its exisiting data parallel constructs.
\item ROSE compiler is used for performing syntactic and semantic analysis on OpenMP directives, transforming them into run-time library calls in the intermediate program.The ROSE common OpenMP run-time library maps the run-time calls to function in the Qthreads library.
\item Qthreads vs Pthread, why Qthreads is needed in their paper?
\begin{itemize}
\item Each worker pthread is pinned to a processor core and assigned to a locality domain, termed a shepherd.
\item what is FEB operation, a contex switch is triggered.
\end{itemize}
\item "We used the Qthreads queueing implementation as a starting point for our scheduling work."
\item "We implement OpenMP threads as worker pthreads. Unlike many OpenMP implementations, default loop scheduling is self-guided rather than static."
\item "For task par- allelism, we implement each OpenMP task as a qthread."
\item "We used the Qthreads FEB synchronization mechanism as a base layer upon which to implement taskwait and barrier sychronization."
\end{itemize}

\subsection{Conclustion}
\label{sec-4-4}
\begin{itemize}
\item Their MTS scheduler, combination of shared LIFO queues and work stealing maintains good load balance while supporting effective cache performance and limiting overhead cost. Notice: pure work stealing has been shown to provide the least variability in performance which is an important consideration for distributed applications in which barriers cause the application to run at the speed of the slowest worker.
\item One challenge posed by their hierarchical scheduling strategy is the need for an efficient queue supporting concurrent access on both end, since works within a shepherd share a queue. (Lock-free dequeue).
\end{itemize}
\section{OpenMP Extension for Explicit Task Allocation on NUMA Architecture \cite{10.1007/978-3-319-45550-1_7}}
\label{sec-5}
\subsection{Abstract}
\label{sec-5-1}
In this paper, we propose an extension for the OpenMP task construct to specify the loca- tion of tasks to exploit the locality in an explicit manner. The prototype compiler is implemented based on GCC.

\subsection{Introduction}
\label{sec-5-2}
\begin{itemize}
\item In the early version of OpenMP, the programming model had focused on data parallelism described by loop work sharing, which requires global synchro- nization in a parallel region. When the number of cores increases, synchronization overhead is getting bigger, and load imbalance among cores causes a significant performance drop.
\item In OpenMP 4.0, task dependency can be specified using the depend clause in the task construct. Task parallelism can exploit potential parallelism in irregular applications. Task dependency can reduce synchronization overhead because it generates fine-grain synchronization between dependent tasks.
\item To exploit memory bandwidth with NUMA architectures, OpenMP provides thread affinity options.
\begin{itemize}
\item For OpenMP4.5, the \texttt{proc\_bind} clause is discussed to specify a thread affinity scheme for a parallel region. These can be helpful to improve data locality when performing data parallelism with loop work sharing.
\item However, the current specification lacks functionality to do the same thing for task parallelism.
\end{itemize}
\item An OpenMP extension to describe NUMA-aware task allocation explicitly. The extension specifies the data that the target task would access.
\end{itemize}

\subsection{Related work}
\label{sec-5-3}
\begin{itemize}
\item Some NUMA-aware task scheduler based on work-stealing, see \cite{vikranth13_topol_aware_task_steal_chip,DBLP:journals/corr/Tahan14,drebes14_topol_aware_depen_aware_sched,olivier12_openm_task_sched_strat_multic_numa_system}
\item Manual data distribution among NUMA nodes and their NUMA-aware task scheduling algorithm in runtime. \cite{muddukrishna15_local_aware_task_sched_data}
\item their is similar, also requeires explicit data distribution. However, task allocation is done explicityly using the extended OpenMP task construct.
\end{itemize}

\subsection{OpenMP Extension for NUMA-Aware Task Allocation}
\label{sec-5-4}
Generally, improving data locality and reducing remote memory access can exploit potential memory performance on the NUMA architecture. The same is true for task parallelism in OpenMP. A task should be executed on the NUMA node where its processing data is allocated to get the highest memory bandwidth. They propose a new clause named \verb~node_bind~ for OpenMP task construct. It specifies a NUMA node that the target task should be scheduled.

(to be continued)
\section{\href{http://man7.org/linux/man-pages/man3/numa.3.html}{Linux libnuma}}
\label{sec-6}
\subsection{Description}
\label{sec-6-1}
\subsubsection{It offers a programming interface to the NUMA policy. Available policies are:}
\label{sec-6-1-1}
\begin{itemize}
\item page interleaving (allocated in a round-robin fashion from all)
\item subset of the nodes on the system
\item preferred node allocation (preferably allocate on a particular node)
\item local allocation (allocate on the node on which the task is currently executing)
\item allocation only on specific nodes
\end{itemize}

\subsubsection{Note:}
\label{sec-6-1-2}
The default memory allocation policy for tasks and all memory range is local allocation.
For setting a specific policy globally for all memory allocations in a process and its children, it is easiest to start it with the \href{http://man7.org/linux/man-pages/man8/numactl.8.html}{numactl} utility.
All numa memory allocation policy only takes effect when a page is actually faulted into the address space of a process by accessing it. (First touch policy)

\section{Links related with C++ and libnuma}
\label{sec-7}
\subsection{\href{https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/}{C++11 threads, affinity and hyperthreading}}
\label{sec-7-1}
\subsubsection{Background and introduction}
\label{sec-7-1-1}
This post use C+++ threads as the main threading mechanism to demonstrate its points.
\subsubsection{Logical CPUs, cores and threads}
\label{sec-7-1-2}
\begin{itemize}
\item Modern machines are multi-CPU, where these CPUs are divided into sockets and hardware cores. But the OS sees a number of "logical" CPUs that can execute tasks concurrently.
\item To get such information on Linux, run \verb~cat /proc/cpuinfo~, a summary output can be obtained from \verb~lscpu~
\end{itemize}
\subsubsection{Launching a thread per CPU}
\label{sec-7-1-3}
\begin{verbatim}
int main(int argc, const char** argv) {
  unsigned num_cpus = std::thread::hardware_concurrency();
  std::cout << "Launching " << num_cpus << " threads\n";

  // A mutex ensures orderly access to std::cout from multiple threads.
  std::mutex iomutex;
  std::vector<std::thread> threads(num_cpus);
  for (unsigned i = 0; i < num_cpus; ++i) {
    threads[i] = std::thread([&iomutex, i] {
	{
	  // Use a lexical scope and lock_guard to safely lock the mutex only for
	  // the duration of std::cout usage.
	  std::lock_guard<std::mutex> iolock(iomutex);
	  std::cout << "Thread #" << i << " is running\n";
	}

	// Simulate important work done by the tread by sleeping for a bit...
	std::this_thread::sleep_for(std::chrono::milliseconds(200));

      });
  }

  for (auto& t : threads) {
    t.join();
  }
  return 0;
}
\end{verbatim}

\subsubsection{Detour - thread IDs and native handles}
\label{sec-7-1-4}
The thread library also lets us interact with platform-specific threading APIs by exposing native handles. 
Here's an example program that launches a single thread, and then queries its thread ID along with the native handle:
\begin{verbatim}
int main(int argc, const char** argv) {
  std::mutex iomutex;
  std::thread t = std::thread([&iomutex] {
      {
	std::lock_guard<std::mutex> iolock(iomutex);
	std::cout << "Thread: my id = " << std::this_thread::get_id() << "\n"
		  << "        my pthread id = " << pthread_self() << "\n";
      }
    });

  {
    std::lock_guard<std::mutex> iolock(iomutex);
    std::cout << "Launched t: id = " << t.get_id() << "\n"
	      << "            native_handle = " << t.native_handle() << "\n";
  }

  t.join();
  return 0;
}
\end{verbatim}

\subsubsection{Setting CPU affinity programatically}
\label{sec-7-1-5}
As we've seen earlier, command-line tools like taskset let us control the CPU affinity of a whole process. Sometimes, however, we'd like to do something more fine-grained and set the affinities of specific threads from within the program. How do we do that?
Use \texttt{pthread\_setaffinity\_np}:  Here is the example which pin each thread to a single know CPU by settng its affinity:
\begin{verbatim}
int main(int argc, const char** argv) {
  constexpr unsigned num_threads = 4;
  // A mutex ensures orderly access to std::cout from multiple threads.
  std::mutex iomutex;
  std::vector<std::thread> threads(num_threads);
  for (unsigned i = 0; i < num_threads; ++i) {
    threads[i] = std::thread([&iomutex, i] {
	std::this_thread::sleep_for(std::chrono::milliseconds(20));
	while (1) {
	  {
	    // Use a lexical scope and lock_guard to safely lock the mutex only
	    // for the duration of std::cout usage.
	    std::lock_guard<std::mutex> iolock(iomutex);
	    std::cout << "Thread #" << i << ": on CPU " << sched_getcpu() << "\n";
	  }

	  // Simulate important work done by the tread by sleeping for a bit...
	  std::this_thread::sleep_for(std::chrono::milliseconds(900));
	}
      });

    // Create a cpu_set_t object representing a set of CPUs. Clear it and mark
    // only CPU i as set.
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(i, &cpuset);
    int rc = pthread_setaffinity_np(threads[i].native_handle(),
				    sizeof(cpu_set_t), &cpuset);
    if (rc != 0) {
      std::cerr << "Error calling pthread_setaffinity_np: " << rc << "\n";
    }
  }

  for (auto& t : threads) {
    t.join();
  }
  return 0;
}
\end{verbatim}
\begin{itemize}
\item use the \verb~native_handle~ method in order to pass the underlying native handle to the pthread call (it takes a \verb~pthread_t~ ID as its first argument).
\end{itemize}

\subsubsection{Sharing a core with hyperthreading}
\label{sec-7-1-6}
To see the topology of processor on Linux:
\begin{enumerate}
\item \verb~lstopo~
\item An alternative non-graphical way to see which threads share the same core is to look at a special system file that exists per logical CPU. For example, for CPU 0:
\verb~cat /sys/devices/system/cpu/cpu0/topology/thread_siblings_list~
\end{enumerate}

The hardware thread explain: for example, a processor has 4 cores, each with 2 threads , for a total of hardware 8-threads -- 8 logical CPUs for the OS.

Server-class processors will have multiple sockets, each with a multi-core CPU.For example, a machine with 2 sockets, each of which is a 8-core CPU with hyper-threads enable: a total 2 * 8 * 2 = 32 hardware threads.


\subsubsection{Performance demos of core sharing vs separate cores}
\label{sec-7-1-7}
(a benchmark which do data parallelism could be used \href{https://github.com/eliben/code-for-blog/tree/master/2016/threads-affinity}{here})
He do some interesting experiment which shows sometimes running multiple threads on the same core actually hurts it. The reason is threads could compete over the execution units of the core and slow each other down.

\subsubsection{Summary}
\label{sec-7-1-8}
\begin{itemize}
\item how to examine and set thread affinity
\item how to control placement of threads on logical CPUs by using the C++ standard threading library in conjunction with POSIX calls, and the bridging native handles exposed by the C++ threading library for this purpose.
\item Different workloads have very different CPU utilization characteristics, which makes them more or less suitable for sharing a CPU core, sharing a socket or sharing a NUMA node.
\end{itemize}


\subsection{\href{https://scicomp.stackexchange.com/questions/2028/portable-multicore-numa-memory-allocation-initialization-best-practices}{Portable multicore/NUMA memory allocation/initialization best practices}}
\label{sec-7-2}
\subsubsection{The Questions}
\label{sec-7-2-1}
Most operating system have ways to set threads affinity, but nost of them do not provide ways to set NUMA memory policies, except Linux. Its \verb~libnuma~ allow the application to manipulate memory policy and page migration at page granularity.

Working with a "first touch" policy means that the caller should create and distribute threads later when frist writing to the freshly allocated memory. (Very few systems are configured such that \textasciitilde{}malloc()= actually finds pages, it just promises to find them when they are actually faulted, perhaps by different threads.) This implies that allocation using \textasciitilde{}calloc()= or immediately initializing memory after allocation using \textasciitilde{}memset()= is harmful since it will tend to fault all the memory onto the memory bus of the core running the allocating thread, leading to worst-case memory bandwidth when the memory is accessed from multiple threads.

Are any solutions to NUMA allocation/initialization considered idiomatic?

\subsubsection{Answers from others:}
\label{sec-7-2-2}
\begin{enumerate}
\item One solution to this problem is: disaggregate threads and tasks at the effectively, memory controller level.
\label{sec-7-2-2-1}
Remove the NUMA aspects from your code by having one task per CPU socket or memory controller and then threads under each task. You should be able to bind all memory to that socket/controller safely either via first-touch or one of the available API no matter which thread actually does the work of allocation or initialization.

\item One solution talks about C++ \verb~new~ overloading
\label{sec-7-2-2-2}
\begin{verbatim}
#include <cstddef>
#include <iostream>
#include <new>

// Just to use two different classes.
class arena { };
class policy { };

struct A
{
  void* operator new(std::size_t, arena& arena_obj, policy& policy_obj)
  {
    std::cout << "special operator new\n";
    return (void*)0x1234; //Just to test
  }
};

void* operator new(std::size_t, arena& arena_obj, policy& policy_obj)
{
  std::cout << "special operator new (global)\n";
  return (void*)0x5678; //Just to test
}

int main ()
{
  arena arena_obj;
  policy policy_obj;
  A* ptr = new(arena_obj, policy_obj) A;
  int* iptr = new(arena_obj, policy_obj) int;
  std::cout << ptr << "\n";
  std::cout << iptr << "\n";
}
\end{verbatim}
\end{enumerate}

\subsection{\href{http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html}{Affinity control outside OpenMP} (OpenMP topic: Affinity)}
\label{sec-7-3}
\subsubsection{OpenMP thread affinity control}
\label{sec-7-3-1}
\begin{enumerate}
\item Thread binding
\label{sec-7-3-1-1}
Suppose there are two sockets and have total of 16 cores. core \{0\textasciitilde{}7\} is on socket 0, \{8\textasciitilde{}15\} in on socket 1.
\begin{itemize}
\item If you define
OMP$_{\text{PLACES}}$=cores
OMP$_{\text{PROC}}$$_{\text{BIND}}$=close

Then
\begin{itemize}
\item thread 0 goes to core 0, which is on socket 0,
\item thread 1 goes to core 1, which is on socket 0,
\item thread 2 goes to core 2, which is on socket 0,
\item and so on, until thread 7 goes to core 7 on socket 0, and
\item thread 8 goes to core 8, which is on socket 1,
\item et cetera.
\end{itemize}
The value OMP$_{\text{PROC}}$$_{\text{BIND}}$=close means that the assignment goes successively through the available places.

\item If you define
OMP$_{\text{PLACES}}$=cores
OMP$_{\text{PROC}}$$_{\text{BIND}}$=spread

Then
\begin{itemize}
\item thread 0 goes to core 0, which is on socket 0,
\item thread 1 goes to core 8, which is on socket 1,
\item thread 2 goes to core 1, which is on socket 0,
\item thread 3 goes to core 9, which is on socket 1,
\item and so on, until thread 14 goes to core 7 on socket 0, and
\item thread 15 goes to core 15, which is on socket 1.
\end{itemize}
The variable OMP$_{\text{PROC}}$$_{\text{BIND}}$ can also be set to spread , which spreads the threads over the places.

\item If you define
OMP$_{\text{PLACES}}$=sockets

Then
\begin{itemize}
\item thread 0 goes to socket 0,
\item thread 1 goes to socket 1,
\item thread 2 goes to socket 0 again,
\item and so on
\end{itemize}
It is very similar to previous example, expect the it does not bind a thread to a specific core, sor the OS can move threads about and it can put more than one thread on the same core, even if there is another core still unused.
\end{itemize}

\item Place Definition
\label{sec-7-3-1-2}
\begin{itemize}
\item three predefined value for \texttt{OMP\_PLACES}: sockets, cores, and threads. The threads value becomes relevant on processor that have hardware threads. In that case, OMP$_{\text{PLACES}}$=cores does not tie a thread to a specific hardware thread, leading to possible collisions. Setting OMP$_{\text{PLACES}}$=threads bonds each OpenMP thread to a specific hardware thread.

\item General syntax: \verb~location:number:stride~
\begin{enumerate}
\item "\}
Has the same effect of sockets on a two-socket design with eight cores per socket.
It defines two places, each having eight consecutive cores. The threads are then places alternating between the two places, but no further specified inside the place.
\item The setting cores is equal to
\begin{verbatim}
{OMP_PLACES="{0},{1},{2},...,{15}"
\end{verbatim}
"\}
\item On a four-socket design, the specification
:4:1"\}
states that the place 0, 8, 16, 24 needs to be repeated 4 time, with a stride of 1.
\end{enumerate}
\end{itemize}

\item Binding possibilities
\label{sec-7-3-1-3}
Values of OMP$_{\text{PROC}}$$_{\text{BIND}}$ are:
\begin{itemize}
\item false, set no binding
\item true, lock threads to a core
\item master, collocate threads with the master thread
\item close, place threads close to the master in the places list
\item spread, spread out threads as much as possible
\end{itemize}

The effect can be made local by using \texttt{proc\_bind} clause in the parallel directive.

A safe default setting is: \verb~export OMP_PROC_BIND=true~, which prevents the OS form migrating a thread.
\end{enumerate}


\subsubsection{First-touch}
\label{sec-7-3-2}
\begin{enumerate}
\item First-touch
\label{sec-7-3-2-1}
The affinity issue shows up in the first-touch phenomemon. Memory allocated with malloc and like routines is not actually allocated; that only happens when data is written to it. Consider the following code:
\begin{verbatim}
double *x = (double*) malloc(N*sizeof(double));

for (i=0; ilt;N; i++)
  x[i] = 0;

#pragma omp parallel for
for (i=0; ilt;N; i++)
  .... something with x[i] ...
\end{verbatim}

Since the initialization loop is not parallel it is executed by the master thread, \textbf{making all the memory associated with the socket of that thread}. Subsequent access by other socket will then access data from memory not attached to that socket.
\end{enumerate}

\subsubsection{SPMD sytle of programming}
\label{sec-7-3-3}
By regarding affinity, by adopting an SPMD style of programming. You could make this explicit by having each thread allocate its part of the arrays separately, and storing a private pointer as \verb~threadprivate~. 

However, this makes it impossible for threads to access each other's parts of the distributed array, so this is only suitable for \emph{total} data  parallel or embarassingly parallel applications.

\begin{itemize}
\item embarrassingly parallel workload or problem is one where lietter or no effort is needed to separate the problem into number of parallel tasks. This is often the case where there is little or no dependency or need for communication between those parallel tasks, or for results between them.
\end{itemize}


\subsection{\href{https://yunmingzhang.wordpress.com/2018/01/20/openmp-numa-and-libnuma-note/}{Blog about OpenMP numa and libnuma note}}
\label{sec-7-4}
Some notes on using libnuma, and achieving some similar functionalities in OpenMP using task affinity and places.
\subsubsection{libnuma}
\label{sec-7-4-1}
For memory allocation, ususally routine such as malloc uses first touch. However, "numa alloc" it would actually go in touch every location and make sure it is allocated on a certian node.

\subsection{\href{https://stackoverflow.com/questions/23142702/how-to-instantiate-c-objects-on-specific-numa-memory-nodes}{How to instantiate C++ objects on specific NUMA memory nodes?}}
\label{sec-7-5}
\begin{verbatim}
void *blob = numa_alloc_onnode(sizeof(Object), ...);
Object *object = new(blob) Object;
\end{verbatim}
\subsubsection{\href{http://en.cppreference.com/w/cpp/language/new}{new expression} in C++}
\label{sec-7-5-1}

\section{A blog shows what attribute need to monitor for NUMA node performance analysis}
\label{sec-8}
\href{http://www.acceleware.com/blog/real-time-NUMA-node-performance-analysis-using-intel-performance-counter-monitor}{Real-Time NUMA Node Performance Analysis Using Intel Performance Counter Monitor}
\section{Summary}
\label{sec-9}
\subsection{What is data parallelism and what is task parallelism?}
\label{sec-9-1}
A parallel program is composed of simultaneously executing processes. Problem decomposition relates to the way in which the constituent processes are formulated
\begin{itemize}
\item Task parallelism, A task-parallel model focuses on processes, or threads of execution. These processes will often be behaviourally distinct, which emphasises the need for communication. Task parallelism is a natural way to express message-passing communication. 
Task parallelism is usually classified as MIMD/MPMD or MISD
\item Data parallelism, A data-parallel model focuses on performing operations on a data set, typically a regularly structured array. A set of tasks will operate on this data, but independently on disjoint partitions.
Data parallelism is usually classified as MIMD/SPMD or SIMD
\includegraphics[width=.9\linewidth]{/Users/zw/Documents/screenshots/classification-parallel.png}
\end{itemize}

\subsection{How does each of them related to OpenMP}
\label{sec-9-2}
\subsection{What's their current status for using OpenMP under NUMA architectures?}
\label{sec-9-3}
\subsection{Which algorithm do I need to use, data/task parallelism or both?}
\label{sec-9-4}
\subsection{Operating system concepts}
\label{sec-9-5}
\subsubsection{work-stealing \cite{blumofe99_sched_multit_comput_by_work_steal}}
\label{sec-9-5-1}
\subsection{OpenMP affinity}
\label{sec-9-6}
\subsubsection{Concepts}
\label{sec-9-6-1}
\begin{itemize}
\item OpenMP affinity consists of a \texttt{proc\_bind} policy and a specification of places. It enables users to bind computations on specific places. The placement will hold for the duration of the parallel region.
\begin{itemize}
\item place refers to processors which could be cores, or hardware threads, sockets.
\end{itemize}
\item However, the runtime is free to migrate the OpenMP threads to different cores (hardware thread, or sockets) prescribed within a given place, if two or more cores(hardware threads, sockets) have been assigned to a given place.
\item \texttt{OMP\_PLACES} specify the places, witout setting it, OpenMP runtime will distribute and bind threads using the entire range of processors for the OpenMP program, based on the policy specified by \texttt{proc\_bind}.
\item SMT(Simultaneous Multi-Threading)
\item HW-thread, hardware thread
\item OpenMP places use the processor number to designate binding locations.
\item Threads of a team are positioned onto places in a compact manner, a scattered distribution, or onto a master's place, by setting \texttt{proc\_bind} clause to \emph{close}, \emph{spread}, or \emph{master}
\end{itemize}
\subsubsection{The \texttt{proc\_bind} clause}
\label{sec-9-6-2}
\begin{itemize}
\item picture of hierarchy: socket -> physical core -> hardware thread
\item If a machine has 2 sockets, each of them has 4 cores, and each core has 2 hardware  threads. Then the \texttt{OMP\_PLACES} variable could be set like:
\end{itemize}
"\{0,1\},\{2,3\},\{4,5\},\{6,7\},\{8,9\},\{10,11\},\{12,13\},\{14,15\}" or equivalently "\{0:2\}:8:2"

\begin{enumerate}
\item Spread affinity policy
\label{sec-9-6-2-1}
(assuming the \texttt{OMP\_PLACES} is the same)
When the number of threads is less than or equal to the number of places in the parent' place partition. Such as:
\texttt{\#pragma omp parallel proc\_bind(spread) num\_threads(4)}
\begin{enumerate}
\item If the master thread is initially started on p0, the following placement of threads will be applied in the parallel region:
\begin{itemize}
\item thread 0 executes on p0 with the place partition p0,p1
\item thread 1 executes on p2 with the place partition p2,p3
\item thread 2 executes on p4 with the place partition p4,p5
\item thread 3 executes on p6 with the place partition p6,p7
\end{itemize}
\item If the master thread would initially be started on p2:
\begin{itemize}
\item thread 0 executes on p2 with the place partition p2,p3
\item thread 1 executes on p4 with the place partition p4,p5
\item thread 2 executes on p6 with the place partition p6,p7
\item thread 3 executes on p0 with the place partition p0,p1
\end{itemize}
\end{enumerate}

When the number of thread is greater than the number of places in the parent's place partition.
\texttt{\#pragma omp parallel num\_threads(16) proc\_bind(spread)}
\begin{enumerate}
\item If the master thread is initially started on p0:
\begin{itemize}
\item threads 0,1 execute on p0 with the place partition p0
\item threads 2,3 execute on p1 with the place partition p1
\item threads 4,5 execute on p2 with the place partition p2
\item threads 6,7 execute on p3 with the place partition p3
\item threads 8,9 execute on p4 with the place partition p4
\item threads 10,11 execute on p5 with the place partition p5
\item threads 12,13 execute on p6 with the place partition p6
\item threads 14,15 execute on p7 with the place partition p7
\end{itemize}
\item If the master thread is initially started on p2:
\begin{itemize}
\item threads 0,1 execute on p2 with the place partition p2
\item threads 2,3 execute on p3 with the place partition p3
\item threads 4,5 execute on p4 with the place partition p4
\item threads 6,7 execute on p5 with the place partition p5
\item threads 8,9 execute on p6 with the place partition p6
\item threads 10,11 execute on p7 with the place partition p7
\item threads 12,13 execute on p0 with the place partition p0
\item threads 14,15 execute on p1 with the place partition p1
\end{itemize}
\end{enumerate}

\item Close affinity policy
\label{sec-9-6-2-2}
When the number of threads is less than or equal to the number of places in parent's place partition:
\texttt{\#pragma omp parallel proc\_bind(close) num\_threads(4)}
\begin{enumerate}
\item If the master thread is initially started on p0, the following placement of threads will be applied in the \verb~parallel~ region:
\begin{itemize}
\item thread 0 executes on p0 with the place partition p0-p7
\item thread 1 executes on p1 with the place partition p0-p7
\item thread 2 executes on p2 with the place partition p0-p7
\item thread 3 executes on p3 with the place partition p0-p7
\end{itemize}
\item If the master starts on p2:
\begin{itemize}
\item thread 0 executes on p2 with the place partition p0-p7
\item thread 1 executes on p3 with the place partition p0-p7
\item thread 2 executes on p4 with the place partition p0-p7
\item thread 3 executes on p5 with the place partition p0-p7
\end{itemize}
\end{enumerate}

When the number of thread is greater than the number of places in parent's place partition:
\texttt{\#pragma omp parallel num\_threads(16) proc\_bind(close)}
\begin{enumerate}
\item If master is on p0
\begin{itemize}
\item threads 0,1 execute on p0 with the place partition p0-p7
\item threads 2,3 execute on p1 with the place partition p0-p7
\item threads 4,5 execute on p2 with the place partition p0-p7
\item threads 6,7 execute on p3 with the place partition p0-p7
\item threads 8,9 execute on p4 with the place partition p0-p7
\item threads 10,11 execute on p5 with the place partition p0-p7
\item threads 12,13 execute on p6 with the place partition p0-p7
\item threads 14,15 execute on p7 with the place partition p0-p7
\end{itemize}
\item If the master is initially started on p2
\begin{itemize}
\item threads 0,1 execute on p2 with the place partition p0-p7
\item threads 2,3 execute on p3 with the place partition p0-p7
\item threads 4,5 execute on p4 with the place partition p0-p7
\item threads 6,7 execute on p5 with the place partition p0-p7
\item threads 8,9 execute on p6 with the place partition p0-p7
\item threads 10,11 execute on p7 with the place partition p0-p7
\item threads 12,13 execute on p0 with the place partition p0-p7
\item threads 14,15 execute on p1 with the place partition p0-p7
\end{itemize}
\end{enumerate}

\item Master affinity policy
\label{sec-9-6-2-3}
\texttt{\#pragma omp parallel proc\_bind(master) num\_threads(4)}
\begin{enumerate}
\item If the master thread is initially running on p0:
\begin{itemize}
\item threads 0-3 execute on p0 with the place partition p0-p7
\end{itemize}
\item If the master thread would initially be started on p2
\begin{itemize}
\item threads 0-3 execute on p2 with the place partition p0-p7
\end{itemize}
\end{enumerate}
\item Questions:
\label{sec-9-6-2-4}
What is place partition? 
For example, in a 2 socket system with 8 cores in each socket, and sequential numbering in the socket for the core numbers, the \texttt{OMP\_PLACES} variable would be set to
"\{0:8\},\{8:8\}", using the place syntax \{lower$_{\text{bound}}$:length:stride\}, and the default stride is 1.
\end{enumerate}

\subsubsection{Affinity query function}
\label{sec-9-6-3}
\begin{itemize}
\item \textasciitilde{}omp$_{\text{get}}$$_{\text{num}}$$_{\text{places}}$()=
\item \textasciitilde{}omp$_{\text{get}}$$_{\text{place}}$$_{\text{num}}$$_{\text{procs}}$()=
\end{itemize}
\subsection{Next plan}
\label{sec-9-7}
\begin{itemize}
\item The OpenMP affinity is about allocating the executing of tasks among different CPUs. But where each CPU access which part of memory is not determined. We need to allocate memory on fixed cores. Such as,

\begin{itemize}
\item Array[0..N] will be allocation among different cores.
\item create multiple thread on those cores using OpenMP affinity
\item compute the results
\end{itemize}
\end{itemize}

\section{Bibliography}
\label{sec-10}
\bibliographystyle{abbrv}
\bibliography{../parallel-numa}
% Emacs 25.3.1 (Org mode 8.2.10)
\end{document}