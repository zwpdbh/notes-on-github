<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>2018-04-12 reading notes for OpenMP NUMA</title>
<!-- 2018-04-17 Tue 12:31 -->
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<meta  name="author" content="zwpdbh" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">2018-04-12 reading notes for OpenMP NUMA</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. OpenMP on NUMA architectures</a>
<ul>
<li><a href="#sec-1-1">1.1. Investigating NUMA topologies</a></li>
<li><a href="#sec-1-2">1.2. Optimizing NUMA accesses</a></li>
<li><a href="#sec-1-3">1.3. Thread placement in OpenMP</a>
<ul>
<li><a href="#sec-1-3-1">1.3.1. Selecting the right binding strategy depends not only on the topology, but also on the characteristics of the application.</a></li>
<li><a href="#sec-1-3-2">1.3.2. Available strategies</a></li>
<li><a href="#sec-1-3-3">1.3.3. Abstract names for  <code>OMP_PLACES</code></a></li>
</ul>
</li>
<li><a href="#sec-1-4">1.4. Data Placement</a>
<ul>
<li><a href="#sec-1-4-1">1.4.1. First touch in action</a></li>
</ul>
</li>
<li><a href="#sec-1-5">1.5. Memory and thread placement in Linux</a></li>
</ul>
</li>
<li><a href="#sec-2">2. OpenMP overview</a>
<ul>
<li><a href="#sec-2-1">2.1. OpenMp Compilation process</a></li>
<li><a href="#sec-2-2">2.2. Notation</a>
<ul>
<li><a href="#sec-2-2-1">2.2.1. Syntax</a></li>
<li><a href="#sec-2-2-2">2.2.2. Notation (OpenMP)</a></li>
<li><a href="#sec-2-2-3">2.2.3. Scope of Variables</a></li>
<li><a href="#sec-2-2-4">2.2.4. <code>parallel for</code> directive</a></li>
<li><a href="#sec-2-2-5">2.2.5. <code>reduce</code> clause</a></li>
<li><a href="#sec-2-2-6">2.2.6. <code>critical</code> sections</a></li>
<li><a href="#sec-2-2-7">2.2.7. <code>Atomic</code> statements</a></li>
<li><a href="#sec-2-2-8">2.2.8. More synchronization constructs</a></li>
</ul>
</li>
<li><a href="#sec-2-3">2.3. ForestGOMP: NUMA with OpenMP</a>
<ul>
<li><a href="#sec-2-3-1">2.3.1. BubbleSched: hierarchical buble-based thread scheduler</a></li>
<li><a href="#sec-2-3-2">2.3.2. Mami: NUMA-aware memory manager</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-3">3. ForestGOMP: an efficient OpenMP environment for NUMA architectures </a></li>
<li><a href="#sec-4">4. OpenMP task scheduling strategies for multicore NUMA systems </a>
<ul>
<li><a href="#sec-4-1">4.1. Abstract</a></li>
<li><a href="#sec-4-2">4.2. Introduction</a>
<ul>
<li><a href="#sec-4-2-1">4.2.1. Their contributions</a></li>
</ul>
</li>
<li><a href="#sec-4-3">4.3. Background</a></li>
<li><a href="#sec-4-4">4.4. Conclustion</a></li>
</ul>
</li>
<li><a href="#sec-5">5. OpenMP Extension for Explicit Task Allocation on NUMA Architecture </a>
<ul>
<li><a href="#sec-5-1">5.1. Abstract</a></li>
<li><a href="#sec-5-2">5.2. Introduction</a></li>
<li><a href="#sec-5-3">5.3. Related work</a></li>
<li><a href="#sec-5-4">5.4. OpenMP Extension for NUMA-Aware Task Allocation</a></li>
</ul>
</li>
<li><a href="#sec-6">6. Linux libnuma</a>
<ul>
<li><a href="#sec-6-1">6.1. Description</a>
<ul>
<li><a href="#sec-6-1-1">6.1.1. It offers a programming interface to the NUMA policy. Available policies are:</a></li>
<li><a href="#sec-6-1-2">6.1.2. Note:</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-7">7. Links related with C++ and libnuma</a>
<ul>
<li><a href="#sec-7-1">7.1. C++11 threads, affinity and hyperthreading</a>
<ul>
<li><a href="#sec-7-1-1">7.1.1. Background and introduction</a></li>
<li><a href="#sec-7-1-2">7.1.2. Logical CPUs, cores and threads</a></li>
<li><a href="#sec-7-1-3">7.1.3. Launching a thread per CPU</a></li>
<li><a href="#sec-7-1-4">7.1.4. Detour - thread IDs and native handles</a></li>
<li><a href="#sec-7-1-5">7.1.5. Setting CPU affinity programatically</a></li>
<li><a href="#sec-7-1-6">7.1.6. Sharing a core with hyperthreading</a></li>
<li><a href="#sec-7-1-7">7.1.7. Performance demos of core sharing vs separate cores</a></li>
<li><a href="#sec-7-1-8">7.1.8. Summary</a></li>
</ul>
</li>
<li><a href="#sec-7-2">7.2. Portable multicore/NUMA memory allocation/initialization best practices</a>
<ul>
<li><a href="#sec-7-2-1">7.2.1. The Questions</a></li>
<li><a href="#sec-7-2-2">7.2.2. Answers from others:</a></li>
</ul>
</li>
<li><a href="#sec-7-3">7.3. Affinity control outside OpenMP (OpenMP topic: Affinity)</a>
<ul>
<li><a href="#sec-7-3-1">7.3.1. OpenMP thread affinity control</a></li>
<li><a href="#sec-7-3-2">7.3.2. First-touch</a></li>
<li><a href="#sec-7-3-3">7.3.3. SPMD sytle of programming</a></li>
</ul>
</li>
<li><a href="#sec-7-4">7.4. Blog about OpenMP numa and libnuma note</a>
<ul>
<li><a href="#sec-7-4-1">7.4.1. libnuma</a></li>
</ul>
</li>
<li><a href="#sec-7-5">7.5. How to instantiate C++ objects on specific NUMA memory nodes?</a>
<ul>
<li><a href="#sec-7-5-1">7.5.1. new expression in C++</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-8">8. A blog shows what attribute need to monitor for NUMA node performance analysis</a></li>
<li><a href="#sec-9">9. Summary</a>
<ul>
<li><a href="#sec-9-1">9.1. What is data parallelism and what is task parallelism?</a></li>
<li><a href="#sec-9-2">9.2. How does each of them related to OpenMP</a></li>
<li><a href="#sec-9-3">9.3. What's their current status for using OpenMP under NUMA architectures?</a></li>
<li><a href="#sec-9-4">9.4. Which algorithm do I need to use, data/task parallelism or both?</a></li>
<li><a href="#sec-9-5">9.5. Operating system concepts</a>
<ul>
<li><a href="#sec-9-5-1">9.5.1. work-stealing </a></li>
</ul>
</li>
<li><a href="#sec-9-6">9.6. OpenMP affinity</a>
<ul>
<li><a href="#sec-9-6-1">9.6.1. Concepts</a></li>
<li><a href="#sec-9-6-2">9.6.2. The <code>proc_bind</code> clause</a></li>
<li><a href="#sec-9-6-3">9.6.3. Affinity query function</a></li>
</ul>
</li>
<li><a href="#sec-9-7">9.7. Next plan</a></li>
</ul>
</li>
<li><a href="#sec-10">10. Bibliography</a></li>
</ul>
</div>
</div>

<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> OpenMP on NUMA architectures</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Investigating NUMA topologies</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li><code>numactl --hardware</code> print information on NUMA nodes in the system
</li>
</ul>
<div class="org-src-container">

<pre class="src src-sh">wzhao@r815:~$ numactl --hardware
available: 8 nodes (0-7)
node 0 cpus: 0 4 8 12 16 20 24 28
node 0 size: 65526 MB
node 0 free: 63750 MB
node 1 cpus: 32 36 40 44 48 52 56 60
node 1 size: 65536 MB
node 1 free: 63869 MB
node 2 cpus: 2 6 10 14 18 22 26 30
node 2 size: 65536 MB
node 2 free: 63860 MB
node 3 cpus: 34 38 42 46 50 54 58 62
node 3 size: 65536 MB
node 3 free: 63861 MB
node 4 cpus: 3 7 11 15 19 23 27 31
node 4 size: 65536 MB
node 4 free: 63863 MB
node 5 cpus: 35 39 43 47 51 55 59 63
node 5 size: 65536 MB
node 5 free: 63869 MB
node 6 cpus: 1 5 9 13 17 21 25 29
node 6 size: 65536 MB
node 6 free: 63871 MB
node 7 cpus: 33 37 41 45 49 53 57 61
node 7 size: 65520 MB
node 7 free: 63836 MB
node distances:
node   0   1   2   3   4   5   6   7 
0:  10  16  16  22  16  22  16  22 
1:  16  10  22  16  16  22  22  16 
2:  16  22  10  16  16  16  16  16 
3:  22  16  16  10  16  16  22  22 
4:  16  16  16  16  10  16  16  22 
5:  22  22  16  16  16  10  22  16 
6:  16  22  16  22  16  22  10  16 
7:  22  16  16  22  22  16  16  10
</pre>
</div>
<ul class="org-ul">
<li><code>numactl --show</code> prints information on available resources for the process
</li>
</ul>
<div class="org-src-container">

<pre class="src src-sh">wzhao@r815:~$ numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 
cpubind: 0 1 2 3 4 5 6 7 
nodebind: 0 1 2 3 4 5 6 7 
membind: 0 1 2 3 4 5 6 7
</pre>
</div>

<ul class="org-ul">
<li>use <code>lstopo</code> show the system topology
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Optimizing NUMA accesses</h3>
<div class="outline-text-3" id="text-1-2">
<p>
Goal: minimize the number of remote memory accesses as much as possible.
</p>
<ul class="org-ul">
<li>how are threads distributed on the system?
</li>
<li>how is the data distributed on the system?
</li>
<li>how is work distributed across thread?
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Thread placement in OpenMP</h3>
<div class="outline-text-3" id="text-1-3">
</div><div id="outline-container-sec-1-3-1" class="outline-4">
<h4 id="sec-1-3-1"><span class="section-number-4">1.3.1</span> Selecting the right binding strategy depends not only on the topology, but also on the characteristics of the application.</h4>
<div class="outline-text-4" id="text-1-3-1">
</div><ol class="org-ol"><li><a id="sec-1-3-1-1" name="sec-1-3-1-1"></a>Putting threads far part, on different sockets<br  /><div class="outline-text-5" id="text-1-3-1-1">
<ul class="org-ul">
<li>May improve the aggregated memory bandwidth available to your application.
</li>
<li>May improve the combined cache size available to your application.
</li>
<li>May decrease performance of synchronization constructs
</li>
</ul>
</div>
</li>
<li><a id="sec-1-3-1-2" name="sec-1-3-1-2"></a>Putting threads close together, i.e. on two adjacent cores which possibly shared some caches<br  /><div class="outline-text-5" id="text-1-3-1-2">
<ul class="org-ul">
<li>May improve performance of synchronization constructs
</li>
<li>May decrease the available memory bandwidth and cache size
</li>
</ul>
</div>
</li></ol>
</div>
<div id="outline-container-sec-1-3-2" class="outline-4">
<h4 id="sec-1-3-2"><span class="section-number-4">1.3.2</span> Available strategies</h4>
<div class="outline-text-4" id="text-1-3-2">
<ul class="org-ul">
<li>close: put threads close together on the system
</li>
<li>spread: place threads far apart from each other
</li>
<li>master: run on the same place as the master thread
</li>
</ul>

<p>
Assume the following machine:
</p>

<div id="fig:numa-example-01" class="figure">
<p><img src="file:///Users/zw/Documents/screenshots/numa-nodes.png" alt="numa-nodes.png" width="200/250/300/400/500/600px" />
</p>
<p><span class="figure-number">Figure 1:</span> numa-example, 2 sockets, 4 cores per socket, 4 hyper-threads per core</p>
</div>
</div>
</div>
<div id="outline-container-sec-1-3-3" class="outline-4">
<h4 id="sec-1-3-3"><span class="section-number-4">1.3.3</span> Abstract names for  <code>OMP_PLACES</code></h4>
<div class="outline-text-4" id="text-1-3-3">
<ul class="org-ul">
<li>threads: each place corresponds to a single hardware thread on the target machine.
</li>
<li>cores: each place corresponds to a single core (having one or more hardware threads) on the target machine.
</li>
<li>sockets: each place corresponds to a single socket (consisting of one or more cores) on the target machine.
</li>
</ul>
</div>

<ol class="org-ol"><li><a id="sec-1-3-3-1" name="sec-1-3-3-1"></a>Example's objective<br  /><div class="outline-text-5" id="text-1-3-3-1">
<ul class="org-ul">
<li>Separate cores for outer loop and near cores for inner loop
</li>
</ul>
<div class="org-src-container">

<pre class="src src-c">OMP_PLACES={0,1,2,3}, {4,5,6,7}, ... = {0:4}:8:4
<span style="color: #268bd2;">#pragma</span> omp parallel proc_bind(spread)
<span style="color: #268bd2;">#pragma</span> omp parallel proc_bind(close)
</pre>
</div>
<ul class="org-ul">
<li><code>OMP_PLACES</code>
</li>
<li><code>proc_bind</code>
</li>
</ul>
</div>
</li></ol>
</div>
</div>
<div id="outline-container-sec-1-4" class="outline-3">
<h3 id="sec-1-4"><span class="section-number-3">1.4</span> Data Placement</h3>
<div class="outline-text-3" id="text-1-4">
<ul class="org-ul">
<li>OpenMP does not provide support for cc-NUMA 
</li>
<li>Placement comes from the operating system (Operating system dependent)
</li>
<li>By default, use the "First Touch" placement policy
</li>
</ul>
</div>

<div id="outline-container-sec-1-4-1" class="outline-4">
<h4 id="sec-1-4-1"><span class="section-number-4">1.4.1</span> First touch in action</h4>
<div class="outline-text-4" id="text-1-4-1">
</div><ol class="org-ol"><li><a id="sec-1-4-1-1" name="sec-1-4-1-1"></a>In serial code, all array element are allocated in the memory of the NUMA node containing the core executing this thread<br  /><div class="outline-text-5" id="text-1-4-1-1">
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">double</span>* <span style="color: #268bd2;">A</span>;
A = (<span style="color: #b58900;">double</span>*) malloc(<span style="color: #b58900;">N</span> * <span style="color: #859900; font-weight: bold;">sizeof</span>(<span style="color: #b58900;">double</span>));
<span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #b58900;">int</span> <span style="color: #268bd2;">i</span> = 0; i &lt; N; i++) {
  A[i] = 0.0;
 }
</pre>
</div>


<div id="fig:numa-serial-init" class="figure">
<p><img src="file:///Users/zw/Documents/screenshots/numa-serial-init.png" alt="numa-serial-init.png" />
</p>
<p><span class="figure-number">Figure 2:</span> serial code first toucle example</p>
</div>
</div>
</li>

<li><a id="sec-1-4-1-2" name="sec-1-4-1-2"></a>In parallel code<br  /><div class="outline-text-5" id="text-1-4-1-2">
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">double</span>* <span style="color: #268bd2;">A</span>;
A = (<span style="color: #b58900;">double</span>*)malloc(<span style="color: #b58900;">N</span> * <span style="color: #859900; font-weight: bold;">sizeof</span>(<span style="color: #b58900;">double</span>));
omp_set_num_threads(2);
<span style="color: #268bd2;">#pragma</span> omp parallel <span style="color: #859900; font-weight: bold;">for</span>
<span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #b58900;">int</span> <span style="color: #268bd2;">i</span> = 0; i &lt; N; i++) {
  A[i] = 0.0;
 }
</pre>
</div>


<div id="fig:numa-parallel-init" class="figure">
<p><img src="file:///Users/zw/Documents/screenshots/numa-parallel-init.png" alt="numa-parallel-init.png" />
</p>
<p><span class="figure-number">Figure 3:</span> parallel code first touch example</p>
</div>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-1-5" class="outline-3">
<h3 id="sec-1-5"><span class="section-number-3">1.5</span> Memory and thread placement in Linux</h3>
<div class="outline-text-3" id="text-1-5">
<p>
<code>numactl</code> command line tool to investigate and handle NUMA under Linux
</p>
<ul class="org-ul">
<li><code>numactl --cpunodebind 0,1,2 ./a.out</code>, only use cores of NUMA node 0-2 to execute a.out
</li>
<li><code>numactl --physcpubind 0-17 ./a.out</code>, only use cores 0-17 to execute a.out.
</li>
<li><code>numactl --membind 0,3 ./a.out</code>, only use memory of NUMA node 0 and 3 to execute a.out.
</li>
<li><code>numactl --interleave 0-3 ./a.out</code>, distribute memory pages on NUMA nodes 0-3 in a round-robin fashion, overwrite first-touch policy.
</li>
</ul>

<p>
<code>libnuma</code> library of NUMA control
</p>
<ul class="org-ul">
<li><code>void *numa_alloc_local(size_t size)</code>, allocate memory on the local NUMA node.
</li>
<li><code>void *numa_alloc_onnode(size_t size, int node)</code>, allocate memory on NUMA node node.
</li>
<li><code>void *numa_alloc_interleaved(size_t size)</code> allocate memory distributed round-robin on all NUMA nodes.
</li>
<li><code>int numa_move_pages(int pid, unsigned long count, void **pages, const int *nodes, int *status, int flags)</code>, migrate memory pages at runtime to different NUMA nodes.
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> OpenMP overview</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> OpenMp Compilation process</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>Annotated source code -&gt; OpenMP compiler -&gt; parallel object code
</li>
<li>Compiler can also generate sequential object cde
</li>
<li>compiler front end: parse OpenMP directives, correctness checks
</li>
<li>compiler back end: replace constructs by calls to runtime library, change structure of program
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Notation</h3>
<div class="outline-text-3" id="text-2-2">
</div><div id="outline-container-sec-2-2-1" class="outline-4">
<h4 id="sec-2-2-1"><span class="section-number-4">2.2.1</span> Syntax</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>directive: <code>pragma</code> statement
</li>
<li>runtime library routine: function defined in <code>omp.h</code>
</li>
<li>structured block: simgle statement or compound statement with a single entry at the top and a single exit at the bottom.
</li>
<li>clause: modifies a directive's behavior
</li>
<li>Directives combined with code form a construct, which is a pattern to accomplish something. Such as a parallel construct is a <code>parallel</code> directive, with its optional clauses, and any code to be executed.
</li>
<li>Environment variable: defined outside the program
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2-2" class="outline-4">
<h4 id="sec-2-2-2"><span class="section-number-4">2.2.2</span> Notation (OpenMP)</h4>
<div class="outline-text-4" id="text-2-2-2">
<ul class="org-ul">
<li>master thread: original thread
</li>
<li>slave thread: all additional threads
</li>
<li>team: master thread + slave thread
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2-3" class="outline-4">
<h4 id="sec-2-2-3"><span class="section-number-4">2.2.3</span> Scope of Variables</h4>
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li>shared scope
variable can be accessed by all threads in team, variables declared outside a structured block following a parallel directive
</li>
<li>private scope
variable can be accessed by a single thread, variable declared inside a structured block following a parallel directive.
</li>
</ul>
</div>
<ol class="org-ol"><li><a id="sec-2-2-3-1" name="sec-2-2-3-1"></a>Handout only: scope of variables<br  /><div class="outline-text-5" id="text-2-2-3-1">
<ul class="org-ul">
<li>private variables are uninitialized
</li>
<li>initialize variables with value from master thread: <code>firstprivate</code>
</li>
<li><code>default(none)</code> requires programmer to specify visibility for all variables implicitly, good practice.
</li>
</ul>
</div>
</li></ol>
</div>
<div id="outline-container-sec-2-2-4" class="outline-4">
<h4 id="sec-2-2-4"><span class="section-number-4">2.2.4</span> <code>parallel for</code> directive</h4>
<div class="outline-text-4" id="text-2-2-4">
<ul class="org-ul">
<li>run loop interations in parallel
</li>
<li>shortcut: <code>#pragma omp parallel for</code>
</li>
<li>loop iterations must be data-independent
</li>
<li>OpenMP must be able to determine the number of iterations before the loop is executed.
</li>
<li>Mapping of iterations to threads controlled by <code>schedule</code> clause
<ul class="org-ul">
<li>schedule(static [, chunksize]): block of chunksize iterations statically assigned to thread
</li>
<li>schedule(dynamic [, chunksize]): thread reserves chunksize iterations from queue
</li>
<li>schedule(guided [, chunksize]): same as dynamnic, but chunk size starts big and gets smaller and smaller, until it reaches chunksize.
</li>
<li>schedule(runtime): scheduling behavior determined by environment variable
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-2-5" class="outline-4">
<h4 id="sec-2-2-5"><span class="section-number-4">2.2.5</span> <code>reduce</code> clause</h4>
</div>
<div id="outline-container-sec-2-2-6" class="outline-4">
<h4 id="sec-2-2-6"><span class="section-number-4">2.2.6</span> <code>critical</code> sections</h4>
</div>
<div id="outline-container-sec-2-2-7" class="outline-4">
<h4 id="sec-2-2-7"><span class="section-number-4">2.2.7</span> <code>Atomic</code> statements</h4>
</div>
<div id="outline-container-sec-2-2-8" class="outline-4">
<h4 id="sec-2-2-8"><span class="section-number-4">2.2.8</span> More synchronization constructs</h4>
<div class="outline-text-4" id="text-2-2-8">
<ul class="org-ul">
<li><code>#pragma omp barrier</code>: wait until all threads arrive
</li>
<li><code>#pragma omp for nowait</code>: remove implicit barrier after for loop (also exists for other directives)
</li>
<li><code>#pragma omp master</code>: only executed by master thread
</li>
<li><code>#pragma omp single</code>: only executed by one thread
</li>
<li>Sections: define a number of blocks, every thread executes one block
</li>
<li>Locks: <code>omp_init_lock()</code>, <code>omp_set_lock()</code>, <code>omp_unset_lock()</code>,&#x2026;
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> ForestGOMP: NUMA with OpenMP</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>Objectives and motivations
<ol class="org-ol">
<li>Keep buffer and threads operating on them on the same NUMA node (reducing contention)
</li>
<li>Processor level: group threads sharing data intensively(improve cache usage)
</li>
</ol>
</li>
<li>Triggers for scheduling
<ol class="org-ol">
<li>Allocation/deallocation of resources
</li>
<li>Processor becomes idle
</li>
<li>Change of hardware counters (e.g., cache miss, remote acess rate)
</li>
</ol>
</li>
</ul>
</div>
<div id="outline-container-sec-2-3-1" class="outline-4">
<h4 id="sec-2-3-1"><span class="section-number-4">2.3.1</span> BubbleSched: hierarchical buble-based thread scheduler</h4>
<div class="outline-text-4" id="text-2-3-1">
<ul class="org-ul">
<li>Runqueue for different hierarchical levels
</li>
<li>Bubble: group of threads sharing data or heavy synchronization
</li>
<li>Responsible for scheduling threads
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-2-3-2" class="outline-4">
<h4 id="sec-2-3-2"><span class="section-number-4">2.3.2</span> Mami: NUMA-aware memory manager</h4>
<div class="outline-text-4" id="text-2-3-2">
<ul class="org-ul">
<li>API for memory allocation 
</li>
<li>Can migrate memory to a different NUMA node
</li>
<li>Support next touch policy: migrate data to NUMA node of accessing thread.
<ul class="org-ul">
<li>Buffers are marked as migrate-on-next-touch when a thread migration is expected
</li>
<li>Buffer is relocated if thread thouches buffer that is not located on local node
</li>
<li>Implemented in kernel mode
</li>
</ul>
</li>
<li>ForestGOMP: Mami-aware OpenMP Runtime
<ul class="org-ul">
<li>Mami attaches memory hints: e.g., which regions are access frequently by a certain thread
</li>
<li>Initial distribution: put thread and corresponding memory on same NUMA node (local accesses)
</li>
<li>Handle idleness: steal threads from local core, then from different NUMA node (also migrates memory; prefers threads with less memory)
</li>
<li>Two levels of distribution: memory-aware, then cache-aware
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> ForestGOMP: an efficient OpenMP environment for NUMA architectures <a class='org-ref-reference' href="#broquedis10_fores">broquedis10_fores</a></h2>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> OpenMP task scheduling strategies for multicore NUMA systems <a class='org-ref-reference' href="#olivier12_openm_task_sched_strat_multic_numa_system">olivier12_openm_task_sched_strat_multic_numa_system</a></h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> Abstract</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>Efficient scheduling of tasks on modern multi-socket multicore shared memory system requires consideration of shared caches and NUMA characteristics.
</li>
<li>They extendent the open source Qthreads threadling library to implement different scheduler designs, accepting OpenMP programs through the ROSE compiler.
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> Introduction</h3>
<div class="outline-text-3" id="text-4-2">
<ul class="org-ul">
<li>What is task-parallel programming models? what are the benefits?
</li>
<li>Efficient task scheduler
<ul class="org-ul">
<li>exploit cache and memory locality
</li>
<li>maintain load balance
</li>
<li>minimize overhead costs
</li>
</ul>
<p>
Trade off between them: 
</p>
<ul class="org-ul">
<li>However, load balancing operations can also contribute to overhead costs. Load balancing operations between sockets increase memory access time due to more cold cache misses and more high-latency remote memory accesses.
</li>
</ul>
</li>
</ul>
</div>
<div id="outline-container-sec-4-2-1" class="outline-4">
<h4 id="sec-4-2-1"><span class="section-number-4">4.2.1</span> Their contributions</h4>
<div class="outline-text-4" id="text-4-2-1">
</div><ol class="org-ol"><li><a id="sec-4-2-1-1" name="sec-4-2-1-1"></a>A hierarchical scheduling strategy targeting modern multi-socket multicore shared memory systems.<br  /><div class="outline-text-5" id="text-4-2-1-1">
<ul class="org-ul">
<li>NUMA architecture is not well supported by work-stealing scheduler with one queue per core or by centralized scheduler.
</li>
<li>work-stealing, a scheduling strategy for multithreaded computer programs. It solved the problem of executing a dynamically multithreaded commputation, one that can "spawn" new threads of execution, on a statically multithreaded computer, with a fixed number of processors. 
See <a class='org-ref-reference' href="#blumofe99_sched_multit_comput_by_work_steal">blumofe99_sched_multit_comput_by_work_steal</a>, a important paper.
Also, see paper <a class='org-ref-reference' href="#beaumont06_centr">beaumont06_centr</a> for comparsion.
</li>
</ul>
</div>
</li>

<li><a id="sec-4-2-1-2" name="sec-4-2-1-2"></a>A detailed performance study on a current generation multi-socket multicore Intel system<br  /></li>
<li><a id="sec-4-2-1-3" name="sec-4-2-1-3"></a>Additional performance evaluation on a two-socket multicore AMD system and a 192-processor SGI Altix<br  /></li></ol>
</div>
</div>

<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> Background</h3>
<div class="outline-text-3" id="text-4-3">
<ul class="org-ul">
<li>OpenMP 3.0 explicit task parallelism to complement its exisiting data parallel constructs. 
</li>
<li>ROSE compiler is used for performing syntactic and semantic analysis on OpenMP directives, transforming them into run-time library calls in the intermediate program.The ROSE common OpenMP run-time library maps the run-time calls to function in the Qthreads library.
</li>
<li>Qthreads vs Pthread, why Qthreads is needed in their paper?
<ul class="org-ul">
<li>Each worker pthread is pinned to a processor core and assigned to a locality domain, termed a shepherd.
</li>
<li>what is FEB operation, a contex switch is triggered.
</li>
</ul>
</li>
<li>"We used the Qthreads queueing implementation as a starting point for our scheduling work."
</li>
<li>"We implement OpenMP threads as worker pthreads. Unlike many OpenMP implementations, default loop scheduling is self-guided rather than static."
</li>
<li>"For task par- allelism, we implement each OpenMP task as a qthread."
</li>
<li>"We used the Qthreads FEB synchronization mechanism as a base layer upon which to implement taskwait and barrier sychronization."
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4"><span class="section-number-3">4.4</span> Conclustion</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li>Their MTS scheduler, combination of shared LIFO queues and work stealing maintains good load balance while supporting effective cache performance and limiting overhead cost. Notice: pure work stealing has been shown to provide the least variability in performance which is an important consideration for distributed applications in which barriers cause the application to run at the speed of the slowest worker.
</li>
<li>One challenge posed by their hierarchical scheduling strategy is the need for an efficient queue supporting concurrent access on both end, since works within a shepherd share a queue. (Lock-free dequeue).  
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> OpenMP Extension for Explicit Task Allocation on NUMA Architecture <a class='org-ref-reference' href="#10.1007/978-3-319-45550-1_7">10.1007/978-3-319-45550-1_7</a></h2>
<div class="outline-text-2" id="text-5">
</div><div id="outline-container-sec-5-1" class="outline-3">
<h3 id="sec-5-1"><span class="section-number-3">5.1</span> Abstract</h3>
<div class="outline-text-3" id="text-5-1">
<p>
In this paper, we propose an extension for the OpenMP task construct to specify the loca- tion of tasks to exploit the locality in an explicit manner. The prototype compiler is implemented based on GCC.
</p>
</div>
</div>

<div id="outline-container-sec-5-2" class="outline-3">
<h3 id="sec-5-2"><span class="section-number-3">5.2</span> Introduction</h3>
<div class="outline-text-3" id="text-5-2">
<ul class="org-ul">
<li>In the early version of OpenMP, the programming model had focused on data parallelism described by loop work sharing, which requires global synchro- nization in a parallel region. When the number of cores increases, synchronization overhead is getting bigger, and load imbalance among cores causes a significant performance drop.
</li>
<li>In OpenMP 4.0, task dependency can be specified using the depend clause in the task construct. Task parallelism can exploit potential parallelism in irregular applications. Task dependency can reduce synchronization overhead because it generates fine-grain synchronization between dependent tasks.
</li>
<li>To exploit memory bandwidth with NUMA architectures, OpenMP provides thread affinity options.
<ul class="org-ul">
<li>For OpenMP4.5, the <code>proc_bind</code> clause is discussed to specify a thread affinity scheme for a parallel region. These can be helpful to improve data locality when performing data parallelism with loop work sharing.
</li>
<li>However, the current specification lacks functionality to do the same thing for task parallelism.
</li>
</ul>
</li>
<li>An OpenMP extension to describe NUMA-aware task allocation explicitly. The extension specifies the data that the target task would access. 
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-5-3" class="outline-3">
<h3 id="sec-5-3"><span class="section-number-3">5.3</span> Related work</h3>
<div class="outline-text-3" id="text-5-3">
<ul class="org-ul">
<li>Some NUMA-aware task scheduler based on work-stealing, see <a class='org-ref-reference' href="#vikranth13_topol_aware_task_steal_chip">vikranth13_topol_aware_task_steal_chip</a>,<a class='org-ref-reference' href="#DBLP:journals/corr/Tahan14">DBLP:journals/corr/Tahan14</a>,<a class='org-ref-reference' href="#drebes14_topol_aware_depen_aware_sched">drebes14_topol_aware_depen_aware_sched</a>,<a class='org-ref-reference' href="#olivier12_openm_task_sched_strat_multic_numa_system">olivier12_openm_task_sched_strat_multic_numa_system</a>    
</li>
<li>Manual data distribution among NUMA nodes and their NUMA-aware task scheduling algorithm in runtime. <a class='org-ref-reference' href="#muddukrishna15_local_aware_task_sched_data">muddukrishna15_local_aware_task_sched_data</a>
</li>
<li>their is similar, also requeires explicit data distribution. However, task allocation is done explicityly using the extended OpenMP task construct.
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-5-4" class="outline-3">
<h3 id="sec-5-4"><span class="section-number-3">5.4</span> OpenMP Extension for NUMA-Aware Task Allocation</h3>
<div class="outline-text-3" id="text-5-4">
<p>
Generally, improving data locality and reducing remote memory access can exploit potential memory performance on the NUMA architecture. The same is true for task parallelism in OpenMP. A task should be executed on the NUMA node where its processing data is allocated to get the highest memory bandwidth. They propose a new clause named <code>node_bind</code> for OpenMP task construct. It specifies a NUMA node that the target task should be scheduled.
</p>

<p>
(to be continued)
</p>
</div>
</div>
</div>
<div id="outline-container-sec-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> <a href="http://man7.org/linux/man-pages/man3/numa.3.html">Linux libnuma</a></h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-sec-6-1" class="outline-3">
<h3 id="sec-6-1"><span class="section-number-3">6.1</span> Description</h3>
<div class="outline-text-3" id="text-6-1">
</div><div id="outline-container-sec-6-1-1" class="outline-4">
<h4 id="sec-6-1-1"><span class="section-number-4">6.1.1</span> It offers a programming interface to the NUMA policy. Available policies are:</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>page interleaving (allocated in a round-robin fashion from all)
</li>
<li>subset of the nodes on the system
</li>
<li>preferred node allocation (preferably allocate on a particular node)
</li>
<li>local allocation (allocate on the node on which the task is currently executing)
</li>
<li>allocation only on specific nodes
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-6-1-2" class="outline-4">
<h4 id="sec-6-1-2"><span class="section-number-4">6.1.2</span> Note:</h4>
<div class="outline-text-4" id="text-6-1-2">
<p>
The default memory allocation policy for tasks and all memory range is local allocation.
For setting a specific policy globally for all memory allocations in a process and its children, it is easiest to start it with the <a href="http://man7.org/linux/man-pages/man8/numactl.8.html">numactl</a> utility.
All numa memory allocation policy only takes effect when a page is actually faulted into the address space of a process by accessing it. (First touch policy)
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Links related with C++ and libnuma</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-sec-7-1" class="outline-3">
<h3 id="sec-7-1"><span class="section-number-3">7.1</span> <a href="https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/">C++11 threads, affinity and hyperthreading</a></h3>
<div class="outline-text-3" id="text-7-1">
</div><div id="outline-container-sec-7-1-1" class="outline-4">
<h4 id="sec-7-1-1"><span class="section-number-4">7.1.1</span> Background and introduction</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
This post use C+++ threads as the main threading mechanism to demonstrate its points.
</p>
</div>
</div>
<div id="outline-container-sec-7-1-2" class="outline-4">
<h4 id="sec-7-1-2"><span class="section-number-4">7.1.2</span> Logical CPUs, cores and threads</h4>
<div class="outline-text-4" id="text-7-1-2">
<ul class="org-ul">
<li>Modern machines are multi-CPU, where these CPUs are divided into sockets and hardware cores. But the OS sees a number of "logical" CPUs that can execute tasks concurrently.
</li>
<li>To get such information on Linux, run <code>cat /proc/cpuinfo</code>, a summary output can be obtained from <code>lscpu</code>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-7-1-3" class="outline-4">
<h4 id="sec-7-1-3"><span class="section-number-4">7.1.3</span> Launching a thread per CPU</h4>
<div class="outline-text-4" id="text-7-1-3">
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">int</span> <span style="color: #268bd2;">main</span>(<span style="color: #b58900;">int</span> <span style="color: #268bd2;">argc</span>, <span style="color: #859900; font-weight: bold;">const</span> <span style="color: #b58900;">char</span>** <span style="color: #268bd2;">argv</span>) {
  <span style="color: #b58900;">unsigned</span> <span style="color: #268bd2;">num_cpus</span> = std::thread::hardware_concurrency();
  std::cout &lt;&lt; <span style="color: #2aa198;">"Launching "</span> &lt;&lt; num_cpus &lt;&lt; <span style="color: #2aa198;">" threads\n"</span>;

  <span style="color: #586e75;">// </span><span style="color: #586e75;">A mutex ensures orderly access to std::cout from multiple threads.</span>
  std::mutex iomutex;
  std::vector&lt;std::thread&gt; threads(num_cpus);
  <span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #b58900;">unsigned</span> <span style="color: #268bd2;">i</span> = 0; i &lt; num_cpus; ++i) {
    threads[i] = std::thread([&amp;iomutex, i] {
        {
          <span style="color: #586e75;">// </span><span style="color: #586e75;">Use a lexical scope and lock_guard to safely lock the mutex only for</span>
          <span style="color: #586e75;">// </span><span style="color: #586e75;">the duration of std::cout usage.</span>
          std::lock_guard&lt;std::mutex&gt; iolock(iomutex);
          std::cout &lt;&lt; <span style="color: #2aa198;">"Thread #"</span> &lt;&lt; i &lt;&lt; <span style="color: #2aa198;">" is running\n"</span>;
        }

        <span style="color: #586e75;">// </span><span style="color: #586e75;">Simulate important work done by the tread by sleeping for a bit...</span>
        std::this_thread::sleep_for(std::chrono::milliseconds(200));

      });
  }

  <span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #859900; font-weight: bold;">auto</span>&amp; t : threads) {
    t.join();
  }
  <span style="color: #859900; font-weight: bold;">return</span> 0;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-7-1-4" class="outline-4">
<h4 id="sec-7-1-4"><span class="section-number-4">7.1.4</span> Detour - thread IDs and native handles</h4>
<div class="outline-text-4" id="text-7-1-4">
<p>
The thread library also lets us interact with platform-specific threading APIs by exposing native handles. 
Here's an example program that launches a single thread, and then queries its thread ID along with the native handle:
</p>
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">int</span> <span style="color: #268bd2;">main</span>(<span style="color: #b58900;">int</span> <span style="color: #268bd2;">argc</span>, <span style="color: #859900; font-weight: bold;">const</span> <span style="color: #b58900;">char</span>** <span style="color: #268bd2;">argv</span>) {
  std::mutex iomutex;
  std::thread t = std::thread([&amp;iomutex] {
      {
        std::lock_guard&lt;std::mutex&gt; iolock(iomutex);
        std::cout &lt;&lt; <span style="color: #2aa198;">"Thread: my id = "</span> &lt;&lt; std::this_thread::get_id() &lt;&lt; <span style="color: #2aa198;">"\n"</span>
                  &lt;&lt; <span style="color: #2aa198;">"        my pthread id = "</span> &lt;&lt; pthread_self() &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
      }
    });

  {
    std::lock_guard&lt;std::mutex&gt; iolock(iomutex);
    std::cout &lt;&lt; <span style="color: #2aa198;">"Launched t: id = "</span> &lt;&lt; t.get_id() &lt;&lt; <span style="color: #2aa198;">"\n"</span>
              &lt;&lt; <span style="color: #2aa198;">"            native_handle = "</span> &lt;&lt; t.native_handle() &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
  }

  t.join();
  <span style="color: #859900; font-weight: bold;">return</span> 0;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-sec-7-1-5" class="outline-4">
<h4 id="sec-7-1-5"><span class="section-number-4">7.1.5</span> Setting CPU affinity programatically</h4>
<div class="outline-text-4" id="text-7-1-5">
<p>
As we've seen earlier, command-line tools like taskset let us control the CPU affinity of a whole process. Sometimes, however, we'd like to do something more fine-grained and set the affinities of specific threads from within the program. How do we do that?
Use <code>pthread_setaffinity_np</code>:  Here is the example which pin each thread to a single know CPU by settng its affinity:
</p>
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">int</span> <span style="color: #268bd2;">main</span>(<span style="color: #b58900;">int</span> <span style="color: #268bd2;">argc</span>, <span style="color: #859900; font-weight: bold;">const</span> <span style="color: #b58900;">char</span>** <span style="color: #268bd2;">argv</span>) {
  constexpr <span style="color: #b58900;">unsigned</span> <span style="color: #268bd2;">num_threads</span> = 4;
  <span style="color: #586e75;">// </span><span style="color: #586e75;">A mutex ensures orderly access to std::cout from multiple threads.</span>
  std::mutex iomutex;
  std::vector&lt;std::thread&gt; threads(num_threads);
  <span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #b58900;">unsigned</span> <span style="color: #268bd2;">i</span> = 0; i &lt; num_threads; ++i) {
    threads[i] = std::thread([&amp;iomutex, i] {
        std::this_thread::sleep_for(std::chrono::milliseconds(20));
        <span style="color: #859900; font-weight: bold;">while</span> (1) {
          {
            <span style="color: #586e75;">// </span><span style="color: #586e75;">Use a lexical scope and lock_guard to safely lock the mutex only</span>
            <span style="color: #586e75;">// </span><span style="color: #586e75;">for the duration of std::cout usage.</span>
            std::lock_guard&lt;std::mutex&gt; iolock(iomutex);
            std::cout &lt;&lt; <span style="color: #2aa198;">"Thread #"</span> &lt;&lt; i &lt;&lt; <span style="color: #2aa198;">": on CPU "</span> &lt;&lt; sched_getcpu() &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
          }

          <span style="color: #586e75;">// </span><span style="color: #586e75;">Simulate important work done by the tread by sleeping for a bit...</span>
          std::this_thread::sleep_for(std::chrono::milliseconds(900));
        }
      });

    <span style="color: #586e75;">// </span><span style="color: #586e75;">Create a cpu_set_t object representing a set of CPUs. Clear it and mark</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">only CPU i as set.</span>
    <span style="color: #b58900;">cpu_set_t</span> <span style="color: #268bd2;">cpuset</span>;
    CPU_ZERO(&amp;cpuset);
    CPU_SET(i, &amp;cpuset);
    <span style="color: #b58900;">int</span> <span style="color: #268bd2;">rc</span> = pthread_setaffinity_np(threads[i].native_handle(),
                                    <span style="color: #859900; font-weight: bold;">sizeof</span>(cpu_set_t), &amp;cpuset);
    <span style="color: #859900; font-weight: bold;">if</span> (rc != 0) {
      std::cerr &lt;&lt; <span style="color: #2aa198;">"Error calling pthread_setaffinity_np: "</span> &lt;&lt; rc &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
    }
  }

  <span style="color: #859900; font-weight: bold;">for</span> (<span style="color: #859900; font-weight: bold;">auto</span>&amp; t : threads) {
    t.join();
  }
  <span style="color: #859900; font-weight: bold;">return</span> 0;
}
</pre>
</div>
<ul class="org-ul">
<li>use the <code>native_handle</code> method in order to pass the underlying native handle to the pthread call (it takes a <code>pthread_t</code> ID as its first argument).
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-7-1-6" class="outline-4">
<h4 id="sec-7-1-6"><span class="section-number-4">7.1.6</span> Sharing a core with hyperthreading</h4>
<div class="outline-text-4" id="text-7-1-6">
<p>
To see the topology of processor on Linux:
</p>
<ol class="org-ol">
<li><code>lstopo</code>
</li>
<li>An alternative non-graphical way to see which threads share the same core is to look at a special system file that exists per logical CPU. For example, for CPU 0:
<code>cat /sys/devices/system/cpu/cpu0/topology/thread_siblings_list</code>
</li>
</ol>

<p>
The hardware thread explain: for example, a processor has 4 cores, each with 2 threads , for a total of hardware 8-threads &#x2013; 8 logical CPUs for the OS.
</p>

<p>
Server-class processors will have multiple sockets, each with a multi-core CPU.For example, a machine with 2 sockets, each of which is a 8-core CPU with hyper-threads enable: a total 2 * 8 * 2 = 32 hardware threads.
</p>
</div>
</div>


<div id="outline-container-sec-7-1-7" class="outline-4">
<h4 id="sec-7-1-7"><span class="section-number-4">7.1.7</span> Performance demos of core sharing vs separate cores</h4>
<div class="outline-text-4" id="text-7-1-7">
<p>
(a benchmark which do data parallelism could be used <a href="https://github.com/eliben/code-for-blog/tree/master/2016/threads-affinity">here</a>)
He do some interesting experiment which shows sometimes running multiple threads on the same core actually hurts it. The reason is threads could compete over the execution units of the core and slow each other down.
</p>
</div>
</div>

<div id="outline-container-sec-7-1-8" class="outline-4">
<h4 id="sec-7-1-8"><span class="section-number-4">7.1.8</span> Summary</h4>
<div class="outline-text-4" id="text-7-1-8">
<ul class="org-ul">
<li>how to examine and set thread affinity 
</li>
<li>how to control placement of threads on logical CPUs by using the C++ standard threading library in conjunction with POSIX calls, and the bridging native handles exposed by the C++ threading library for this purpose.
</li>
<li>Different workloads have very different CPU utilization characteristics, which makes them more or less suitable for sharing a CPU core, sharing a socket or sharing a NUMA node.
</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-sec-7-2" class="outline-3">
<h3 id="sec-7-2"><span class="section-number-3">7.2</span> <a href="https://scicomp.stackexchange.com/questions/2028/portable-multicore-numa-memory-allocation-initialization-best-practices">Portable multicore/NUMA memory allocation/initialization best practices</a></h3>
<div class="outline-text-3" id="text-7-2">
</div><div id="outline-container-sec-7-2-1" class="outline-4">
<h4 id="sec-7-2-1"><span class="section-number-4">7.2.1</span> The Questions</h4>
<div class="outline-text-4" id="text-7-2-1">
<p>
Most operating system have ways to set threads affinity, but nost of them do not provide ways to set NUMA memory policies, except Linux. Its <code>libnuma</code> allow the application to manipulate memory policy and page migration at page granularity.
</p>

<p>
Working with a "first touch" policy means that the caller should create and distribute threads later when frist writing to the freshly allocated memory. (Very few systems are configured such that <code>malloc()</code> actually finds pages, it just promises to find them when they are actually faulted, perhaps by different threads.) This implies that allocation using <code>calloc()</code> or immediately initializing memory after allocation using <code>memset()</code> is harmful since it will tend to fault all the memory onto the memory bus of the core running the allocating thread, leading to worst-case memory bandwidth when the memory is accessed from multiple threads.
</p>

<p>
Are any solutions to NUMA allocation/initialization considered idiomatic?
</p>
</div>
</div>

<div id="outline-container-sec-7-2-2" class="outline-4">
<h4 id="sec-7-2-2"><span class="section-number-4">7.2.2</span> Answers from others:</h4>
<div class="outline-text-4" id="text-7-2-2">
</div><ol class="org-ol"><li><a id="sec-7-2-2-1" name="sec-7-2-2-1"></a>One solution to this problem is: disaggregate threads and tasks at the effectively, memory controller level.<br  /><div class="outline-text-5" id="text-7-2-2-1">
<p>
Remove the NUMA aspects from your code by having one task per CPU socket or memory controller and then threads under each task. You should be able to bind all memory to that socket/controller safely either via first-touch or one of the available API no matter which thread actually does the work of allocation or initialization.
</p>
</div>
</li>

<li><a id="sec-7-2-2-2" name="sec-7-2-2-2"></a>One solution talks about C++ <code>new</code> overloading<br  /><div class="outline-text-5" id="text-7-2-2-2">
<div class="org-src-container">

<pre class="src src-c"><span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;cstddef&gt;</span>
<span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;iostream&gt;</span>
<span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;new&gt;</span>

<span style="color: #586e75;">// </span><span style="color: #586e75;">Just to use two different classes.</span>
<span style="color: #b58900;">class</span> <span style="color: #268bd2;">arena</span> { };
<span style="color: #b58900;">class</span> <span style="color: #268bd2;">policy</span> { };

<span style="color: #859900; font-weight: bold;">struct</span> <span style="color: #b58900;">A</span>
{
  <span style="color: #b58900;">void</span>* <span style="color: #268bd2;">operator</span> new(std::size_t, arena&amp; arena_obj, policy&amp; policy_obj)
  {
    std::cout &lt;&lt; <span style="color: #2aa198;">"special operator new\n"</span>;
    <span style="color: #859900; font-weight: bold;">return</span> (<span style="color: #b58900;">void</span>*)0x1234; <span style="color: #586e75;">//</span><span style="color: #586e75;">Just to test</span>
  }
};

<span style="color: #b58900;">void</span>* <span style="color: #268bd2;">operator</span> new(std::size_t, arena&amp; arena_obj, policy&amp; policy_obj)
{
  std::cout &lt;&lt; <span style="color: #2aa198;">"special operator new (global)\n"</span>;
  <span style="color: #859900; font-weight: bold;">return</span> (<span style="color: #b58900;">void</span>*)0x5678; <span style="color: #586e75;">//</span><span style="color: #586e75;">Just to test</span>
}

<span style="color: #b58900;">int</span> <span style="color: #268bd2;">main</span> ()
{
  <span style="color: #b58900;">arena</span> <span style="color: #268bd2;">arena_obj</span>;
  <span style="color: #b58900;">policy</span> <span style="color: #268bd2;">policy_obj</span>;
  <span style="color: #b58900;">A</span>* <span style="color: #268bd2;">ptr</span> = new(arena_obj, policy_obj) A;
  <span style="color: #b58900;">int</span>* <span style="color: #268bd2;">iptr</span> = new(arena_obj, policy_obj) <span style="color: #b58900;">int</span>;
  std::cout &lt;&lt; ptr &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
  std::cout &lt;&lt; iptr &lt;&lt; <span style="color: #2aa198;">"\n"</span>;
}
</pre>
</div>
</div>
</li></ol>
</div>
</div>

<div id="outline-container-sec-7-3" class="outline-3">
<h3 id="sec-7-3"><span class="section-number-3">7.3</span> <a href="http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html">Affinity control outside OpenMP</a> (OpenMP topic: Affinity)</h3>
<div class="outline-text-3" id="text-7-3">
</div><div id="outline-container-sec-7-3-1" class="outline-4">
<h4 id="sec-7-3-1"><span class="section-number-4">7.3.1</span> OpenMP thread affinity control</h4>
<div class="outline-text-4" id="text-7-3-1">
</div><ol class="org-ol"><li><a id="sec-7-3-1-1" name="sec-7-3-1-1"></a>Thread binding<br  /><div class="outline-text-5" id="text-7-3-1-1">
<p>
Suppose there are two sockets and have total of 16 cores. core {0~7} is on socket 0, {8~15} in on socket 1.
</p>
<ul class="org-ul">
<li>If you define
OMP<sub>PLACES</sub>=cores
OMP<sub>PROC</sub><sub>BIND</sub>=close

<p>
Then
</p>
<ul class="org-ul">
<li>thread 0 goes to core 0, which is on socket 0,
</li>
<li>thread 1 goes to core 1, which is on socket 0,
</li>
<li>thread 2 goes to core 2, which is on socket 0,
</li>
<li>and so on, until thread 7 goes to core 7 on socket 0, and
</li>
<li>thread 8 goes to core 8, which is on socket 1,
</li>
<li>et cetera.
</li>
</ul>
<p>
The value OMP<sub>PROC</sub><sub>BIND</sub>=close means that the assignment goes successively through the available places.
</p>
</li>

<li>If you define
OMP<sub>PLACES</sub>=cores
OMP<sub>PROC</sub><sub>BIND</sub>=spread

<p>
Then
</p>
<ul class="org-ul">
<li>thread 0 goes to core 0, which is on socket 0,
</li>
<li>thread 1 goes to core 8, which is on socket 1,
</li>
<li>thread 2 goes to core 1, which is on socket 0,
</li>
<li>thread 3 goes to core 9, which is on socket 1,
</li>
<li>and so on, until thread 14 goes to core 7 on socket 0, and
</li>
<li>thread 15 goes to core 15, which is on socket 1.
</li>
</ul>
<p>
The variable OMP<sub>PROC</sub><sub>BIND</sub> can also be set to spread , which spreads the threads over the places.
</p>
</li>

<li>If you define
OMP<sub>PLACES</sub>=sockets

<p>
Then
</p>
<ul class="org-ul">
<li>thread 0 goes to socket 0,
</li>
<li>thread 1 goes to socket 1,
</li>
<li>thread 2 goes to socket 0 again,
</li>
<li>and so on
</li>
</ul>
<p>
It is very similar to previous example, expect the it does not bind a thread to a specific core, sor the OS can move threads about and it can put more than one thread on the same core, even if there is another core still unused.
</p>
</li>
</ul>
</div>
</li>

<li><a id="sec-7-3-1-2" name="sec-7-3-1-2"></a>Place Definition<br  /><div class="outline-text-5" id="text-7-3-1-2">
<ul class="org-ul">
<li>three predefined value for <code>OMP_PLACES</code>: sockets, cores, and threads. The threads value becomes relevant on processor that have hardware threads. In that case, OMP<sub>PLACES</sub>=cores does not tie a thread to a specific hardware thread, leading to possible collisions. Setting OMP<sub>PLACES</sub>=threads bonds each OpenMP thread to a specific hardware thread.
</li>

<li>General syntax: <code>location:number:stride</code>
<ol class="org-ol">
<li><code>OMP_PLACES="{0:8:1},{8:8:1}"\</code>
Has the same effect of sockets on a two-socket design with eight cores per socket.
It defines two places, each having eight consecutive cores. The threads are then places alternating between the two places, but no further specified inside the place.
</li>
<li>The setting cores is equal to
<code>OMP_PLACES="{0},{1},{2},...,{15}"\</code>
</li>
<li>On a four-socket design, the specification
<code>OMP_PLACES="{0:4:8}:4:1"\</code>
states that the place 0, 8, 16, 24 needs to be repeated 4 time, with a stride of 1.
</li>
</ol>
</li>
</ul>
</div>
</li>

<li><a id="sec-7-3-1-3" name="sec-7-3-1-3"></a>Binding possibilities<br  /><div class="outline-text-5" id="text-7-3-1-3">
<p>
Values of OMP<sub>PROC</sub><sub>BIND</sub> are:
</p>
<ul class="org-ul">
<li>false, set no binding
</li>
<li>true, lock threads to a core
</li>
<li>master, collocate threads with the master thread
</li>
<li>close, place threads close to the master in the places list
</li>
<li>spread, spread out threads as much as possible
</li>
</ul>

<p>
The effect can be made local by using <code>proc_bind</code> clause in the parallel directive.
</p>

<p>
A safe default setting is: <code>export OMP_PROC_BIND=true</code>, which prevents the OS form migrating a thread.
</p>
</div>
</li></ol>
</div>


<div id="outline-container-sec-7-3-2" class="outline-4">
<h4 id="sec-7-3-2"><span class="section-number-4">7.3.2</span> First-touch</h4>
<div class="outline-text-4" id="text-7-3-2">
</div><ol class="org-ol"><li><a id="sec-7-3-2-1" name="sec-7-3-2-1"></a>First-touch<br  /><div class="outline-text-5" id="text-7-3-2-1">
<p>
The affinity issue shows up in the first-touch phenomemon. Memory allocated with malloc and like routines is not actually allocated; that only happens when data is written to it. Consider the following code:
</p>
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">double</span> *<span style="color: #268bd2;">x</span> = (<span style="color: #b58900;">double</span>*) malloc(<span style="color: #b58900;">N</span>*<span style="color: #859900; font-weight: bold;">sizeof</span>(<span style="color: #b58900;">double</span>));

<span style="color: #859900; font-weight: bold;">for</span> (i=0; ilt;N; i++)
  x[i] = 0;

<span style="color: #268bd2;">#pragma</span> omp parallel <span style="color: #859900; font-weight: bold;">for</span>
<span style="color: #859900; font-weight: bold;">for</span> (i=0; ilt;N; i++)
  .... something with x[i] ...
</pre>
</div>

<p>
Since the initialization loop is not parallel it is executed by the master thread, <b>making all the memory associated with the socket of that thread</b>. Subsequent access by other socket will then access data from memory not attached to that socket.
</p>
</div>
</li></ol>
</div>

<div id="outline-container-sec-7-3-3" class="outline-4">
<h4 id="sec-7-3-3"><span class="section-number-4">7.3.3</span> SPMD sytle of programming</h4>
<div class="outline-text-4" id="text-7-3-3">
<p>
By regarding affinity, by adopting an SPMD style of programming. You could make this explicit by having each thread allocate its part of the arrays separately, and storing a private pointer as <code>threadprivate</code>. 
</p>

<p>
However, this makes it impossible for threads to access each other's parts of the distributed array, so this is only suitable for <i>total</i> data  parallel or embarassingly parallel applications.
</p>

<ul class="org-ul">
<li>embarrassingly parallel workload or problem is one where lietter or no effort is needed to separate the problem into number of parallel tasks. This is often the case where there is little or no dependency or need for communication between those parallel tasks, or for results between them.
</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-sec-7-4" class="outline-3">
<h3 id="sec-7-4"><span class="section-number-3">7.4</span> <a href="https://yunmingzhang.wordpress.com/2018/01/20/openmp-numa-and-libnuma-note/">Blog about OpenMP numa and libnuma note</a></h3>
<div class="outline-text-3" id="text-7-4">
<p>
Some notes on using libnuma, and achieving some similar functionalities in OpenMP using task affinity and places.
</p>
</div>
<div id="outline-container-sec-7-4-1" class="outline-4">
<h4 id="sec-7-4-1"><span class="section-number-4">7.4.1</span> libnuma</h4>
<div class="outline-text-4" id="text-7-4-1">
<p>
For memory allocation, ususally routine such as malloc uses first touch. However, "numa alloc" it would actually go in touch every location and make sure it is allocated on a certian node.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-7-5" class="outline-3">
<h3 id="sec-7-5"><span class="section-number-3">7.5</span> <a href="https://stackoverflow.com/questions/23142702/how-to-instantiate-c-objects-on-specific-numa-memory-nodes">How to instantiate C++ objects on specific NUMA memory nodes?</a></h3>
<div class="outline-text-3" id="text-7-5">
<div class="org-src-container">

<pre class="src src-c"><span style="color: #b58900;">void</span> *<span style="color: #268bd2;">blob</span> = numa_alloc_onnode(<span style="color: #859900; font-weight: bold;">sizeof</span>(Object), ...);
<span style="color: #b58900;">Object</span> *<span style="color: #268bd2;">object</span> = new(blob) Object;
</pre>
</div>
</div>
<div id="outline-container-sec-7-5-1" class="outline-4">
<h4 id="sec-7-5-1"><span class="section-number-4">7.5.1</span> <a href="http://en.cppreference.com/w/cpp/language/new">new expression</a> in C++</h4>
</div>
</div>
</div>

<div id="outline-container-sec-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> A blog shows what attribute need to monitor for NUMA node performance analysis</h2>
<div class="outline-text-2" id="text-8">
<p>
<a href="http://www.acceleware.com/blog/real-time-NUMA-node-performance-analysis-using-intel-performance-counter-monitor">Real-Time NUMA Node Performance Analysis Using Intel Performance Counter Monitor</a>
</p>
</div>
</div>
<div id="outline-container-sec-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> Summary</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-sec-9-1" class="outline-3">
<h3 id="sec-9-1"><span class="section-number-3">9.1</span> What is data parallelism and what is task parallelism?</h3>
<div class="outline-text-3" id="text-9-1">
<p>
A parallel program is composed of simultaneously executing processes. Problem decomposition relates to the way in which the constituent processes are formulated
</p>
<ul class="org-ul">
<li>Task parallelism, A task-parallel model focuses on processes, or threads of execution. These processes will often be behaviourally distinct, which emphasises the need for communication. Task parallelism is a natural way to express message-passing communication. 
Task parallelism is usually classified as MIMD/MPMD or MISD
</li>
<li>Data parallelism, A data-parallel model focuses on performing operations on a data set, typically a regularly structured array. A set of tasks will operate on this data, but independently on disjoint partitions.
Data parallelism is usually classified as MIMD/SPMD or SIMD
<img src="file:///Users/zw/Documents/screenshots/classification-parallel.png" alt="classification-parallel.png" />
</li>
</ul>
</div>
</div>

<div id="outline-container-sec-9-2" class="outline-3">
<h3 id="sec-9-2"><span class="section-number-3">9.2</span> How does each of them related to OpenMP</h3>
</div>
<div id="outline-container-sec-9-3" class="outline-3">
<h3 id="sec-9-3"><span class="section-number-3">9.3</span> What's their current status for using OpenMP under NUMA architectures?</h3>
</div>
<div id="outline-container-sec-9-4" class="outline-3">
<h3 id="sec-9-4"><span class="section-number-3">9.4</span> Which algorithm do I need to use, data/task parallelism or both?</h3>
</div>
<div id="outline-container-sec-9-5" class="outline-3">
<h3 id="sec-9-5"><span class="section-number-3">9.5</span> Operating system concepts</h3>
<div class="outline-text-3" id="text-9-5">
</div><div id="outline-container-sec-9-5-1" class="outline-4">
<h4 id="sec-9-5-1"><span class="section-number-4">9.5.1</span> work-stealing <a class='org-ref-reference' href="#blumofe99_sched_multit_comput_by_work_steal">blumofe99_sched_multit_comput_by_work_steal</a></h4>
</div>
</div>
<div id="outline-container-sec-9-6" class="outline-3">
<h3 id="sec-9-6"><span class="section-number-3">9.6</span> OpenMP affinity</h3>
<div class="outline-text-3" id="text-9-6">
</div><div id="outline-container-sec-9-6-1" class="outline-4">
<h4 id="sec-9-6-1"><span class="section-number-4">9.6.1</span> Concepts</h4>
<div class="outline-text-4" id="text-9-6-1">
<ul class="org-ul">
<li>OpenMP affinity consists of a <code>proc_bind</code> policy and a specification of places. It enables users to bind computations on specific places. The placement will hold for the duration of the parallel region.
<ul class="org-ul">
<li>place refers to processors which could be cores, or hardware threads, sockets.
</li>
</ul>
</li>
<li>However, the runtime is free to migrate the OpenMP threads to different cores (hardware thread, or sockets) prescribed within a given place, if two or more cores(hardware threads, sockets) have been assigned to a given place.
</li>
<li><code>OMP_PLACES</code> specify the places, witout setting it, OpenMP runtime will distribute and bind threads using the entire range of processors for the OpenMP program, based on the policy specified by <code>proc_bind</code>.
</li>
<li>SMT(Simultaneous Multi-Threading)
</li>
<li>HW-thread, hardware thread
</li>
<li>OpenMP places use the processor number to designate binding locations.
</li>
<li>Threads of a team are positioned onto places in a compact manner, a scattered distribution, or onto a master's place, by setting <code>proc_bind</code> clause to <i>close</i>, <i>spread</i>, or <i>master</i>
</li>
</ul>
</div>
</div>
<div id="outline-container-sec-9-6-2" class="outline-4">
<h4 id="sec-9-6-2"><span class="section-number-4">9.6.2</span> The <code>proc_bind</code> clause</h4>
<div class="outline-text-4" id="text-9-6-2">
<ul class="org-ul">
<li>picture of hierarchy: socket -&gt; physical core -&gt; hardware thread
</li>
<li>If a machine has 2 sockets, each of them has 4 cores, and each core has 2 hardware  threads. Then the <code>OMP_PLACES</code> variable could be set like:
</li>
</ul>
<p>
<code>OMP_PLACES="{0,1},{2,3},{4,5},{6,7},{8,9},{10,11},{12,13},{14,15}"\</code>  or equivalently ~OMP<sub>PLACES</sub>="{0:2}:8:2"
</p>
</div>

<ol class="org-ol"><li><a id="sec-9-6-2-1" name="sec-9-6-2-1"></a>Spread affinity policy<br  /><div class="outline-text-5" id="text-9-6-2-1">
<p>
(assuming the <code>OMP_PLACES</code> is the same)
When the number of threads is less than or equal to the number of places in the parent' place partition. Such as:
<code>#pragma omp parallel proc_bind(spread) num_threads(4)</code>
</p>
<ol class="org-ol">
<li>If the master thread is initially started on p0, the following placement of threads will be applied in the parallel region:
<ul class="org-ul">
<li>thread 0 executes on p0 with the place partition p0,p1
</li>
<li>thread 1 executes on p2 with the place partition p2,p3
</li>
<li>thread 2 executes on p4 with the place partition p4,p5
</li>
<li>thread 3 executes on p6 with the place partition p6,p7
</li>
</ul>
</li>
<li>If the master thread would initially be started on p2:
<ul class="org-ul">
<li>thread 0 executes on p2 with the place partition p2,p3
</li>
<li>thread 1 executes on p4 with the place partition p4,p5
</li>
<li>thread 2 executes on p6 with the place partition p6,p7
</li>
<li>thread 3 executes on p0 with the place partition p0,p1
</li>
</ul>
</li>
</ol>

<p>
When the number of thread is greater than the number of places in the parent's place partition.
<code>#pragma omp parallel num_threads(16) proc_bind(spread)</code>
</p>
<ol class="org-ol">
<li>If the master thread is initially started on p0:
<ul class="org-ul">
<li>threads 0,1 execute on p0 with the place partition p0
</li>
<li>threads 2,3 execute on p1 with the place partition p1
</li>
<li>threads 4,5 execute on p2 with the place partition p2
</li>
<li>threads 6,7 execute on p3 with the place partition p3
</li>
<li>threads 8,9 execute on p4 with the place partition p4
</li>
<li>threads 10,11 execute on p5 with the place partition p5
</li>
<li>threads 12,13 execute on p6 with the place partition p6
</li>
<li>threads 14,15 execute on p7 with the place partition p7
</li>
</ul>
</li>
<li>If the master thread is initially started on p2:
<ul class="org-ul">
<li>threads 0,1 execute on p2 with the place partition p2
</li>
<li>threads 2,3 execute on p3 with the place partition p3
</li>
<li>threads 4,5 execute on p4 with the place partition p4
</li>
<li>threads 6,7 execute on p5 with the place partition p5
</li>
<li>threads 8,9 execute on p6 with the place partition p6
</li>
<li>threads 10,11 execute on p7 with the place partition p7
</li>
<li>threads 12,13 execute on p0 with the place partition p0
</li>
<li>threads 14,15 execute on p1 with the place partition p1
</li>
</ul>
</li>
</ol>
</div>
</li>

<li><a id="sec-9-6-2-2" name="sec-9-6-2-2"></a>Close affinity policy<br  /><div class="outline-text-5" id="text-9-6-2-2">
<p>
When the number of threads is less than or equal to the number of places in parent's place partition:
<code>#pragma omp parallel proc_bind(close) num_threads(4)</code>
</p>
<ol class="org-ol">
<li>If the master thread is initially started on p0, the following placement of threads will be applied in the <code>parallel</code> region:
<ul class="org-ul">
<li>thread 0 executes on p0 with the place partition p0-p7
</li>
<li>thread 1 executes on p1 with the place partition p0-p7
</li>
<li>thread 2 executes on p2 with the place partition p0-p7
</li>
<li>thread 3 executes on p3 with the place partition p0-p7
</li>
</ul>
</li>
<li>If the master starts on p2:
<ul class="org-ul">
<li>thread 0 executes on p2 with the place partition p0-p7
</li>
<li>thread 1 executes on p3 with the place partition p0-p7
</li>
<li>thread 2 executes on p4 with the place partition p0-p7
</li>
<li>thread 3 executes on p5 with the place partition p0-p7
</li>
</ul>
</li>
</ol>

<p>
When the number of thread is greater than the number of places in parent's place partition:
<code>#pragma omp parallel num_threads(16) proc_bind(close)</code>
</p>
<ol class="org-ol">
<li>If master is on p0
<ul class="org-ul">
<li>threads 0,1 execute on p0 with the place partition p0-p7
</li>
<li>threads 2,3 execute on p1 with the place partition p0-p7
</li>
<li>threads 4,5 execute on p2 with the place partition p0-p7
</li>
<li>threads 6,7 execute on p3 with the place partition p0-p7
</li>
<li>threads 8,9 execute on p4 with the place partition p0-p7
</li>
<li>threads 10,11 execute on p5 with the place partition p0-p7
</li>
<li>threads 12,13 execute on p6 with the place partition p0-p7
</li>
<li>threads 14,15 execute on p7 with the place partition p0-p7
</li>
</ul>
</li>
<li>If the master is initially started on p2
<ul class="org-ul">
<li>threads 0,1 execute on p2 with the place partition p0-p7
</li>
<li>threads 2,3 execute on p3 with the place partition p0-p7
</li>
<li>threads 4,5 execute on p4 with the place partition p0-p7
</li>
<li>threads 6,7 execute on p5 with the place partition p0-p7
</li>
<li>threads 8,9 execute on p6 with the place partition p0-p7
</li>
<li>threads 10,11 execute on p7 with the place partition p0-p7
</li>
<li>threads 12,13 execute on p0 with the place partition p0-p7
</li>
<li>threads 14,15 execute on p1 with the place partition p0-p7
</li>
</ul>
</li>
</ol>
</div>
</li>

<li><a id="sec-9-6-2-3" name="sec-9-6-2-3"></a>Master affinity policy<br  /><div class="outline-text-5" id="text-9-6-2-3">
<p>
<code>#pragma omp parallel proc_bind(master) num_threads(4)</code>
</p>
<ol class="org-ol">
<li>If the master thread is initially running on p0:
<ul class="org-ul">
<li>threads 0-3 execute on p0 with the place partition p0-p7
</li>
</ul>
</li>
<li>If the master thread would initially be started on p2
<ul class="org-ul">
<li>threads 0-3 execute on p2 with the place partition p0-p7
</li>
</ul>
</li>
</ol>
</div>
</li>
<li><a id="sec-9-6-2-4" name="sec-9-6-2-4"></a>Questions:<br  /><div class="outline-text-5" id="text-9-6-2-4">
<p>
What is place partition? 
For example, in a 2 socket system with 8 cores in each socket, and sequential numbering in the socket for the core numbers, the <code>OMP_PLACES</code> variable would be set to
"{0:8},{8:8}", using the place syntax {lower<sub>bound</sub>:length:stride}, and the default stride is 1.
</p>
</div>
</li></ol>
</div>

<div id="outline-container-sec-9-6-3" class="outline-4">
<h4 id="sec-9-6-3"><span class="section-number-4">9.6.3</span> Affinity query function</h4>
<div class="outline-text-4" id="text-9-6-3">
<ul class="org-ul">
<li><code>omp_get_num_places()</code>
</li>
<li><code>omp_get_place_num_procs()</code>
</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-sec-9-7" class="outline-3">
<h3 id="sec-9-7"><span class="section-number-3">9.7</span> Next plan</h3>
<div class="outline-text-3" id="text-9-7">
<ul class="org-ul">
<li>The OpenMP affinity is about allocating the executing of tasks among different CPUs. But where each CPU access which part of memory is not determined. We need to allocate memory on fixed cores. Such as,

<ul class="org-ul">
<li>Array[0..N] will be allocation among different cores. 
</li>
<li>create multiple thread on those cores using OpenMP affinity 
</li>
<li>compute the results
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-sec-10" class="outline-2">
<h2 id="sec-10"><span class="section-number-2">10</span> Bibliography</h2>
<div class="outline-text-2" id="text-10">
<p>

<h1 class='org-ref-bib-h1'>Bibliography</h1>
<ul class='org-ref-bib'><li><a id="broquedis10_fores">[broquedis10_fores]</a> <a name="broquedis10_fores"></a>Fran\ccois Broquedis, Nathalie Furmento, , Brice Goglin, Pierre-Andr\'e Wacrenier, & Raymond Namyst, Forestgomp: An Efficient Openmp Environment for Numa  Architectures, <i>International Journal of Parallel Programming</i>, <b>38(5-6)</b>, 418-439 (2010). <a href="https://doi.org/10.1007/s10766-010-0136-3">link</a>. <a href="http://dx.doi.org/10.1007/s10766-010-0136-3">doi</a>.</li>
<li><a id="olivier12_openm_task_sched_strat_multic_numa_system">[olivier12_openm_task_sched_strat_multic_numa_system]</a> <a name="olivier12_openm_task_sched_strat_multic_numa_system"></a>Stephen L Olivier, Allan K Porterfield, Kyle B, Wheeler, Michael Spiegel & Jan F Prins, Openmp Task Scheduling Strategies for Multicore Numa  Systems, <i>The International Journal of High Performance
                  Computing Applications</i>, <b>26(2)</b>, 110-124 (2012). <a href="https://doi.org/10.1177/1094342011434065">link</a>. <a href="http://dx.doi.org/10.1177/1094342011434065">doi</a>.</li>
<li><a id="blumofe99_sched_multit_comput_by_work_steal">[blumofe99_sched_multit_comput_by_work_steal]</a> <a name="blumofe99_sched_multit_comput_by_work_steal"></a>Robert Blumofe & Charles Leiserson, Scheduling Multithreaded Computations By Work  Stealing, <i>Journal of the ACM</i>, <b>46(5)</b>, 720-748 (1999). <a href="https://doi.org/10.1145/324133.324234">link</a>. <a href="http://dx.doi.org/10.1145/324133.324234">doi</a>.</li>
<li><a id="beaumont06_centr">[beaumont06_centr]</a> <a name="beaumont06_centr"></a>Beaumont, Carter, Ferrante, , Legrand, Marchal & Robert, Centralized versus distributed schedulers for  multiple bag-of-task applications, nil, in in: Proceedings 20th IEEE International Parallel &
                  Distributed Processing Symposium, edited by (2006)</li>
<li><a id="10.1007/978-3-319-45550-1_7">[10.1007/978-3-319-45550-1_7]</a> <a name="10.1007/978-3-319-45550-1_7"></a>"Lee, , Tsugane, , Murai, & Sato, OpenMP Extension for Explicit Task Allocation on NUMA Architecture, 89-101, in in: "OpenMP: Memory, Devices, and Tasks", edited by "Maruyama, , de Supinski,  & Wahib, Springer International Publishing (2016)</li>
<li><a id="vikranth13_topol_aware_task_steal_chip">[vikranth13_topol_aware_task_steal_chip]</a> <a name="vikranth13_topol_aware_task_steal_chip"></a>Vikranth, Rajeev Wankar, Raghavendra & Rao, Topology Aware Task Stealing for On-Chip Numa  Multi-Core Processors, <i>Procedia Computer Science</i>, <b>18(nil)</b>, 379-388 (2013). <a href="https://doi.org/10.1016/j.procs.2013.05.201">link</a>. <a href="http://dx.doi.org/10.1016/j.procs.2013.05.201">doi</a>.</li>
<li><a id="DBLP:journals/corr/Tahan14">[DBLP:journals/corr/Tahan14]</a> <a name="DBLP:journals/corr/Tahan14"></a>Oussama Tahan, Towards Efficient OpenMP Strategies for Non-Uniform Architectures, <i>CoRR</i>, <b>abs/1411.7131</b>, (2014). <a href="http://arxiv.org/abs/1411.7131">link</a>.</li>
<li><a id="drebes14_topol_aware_depen_aware_sched">[drebes14_topol_aware_depen_aware_sched]</a> <a name="drebes14_topol_aware_depen_aware_sched"></a>Andi Drebes, Karine Heydemann, Nathalie Drach, , Antoniu Pop & Albert Cohen, Topology-Aware and Dependence-Aware Scheduling and  Memory Allocation for Task-Parallel Languages, <i>ACM Transactions on Architecture and Code
                  Optimization</i>, <b>11(3)</b>, 1-25 (2014). <a href="https://doi.org/10.1145/2641764">link</a>. <a href="http://dx.doi.org/10.1145/2641764">doi</a>.</li>
<li><a id="muddukrishna15_local_aware_task_sched_data">[muddukrishna15_local_aware_task_sched_data]</a> <a name="muddukrishna15_local_aware_task_sched_data"></a>Ananya Muddukrishna, Peter Jonsson, Mats & Brorsson, Locality-Aware Task Scheduling and Data Distribution  for Openmp Programs on Numa Systems and Manycore  Processors, <i>Scientific Programming</i>, <b>2015(nil)</b>, 1-16 (2015). <a href="https://doi.org/10.1155/2015/981759">link</a>. <a href="http://dx.doi.org/10.1155/2015/981759">doi</a>.</li>
</ul>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: zwpdbh</p>
<p class="date">Created: 2018-04-17 Tue 12:31</p>
<p class="creator"><a href="http://www.gnu.org/software/emacs/">Emacs</a> 25.3.1 (<a href="http://orgmode.org">Org</a> mode 8.2.10)</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
