
#+title: 2018-04-12 reading notes for OpenMP NUMA


* OpenMP on NUMA architectures
** Investigating NUMA topologies
- ~numactl --hardware~ print information on NUMA nodes in the system
#+BEGIN_SRC sh
  wzhao@r815:~$ numactl --hardware
  available: 8 nodes (0-7)
  node 0 cpus: 0 4 8 12 16 20 24 28
  node 0 size: 65526 MB
  node 0 free: 63750 MB
  node 1 cpus: 32 36 40 44 48 52 56 60
  node 1 size: 65536 MB
  node 1 free: 63869 MB
  node 2 cpus: 2 6 10 14 18 22 26 30
  node 2 size: 65536 MB
  node 2 free: 63860 MB
  node 3 cpus: 34 38 42 46 50 54 58 62
  node 3 size: 65536 MB
  node 3 free: 63861 MB
  node 4 cpus: 3 7 11 15 19 23 27 31
  node 4 size: 65536 MB
  node 4 free: 63863 MB
  node 5 cpus: 35 39 43 47 51 55 59 63
  node 5 size: 65536 MB
  node 5 free: 63869 MB
  node 6 cpus: 1 5 9 13 17 21 25 29
  node 6 size: 65536 MB
  node 6 free: 63871 MB
  node 7 cpus: 33 37 41 45 49 53 57 61
  node 7 size: 65520 MB
  node 7 free: 63836 MB
  node distances:
  node   0   1   2   3   4   5   6   7 
  0:  10  16  16  22  16  22  16  22 
  1:  16  10  22  16  16  22  22  16 
  2:  16  22  10  16  16  16  16  16 
  3:  22  16  16  10  16  16  22  22 
  4:  16  16  16  16  10  16  16  22 
  5:  22  22  16  16  16  10  22  16 
  6:  16  22  16  22  16  22  10  16 
  7:  22  16  16  22  22  16  16  10 
#+END_SRC
- ~numactl --show~ prints information on available resources for the process
#+BEGIN_SRC sh
  wzhao@r815:~$ numactl --show
  policy: default
  preferred node: current
  physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 
  cpubind: 0 1 2 3 4 5 6 7 
  nodebind: 0 1 2 3 4 5 6 7 
  membind: 0 1 2 3 4 5 6 7 
#+END_SRC
- use ~lstopo~ show the system topology
** Optimizing NUMA accesses
Goal: minimize the number of remote memory accesses as much as possible.
- how are threads distributed on the system?
- how is the data distributed on the system?
- how is work distributed across thread?

** Thread placement in OpenMP
*** Selecting the right binding strategy depends not only on the topology, but also on the characteristics of the application.
**** Putting threads far part, on different sockets
- May improve the aggregated memory bandwidth available to your application.
- May improve the combined cache size available to your application.
- May decrease performance of synchronization constructs
**** Putting threads close together, i.e. on two adjacent cores which possibly shared some caches
- May improve performance of synchronization constructs
- May decrease the available memory bandwidth and cache size
*** Available strategies
- close: put threads close together on the system
- spread: place threads far apart from each other
- master: run on the same place as the master thread
  
Assume the following machine:
#+NAME: fig:numa-example-01
#+CAPTION: numa-example, 2 sockets, 4 cores per socket, 4 hyper-threads per core
#+ATTR_ORG: :width 200/250/300/400/500/600
#+ATTR_LATEX: :width 2.0in
#+ATTR_HTML: :width 200/250/300/400/500/600px
[[file:~/Documents/screenshots/numa-nodes.png]]
*** Abstract names for ~OMP_PLACES~
- threads: each place corresponds to a single hardware thread on the target machine.
- cores: each place corresponds to a single core (having one or more hardware threads) on the target machine.
- sockets: each place corresponds to a single socket (consisting of one or more cores) on the target machine.
  
**** Example's objective
- Separate cores for outer loop and near cores for inner loop
#+BEGIN_SRC c
  OMP_PLACES={0,1,2,3}, {4,5,6,7}, ... = {0:4}:8:4
  #pragma omp parallel proc_bind(spread)
  #pragma omp parallel proc_bind(close)
#+END_SRC
- ~OMP_PLACES~
- ~proc_bind~
** Data Placement
- OpenMP does not provide support for cc-NUMA 
- Placement comes from the operating system (Operating system dependent)
- By default, use the "First Touch" placement policy
  
*** First touch in action
**** In serial code, all array element are allocated in the memory of the NUMA node containing the core executing this thread
#+BEGIN_SRC c
  double* A;
  A = (double*) malloc(N * sizeof(double));
  for (int i = 0; i < N; i++) {
    A[i] = 0.0;
   }
#+END_SRC

#+NAME: fig:numa-serial-init
#+CAPTION: serial code first toucle example
[[file:~/Documents/screenshots/numa-serial-init.png]]

**** In parallel code
#+BEGIN_SRC c
  double* A;
  A = (double*)malloc(N * sizeof(double));
  omp_set_num_threads(2);
  #pragma omp parallel for
  for (int i = 0; i < N; i++) {
    A[i] = 0.0;
   }
#+END_SRC

#+NAME: fig:numa-parallel-init
#+CAPTION: parallel code first touch example
[[file:~/Documents/screenshots/numa-parallel-init.png]]

** Memory and thread placement in Linux
~numactl~ command line tool to investigate and handle NUMA under Linux
- ~numactl --cpunodebind 0,1,2 ./a.out~, only use cores of NUMA node 0-2 to execute a.out
- ~numactl --physcpubind 0-17 ./a.out~, only use cores 0-17 to execute a.out.
- ~numactl --membind 0,3 ./a.out~, only use memory of NUMA node 0 and 3 to execute a.out.
- ~numactl --interleave 0-3 ./a.out~, distribute memory pages on NUMA nodes 0-3 in a round-robin fashion, overwrite first-touch policy.

~libnuma~ library of NUMA control
- ~void *numa_alloc_local(size_t size)~, allocate memory on the local NUMA node.
- ~void *numa_alloc_onnode(size_t size, int node)~, allocate memory on NUMA node node.
- ~void *numa_alloc_interleaved(size_t size)~ allocate memory distributed round-robin on all NUMA nodes.
- ~int numa_move_pages(int pid, unsigned long count, void **pages, const int *nodes, int *status, int flags)~, migrate memory pages at runtime to different NUMA nodes.
* OpenMP overview
** OpenMp Compilation process
- Annotated source code -> OpenMP compiler -> parallel object code
- Compiler can also generate sequential object cde
- compiler front end: parse OpenMP directives, correctness checks
- compiler back end: replace constructs by calls to runtime library, change structure of program
** Notation
*** Syntax
- directive: ~pragma~ statement
- runtime library routine: function defined in ~omp.h~
- structured block: simgle statement or compound statement with a single entry at the top and a single exit at the bottom.
- clause: modifies a directive's behavior
- Directives combined with code form a construct, which is a pattern to accomplish something. Such as a parallel construct is a ~parallel~ directive, with its optional clauses, and any code to be executed.
- Environment variable: defined outside the program
*** Notation (OpenMP)  
- master thread: original thread
- slave thread: all additional threads
- team: master thread + slave thread
*** Scope of Variables
- shared scope
  variable can be accessed by all threads in team, variables declared outside a structured block following a parallel directive
- private scope
  variable can be accessed by a single thread, variable declared inside a structured block following a parallel directive.
**** Handout only: scope of variables
- private variables are uninitialized
- initialize variables with value from master thread: ~firstprivate~
- ~default(none)~ requires programmer to specify visibility for all variables implicitly, good practice.
*** ~parallel for~ directive
- run loop interations in parallel
- shortcut: ~#pragma omp parallel for~
- loop iterations must be data-independent
- OpenMP must be able to determine the number of iterations before the loop is executed.
- Mapping of iterations to threads controlled by ~schedule~ clause
  - schedule(static [, chunksize]): block of chunksize iterations statically assigned to thread
  - schedule(dynamic [, chunksize]): thread reserves chunksize iterations from queue
  - schedule(guided [, chunksize]): same as dynamnic, but chunk size starts big and gets smaller and smaller, until it reaches chunksize.
  - schedule(runtime): scheduling behavior determined by environment variable
*** ~reduce~ clause
*** ~critical~ sections
*** ~Atomic~ statements
*** More synchronization constructs
- ~#pragma omp barrier~: wait until all threads arrive
- ~#pragma omp for nowait~: remove implicit barrier after for loop (also exists for other directives)
- ~#pragma omp master~: only executed by master thread
- ~#pragma omp single~: only executed by one thread
- Sections: define a number of blocks, every thread executes one block
- Locks: ~omp_init_lock()~, ~omp_set_lock()~, ~omp_unset_lock()~,...

** ForestGOMP: NUMA with OpenMP
- Objectives and motivations
  1) Keep buffer and threads operating on them on the same NUMA node (reducing contention)
  2) Processor level: group threads sharing data intensively(improve cache usage)
- Triggers for scheduling
  1) Allocation/deallocation of resources
  2) Processor becomes idle
  3) Change of hardware counters (e.g., cache miss, remote acess rate)
*** BubbleSched: hierarchical buble-based thread scheduler
- Runqueue for different hierarchical levels
- Bubble: group of threads sharing data or heavy synchronization
- Responsible for scheduling threads
*** Mami: NUMA-aware memory manager
- API for memory allocation 
- Can migrate memory to a different NUMA node
- Support next touch policy: migrate data to NUMA node of accessing thread.
  - Buffers are marked as migrate-on-next-touch when a thread migration is expected
  - Buffer is relocated if thread thouches buffer that is not located on local node
  - Implemented in kernel mode
- ForestGOMP: Mami-aware OpenMP Runtime
  - Mami attaches memory hints: e.g., which regions are access frequently by a certain thread
  - Initial distribution: put thread and corresponding memory on same NUMA node (local accesses)
  - Handle idleness: steal threads from local core, then from different NUMA node (also migrates memory; prefers threads with less memory)
  - Two levels of distribution: memory-aware, then cache-aware
* ForestGOMP: an efficient OpenMP environment for NUMA architectures cite:broquedis10_fores
* OpenMP task scheduling strategies for multicore NUMA systems cite:olivier12_openm_task_sched_strat_multic_numa_system
** Abstract
- Efficient scheduling of tasks on modern multi-socket multicore shared memory system requires consideration of shared caches and NUMA characteristics.
- They extendent the open source Qthreads threadling library to implement different scheduler designs, accepting OpenMP programs through the ROSE compiler.
** Introduction
- What is task-parallel programming models? what are the benefits?
- Efficient task scheduler
  - exploit cache and memory locality
  - maintain load balance
  - minimize overhead costs

  Trade off between them: 
  - However, load balancing operations can also contribute to overhead costs. Load balancing operations between sockets increase memory access time due to more cold cache misses and more high-latency remote memory accesses.
*** Their contributions
**** A hierarchical scheduling strategy targeting modern multi-socket multicore shared memory systems.
- NUMA architecture is not well supported by work-stealing scheduler with one queue per core or by centralized scheduler.
- work-stealing, a scheduling strategy for multithreaded computer programs. It solved the problem of executing a dynamically multithreaded commputation, one that can "spawn" new threads of execution, on a statically multithreaded computer, with a fixed number of processors. 
  See cite:blumofe99_sched_multit_comput_by_work_steal, a important paper.
  Also, see paper cite:beaumont06_centr for comparsion.

**** A detailed performance study on a current generation multi-socket multicore Intel system
**** Additional performance evaluation on a two-socket multicore AMD system and a 192-processor SGI Altix

** Background
- OpenMP 3.0 explicit task parallelism to complement its exisiting data parallel constructs. 
- ROSE compiler is used for performing syntactic and semantic analysis on OpenMP directives, transforming them into run-time library calls in the intermediate program.The ROSE common OpenMP run-time library maps the run-time calls to function in the Qthreads library.
- Qthreads vs Pthread, why Qthreads is needed in their paper?
  - Each worker pthread is pinned to a processor core and assigned to a locality domain, termed a shepherd.
  - what is FEB operation, a contex switch is triggered.
- "We used the Qthreads queueing implementation as a starting point for our scheduling work."
- "We implement OpenMP threads as worker pthreads. Unlike many OpenMP implementations, default loop scheduling is self-guided rather than static."
- "For task par- allelism, we implement each OpenMP task as a qthread."
- "We used the Qthreads FEB synchronization mechanism as a base layer upon which to implement taskwait and barrier sychronization."

** Conclustion
- Their MTS scheduler, combination of shared LIFO queues and work stealing maintains good load balance while supporting effective cache performance and limiting overhead cost. Notice: pure work stealing has been shown to provide the least variability in performance which is an important consideration for distributed applications in which barriers cause the application to run at the speed of the slowest worker.
- One challenge posed by their hierarchical scheduling strategy is the need for an efficient queue supporting concurrent access on both end, since works within a shepherd share a queue. (Lock-free dequeue).  
* OpenMP Extension for Explicit Task Allocation on NUMA Architecture cite:10.1007/978-3-319-45550-1_7
** Abstract
In this paper, we propose an extension for the OpenMP task construct to specify the loca- tion of tasks to exploit the locality in an explicit manner. The prototype compiler is implemented based on GCC.

** Introduction
- In the early version of OpenMP, the programming model had focused on data parallelism described by loop work sharing, which requires global synchro- nization in a parallel region. When the number of cores increases, synchronization overhead is getting bigger, and load imbalance among cores causes a significant performance drop.
- In OpenMP 4.0, task dependency can be specified using the depend clause in the task construct. Task parallelism can exploit potential parallelism in irregular applications. Task dependency can reduce synchronization overhead because it generates fine-grain synchronization between dependent tasks.
- To exploit memory bandwidth with NUMA architectures, OpenMP provides thread affinity options.
  - For OpenMP4.5, the ~proc_bind~ clause is discussed to specify a thread affinity scheme for a parallel region. These can be helpful to improve data locality when performing data parallelism with loop work sharing.
  - However, the current specification lacks functionality to do the same thing for task parallelism.
  #+BEGIN_COMMENT
  This means I need to decide which parallel model I need to use for SCF kNN algorithm.
  #+END_COMMENT
- An OpenMP extension to describe NUMA-aware task allocation explicitly. The extension specifies the data that the target task would access. 

** Related work
- Some NUMA-aware task scheduler based on work-stealing, see cite:vikranth13_topol_aware_task_steal_chip,DBLP:journals/corr/Tahan14,drebes14_topol_aware_depen_aware_sched,olivier12_openm_task_sched_strat_multic_numa_system    
- Manual data distribution among NUMA nodes and their NUMA-aware task scheduling algorithm in runtime. cite:muddukrishna15_local_aware_task_sched_data
- their is similar, also requeires explicit data distribution. However, task allocation is done explicityly using the extended OpenMP task construct.

** OpenMP Extension for NUMA-Aware Task Allocation
Generally, improving data locality and reducing remote memory access can exploit potential memory performance on the NUMA architecture. The same is true for task parallelism in OpenMP. A task should be executed on the NUMA node where its processing data is allocated to get the highest memory bandwidth. They propose a new clause named ~node_bind~ for OpenMP task construct. It specifies a NUMA node that the target task should be scheduled.

(to be continued)


* [[http://man7.org/linux/man-pages/man3/numa.3.html][Linux libnuma]]
** Description
*** It offers a programming interface to the NUMA policy. Available policies are:
- page interleaving (allocated in a round-robin fashion from all)
- subset of the nodes on the system
- preferred node allocation (preferably allocate on a particular node)
- local allocation (allocate on the node on which the task is currently executing)
- allocation only on specific nodes

*** Note:
The default memory allocation policy for tasks and all memory range is local allocation.
For setting a specific policy globally for all memory allocations in a process and its children, it is easiest to start it with the [[http://man7.org/linux/man-pages/man8/numactl.8.html][numactl]] utility.
All numa memory allocation policy only takes effect when a page is actually faulted into the address space of a process by accessing it. (First touch policy)

* Links related with C++ and libnuma
** [[https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/][C++11 threads, affinity and hyperthreading]]
*** Background and introduction
This post use C+++ threads as the main threading mechanism to demonstrate its points.
*** Logical CPUs, cores and threads
- Modern machines are multi-CPU, where these CPUs are divided into sockets and hardware cores. But the OS sees a number of "logical" CPUs that can execute tasks concurrently.
- To get such information on Linux, run ~cat /proc/cpuinfo~, a summary output can be obtained from ~lscpu~
*** Launching a thread per CPU
#+BEGIN_SRC c
  int main(int argc, const char** argv) {
    unsigned num_cpus = std::thread::hardware_concurrency();
    std::cout << "Launching " << num_cpus << " threads\n";

    // A mutex ensures orderly access to std::cout from multiple threads.
    std::mutex iomutex;
    std::vector<std::thread> threads(num_cpus);
    for (unsigned i = 0; i < num_cpus; ++i) {
      threads[i] = std::thread([&iomutex, i] {
          {
            // Use a lexical scope and lock_guard to safely lock the mutex only for
            // the duration of std::cout usage.
            std::lock_guard<std::mutex> iolock(iomutex);
            std::cout << "Thread #" << i << " is running\n";
          }

          // Simulate important work done by the tread by sleeping for a bit...
          std::this_thread::sleep_for(std::chrono::milliseconds(200));

        });
    }

    for (auto& t : threads) {
      t.join();
    }
    return 0;
  }
#+END_SRC

*** Detour - thread IDs and native handles
The thread library also lets us interact with platform-specific threading APIs by exposing native handles. 
Here's an example program that launches a single thread, and then queries its thread ID along with the native handle:
#+BEGIN_SRC c
  int main(int argc, const char** argv) {
    std::mutex iomutex;
    std::thread t = std::thread([&iomutex] {
        {
          std::lock_guard<std::mutex> iolock(iomutex);
          std::cout << "Thread: my id = " << std::this_thread::get_id() << "\n"
                    << "        my pthread id = " << pthread_self() << "\n";
        }
      });

    {
      std::lock_guard<std::mutex> iolock(iomutex);
      std::cout << "Launched t: id = " << t.get_id() << "\n"
                << "            native_handle = " << t.native_handle() << "\n";
    }

    t.join();
    return 0;
  }
#+END_SRC

*** Setting CPU affinity programatically
As we've seen earlier, command-line tools like taskset let us control the CPU affinity of a whole process. Sometimes, however, we'd like to do something more fine-grained and set the affinities of specific threads from within the program. How do we do that?
Use ~pthread_setaffinity_np~:  Here is the example which pin each thread to a single know CPU by settng its affinity:
#+BEGIN_SRC c
  int main(int argc, const char** argv) {
    constexpr unsigned num_threads = 4;
    // A mutex ensures orderly access to std::cout from multiple threads.
    std::mutex iomutex;
    std::vector<std::thread> threads(num_threads);
    for (unsigned i = 0; i < num_threads; ++i) {
      threads[i] = std::thread([&iomutex, i] {
          std::this_thread::sleep_for(std::chrono::milliseconds(20));
          while (1) {
            {
              // Use a lexical scope and lock_guard to safely lock the mutex only
              // for the duration of std::cout usage.
              std::lock_guard<std::mutex> iolock(iomutex);
              std::cout << "Thread #" << i << ": on CPU " << sched_getcpu() << "\n";
            }

            // Simulate important work done by the tread by sleeping for a bit...
            std::this_thread::sleep_for(std::chrono::milliseconds(900));
          }
        });

      // Create a cpu_set_t object representing a set of CPUs. Clear it and mark
      // only CPU i as set.
      cpu_set_t cpuset;
      CPU_ZERO(&cpuset);
      CPU_SET(i, &cpuset);
      int rc = pthread_setaffinity_np(threads[i].native_handle(),
                                      sizeof(cpu_set_t), &cpuset);
      if (rc != 0) {
        std::cerr << "Error calling pthread_setaffinity_np: " << rc << "\n";
      }
    }

    for (auto& t : threads) {
      t.join();
    }
    return 0;
  }
#+END_SRC
- use the ~native_handle~ method in order to pass the underlying native handle to the pthread call (it takes a ~pthread_t~ ID as its first argument).

*** Sharing a core with hyperthreading
To see the topology of processor on Linux:
1) ~lstopo~
2) An alternative non-graphical way to see which threads share the same core is to look at a special system file that exists per logical CPU. For example, for CPU 0:
   ~cat /sys/devices/system/cpu/cpu0/topology/thread_siblings_list~

The hardware thread explain: for example, a processor has 4 cores, each with 2 threads , for a total of hardware 8-threads -- 8 logical CPUs for the OS.

Server-class processors will have multiple sockets, each with a multi-core CPU.For example, a machine with 2 sockets, each of which is a 8-core CPU with hyper-threads enable: a total 2 * 8 * 2 = 32 hardware threads.


*** Performance demos of core sharing vs separate cores
(a benchmark which do data parallelism could be used [[https://github.com/eliben/code-for-blog/tree/master/2016/threads-affinity][here]])
He do some interesting experiment which shows sometimes running multiple threads on the same core actually hurts it. The reason is threads could compete over the execution units of the core and slow each other down.

*** Summary
- how to examine and set thread affinity 
- how to control placement of threads on logical CPUs by using the C++ standard threading library in conjunction with POSIX calls, and the bridging native handles exposed by the C++ threading library for this purpose.
- Different workloads have very different CPU utilization characteristics, which makes them more or less suitable for sharing a CPU core, sharing a socket or sharing a NUMA node.


** [[https://scicomp.stackexchange.com/questions/2028/portable-multicore-numa-memory-allocation-initialization-best-practices][Portable multicore/NUMA memory allocation/initialization best practices]]
*** The Questions
Most operating system have ways to set threads affinity, but nost of them do not provide ways to set NUMA memory policies, except Linux. Its ~libnuma~ allow the application to manipulate memory policy and page migration at page granularity.

Working with a "first touch" policy means that the caller should create and distribute threads later when frist writing to the freshly allocated memory. (Very few systems are configured such that ~malloc()~ actually finds pages, it just promises to find them when they are actually faulted, perhaps by different threads.) This implies that allocation using ~calloc()~ or immediately initializing memory after allocation using ~memset()~ is harmful since it will tend to fault all the memory onto the memory bus of the core running the allocating thread, leading to worst-case memory bandwidth when the memory is accessed from multiple threads.

Are any solutions to NUMA allocation/initialization considered idiomatic?

*** Answers from others:
**** One solution to this problem is: disaggregate threads and tasks at the effectively, memory controller level.
Remove the NUMA aspects from your code by having one task per CPU socket or memory controller and then threads under each task. You should be able to bind all memory to that socket/controller safely either via first-touch or one of the available API no matter which thread actually does the work of allocation or initialization.

**** One solution talks about C++ ~new~ overloading
#+BEGIN_SRC c
  #include <cstddef>
  #include <iostream>
  #include <new>

  // Just to use two different classes.
  class arena { };
  class policy { };

  struct A
  {
    void* operator new(std::size_t, arena& arena_obj, policy& policy_obj)
    {
      std::cout << "special operator new\n";
      return (void*)0x1234; //Just to test
    }
  };

  void* operator new(std::size_t, arena& arena_obj, policy& policy_obj)
  {
    std::cout << "special operator new (global)\n";
    return (void*)0x5678; //Just to test
  }

  int main ()
  {
    arena arena_obj;
    policy policy_obj;
    A* ptr = new(arena_obj, policy_obj) A;
    int* iptr = new(arena_obj, policy_obj) int;
    std::cout << ptr << "\n";
    std::cout << iptr << "\n";
  }
#+END_SRC

** [[http://pages.tacc.utexas.edu/~eijkhout/pcse/html/omp-affinity.html][Affinity control outside OpenMP]] (OpenMP topic: Affinity)
*** OpenMP thread affinity control
**** Thread binding
Suppose there are two sockets and have total of 16 cores. core {0~7} is on socket 0, {8~15} in on socket 1.
- If you define
  OMP_PLACES=cores
  OMP_PROC_BIND=close

  Then
  - thread 0 goes to core 0, which is on socket 0,
  - thread 1 goes to core 1, which is on socket 0,
  - thread 2 goes to core 2, which is on socket 0,
  - and so on, until thread 7 goes to core 7 on socket 0, and
  - thread 8 goes to core 8, which is on socket 1,
  - et cetera.
  The value OMP_PROC_BIND=close means that the assignment goes successively through the available places.

- If you define
  OMP_PLACES=cores
  OMP_PROC_BIND=spread

  Then
  - thread 0 goes to core 0, which is on socket 0,
  - thread 1 goes to core 8, which is on socket 1,
  - thread 2 goes to core 1, which is on socket 0,
  - thread 3 goes to core 9, which is on socket 1,
  - and so on, until thread 14 goes to core 7 on socket 0, and
  - thread 15 goes to core 15, which is on socket 1.
  The variable OMP_PROC_BIND can also be set to spread , which spreads the threads over the places.

- If you define
  OMP_PLACES=sockets

  Then
  - thread 0 goes to socket 0,
  - thread 1 goes to socket 1,
  - thread 2 goes to socket 0 again,
  - and so on
  It is very similar to previous example, expect the it does not bind a thread to a specific core, sor the OS can move threads about and it can put more than one thread on the same core, even if there is another core still unused.

**** Place Definition
- three predefined value for ~OMP_PLACES~: sockets, cores, and threads. The threads value becomes relevant on processor that have hardware threads. In that case, OMP_PLACES=cores does not tie a thread to a specific hardware thread, leading to possible collisions. Setting OMP_PLACES=threads bonds each OpenMP thread to a specific hardware thread.

- General syntax: ~location:number:stride~
  1) ~OMP_PLACES="{0:8:1},{8:8:1}"\~
     Has the same effect of sockets on a two-socket design with eight cores per socket.
     It defines two places, each having eight consecutive cores. The threads are then places alternating between the two places, but no further specified inside the place.
  2) The setting cores is equal to
     ~OMP_PLACES="{0},{1},{2},...,{15}"\~
  3) On a four-socket design, the specification
     ~OMP_PLACES="{0:4:8}:4:1"\~
     states that the place 0, 8, 16, 24 needs to be repeated 4 time, with a stride of 1.

**** Binding possibilities
Values of OMP_PROC_BIND are:
- false, set no binding
- true, lock threads to a core
- master, collocate threads with the master thread
- close, place threads close to the master in the places list
- spread, spread out threads as much as possible

The effect can be made local by using ~proc_bind~ clause in the parallel directive.

A safe default setting is: ~export OMP_PROC_BIND=true~, which prevents the OS form migrating a thread.


*** First-touch
**** First-touch
The affinity issue shows up in the first-touch phenomemon. Memory allocated with malloc and like routines is not actually allocated; that only happens when data is written to it. Consider the following code:
#+BEGIN_SRC c
  double *x = (double*) malloc(N*sizeof(double));
   
  for (i=0; ilt;N; i++)
    x[i] = 0;
   
  #pragma omp parallel for
  for (i=0; ilt;N; i++)
    .... something with x[i] ...
#+END_SRC

Since the initialization loop is not parallel it is executed by the master thread, *making all the memory associated with the socket of that thread*. Subsequent access by other socket will then access data from memory not attached to that socket.

*** SPMD sytle of programming
By regarding affinity, by adopting an SPMD style of programming. You could make this explicit by having each thread allocate its part of the arrays separately, and storing a private pointer as ~threadprivate~. 

However, this makes it impossible for threads to access each other's parts of the distributed array, so this is only suitable for /total/ data  parallel or embarassingly parallel applications.

- embarrassingly parallel workload or problem is one where lietter or no effort is needed to separate the problem into number of parallel tasks. This is often the case where there is little or no dependency or need for communication between those parallel tasks, or for results between them.


** [[https://yunmingzhang.wordpress.com/2018/01/20/openmp-numa-and-libnuma-note/][Blog about OpenMP numa and libnuma note]] 
Some notes on using libnuma, and achieving some similar functionalities in OpenMP using task affinity and places.
*** libnuma
For memory allocation, ususally routine such as malloc uses first touch. However, "numa alloc" it would actually go in touch every location and make sure it is allocated on a certian node.

** [[https://stackoverflow.com/questions/23142702/how-to-instantiate-c-objects-on-specific-numa-memory-nodes][How to instantiate C++ objects on specific NUMA memory nodes?]]
#+BEGIN_SRC c
  void *blob = numa_alloc_onnode(sizeof(Object), ...);
  Object *object = new(blob) Object;
#+END_SRC
*** [[http://en.cppreference.com/w/cpp/language/new][new expression]] in C++

* A blog shows what attribute need to monitor for NUMA node performance analysis
[[http://www.acceleware.com/blog/real-time-NUMA-node-performance-analysis-using-intel-performance-counter-monitor][Real-Time NUMA Node Performance Analysis Using Intel Performance Counter Monitor]]


  
* Summary 
** What is data parallelism and what is task parallelism? 
   A parallel program is composed of simultaneously executing processes. Problem decomposition relates to the way in which the constituent processes are formulated
   - Task parallelism, A task-parallel model focuses on processes, or threads of execution. These processes will often be behaviourally distinct, which emphasises the need for communication. Task parallelism is a natural way to express message-passing communication. 
     Task parallelism is usually classified as MIMD/MPMD or MISD
   - Data parallelism, A data-parallel model focuses on performing operations on a data set, typically a regularly structured array. A set of tasks will operate on this data, but independently on disjoint partitions.
     Data parallelism is usually classified as MIMD/SPMD or SIMD
     [[file:~/Documents/screenshots/classification-parallel.png]]

** How does each of them related to OpenMP
** What's their current status for using OpenMP under NUMA architectures?
** Which algorithm do I need to use, data/task parallelism or both?
** Operating system concepts
*** work-stealing cite:blumofe99_sched_multit_comput_by_work_steal
** OpenMP affinity
*** Concepts
- OpenMP affinity consists of a ~proc_bind~ policy and a specification of places. It enables users to bind computations on specific places. The placement will hold for the duration of the parallel region.
  - place refers to processors which could be cores, or hardware threads, sockets.
- However, the runtime is free to migrate the OpenMP threads to different cores (hardware thread, or sockets) prescribed within a given place, if two or more cores(hardware threads, sockets) have been assigned to a given place.
- ~OMP_PLACES~ specify the places, witout setting it, OpenMP runtime will distribute and bind threads using the entire range of processors for the OpenMP program, based on the policy specified by ~proc_bind~.
- SMT(Simultaneous Multi-Threading)
- HW-thread, hardware thread
- OpenMP places use the processor number to designate binding locations.
- Threads of a team are positioned onto places in a compact manner, a scattered distribution, or onto a master's place, by setting ~proc_bind~ clause to /close/, /spread/, or /master/
*** The ~proc_bind~ clause
- picture of hierarchy: socket -> physical core -> hardware thread
- If a machine has 2 sockets, each of them has 4 cores, and each core has 2 hardware  threads. Then the ~OMP_PLACES~ variable could be set like:
~OMP_PLACES="{0,1},{2,3},{4,5},{6,7},{8,9},{10,11},{12,13},{14,15}"\~  or equivalently ~OMP_PLACES="{0:2}:8:2"

**** Spread affinity policy
(assuming the ~OMP_PLACES~ is the same)
When the number of threads is less than or equal to the number of places in the parent' place partition. Such as:
~#pragma omp parallel proc_bind(spread) num_threads(4)~
1) If the master thread is initially started on p0, the following placement of threads will be applied in the parallel region:
   - thread 0 executes on p0 with the place partition p0,p1
   - thread 1 executes on p2 with the place partition p2,p3
   - thread 2 executes on p4 with the place partition p4,p5
   - thread 3 executes on p6 with the place partition p6,p7
2) If the master thread would initially be started on p2:
   - thread 0 executes on p2 with the place partition p2,p3
   - thread 1 executes on p4 with the place partition p4,p5
   - thread 2 executes on p6 with the place partition p6,p7
   - thread 3 executes on p0 with the place partition p0,p1

When the number of thread is greater than the number of places in the parent's place partition.
~#pragma omp parallel num_threads(16) proc_bind(spread)~
1) If the master thread is initially started on p0:
   - threads 0,1 execute on p0 with the place partition p0
   - threads 2,3 execute on p1 with the place partition p1
   - threads 4,5 execute on p2 with the place partition p2
   - threads 6,7 execute on p3 with the place partition p3
   - threads 8,9 execute on p4 with the place partition p4
   - threads 10,11 execute on p5 with the place partition p5
   - threads 12,13 execute on p6 with the place partition p6
   - threads 14,15 execute on p7 with the place partition p7
2) If the master thread is initially started on p2:
   - threads 0,1 execute on p2 with the place partition p2
   - threads 2,3 execute on p3 with the place partition p3
   - threads 4,5 execute on p4 with the place partition p4
   - threads 6,7 execute on p5 with the place partition p5
   - threads 8,9 execute on p6 with the place partition p6
   - threads 10,11 execute on p7 with the place partition p7
   - threads 12,13 execute on p0 with the place partition p0
   - threads 14,15 execute on p1 with the place partition p1

**** Close affinity policy
When the number of threads is less than or equal to the number of places in parent's place partition:
~#pragma omp parallel proc_bind(close) num_threads(4)~
1) If the master thread is initially started on p0, the following placement of threads will be applied in the ~parallel~ region:
   - thread 0 executes on p0 with the place partition p0-p7
   - thread 1 executes on p1 with the place partition p0-p7
   - thread 2 executes on p2 with the place partition p0-p7
   - thread 3 executes on p3 with the place partition p0-p7
2) If the master starts on p2:
   - thread 0 executes on p2 with the place partition p0-p7
   - thread 1 executes on p3 with the place partition p0-p7
   - thread 2 executes on p4 with the place partition p0-p7
   - thread 3 executes on p5 with the place partition p0-p7

When the number of thread is greater than the number of places in parent's place partition:
~#pragma omp parallel num_threads(16) proc_bind(close)~
1) If master is on p0
   - threads 0,1 execute on p0 with the place partition p0-p7
   - threads 2,3 execute on p1 with the place partition p0-p7
   - threads 4,5 execute on p2 with the place partition p0-p7
   - threads 6,7 execute on p3 with the place partition p0-p7
   - threads 8,9 execute on p4 with the place partition p0-p7
   - threads 10,11 execute on p5 with the place partition p0-p7
   - threads 12,13 execute on p6 with the place partition p0-p7
   - threads 14,15 execute on p7 with the place partition p0-p7
2) If the master is initially started on p2
   - threads 0,1 execute on p2 with the place partition p0-p7
   - threads 2,3 execute on p3 with the place partition p0-p7
   - threads 4,5 execute on p4 with the place partition p0-p7
   - threads 6,7 execute on p5 with the place partition p0-p7
   - threads 8,9 execute on p6 with the place partition p0-p7
   - threads 10,11 execute on p7 with the place partition p0-p7
   - threads 12,13 execute on p0 with the place partition p0-p7
   - threads 14,15 execute on p1 with the place partition p0-p7

**** Master affinity policy
~#pragma omp parallel proc_bind(master) num_threads(4)~
1) If the master thread is initially running on p0:
   - threads 0-3 execute on p0 with the place partition p0-p7
2) If the master thread would initially be started on p2
   - threads 0-3 execute on p2 with the place partition p0-p7
**** Questions:
What is place partition? 
For example, in a 2 socket system with 8 cores in each socket, and sequential numbering in the socket for the core numbers, the ~OMP_PLACES~ variable would be set to
"{0:8},{8:8}", using the place syntax {lower_bound:length:stride}, and the default stride is 1.

*** Affinity query function
- ~omp_get_num_places()~
- ~omp_get_place_num_procs()~



** Next plan
- The OpenMP affinity is about allocating the executing of tasks among different CPUs. But where each CPU access which part of memory is not determined. We need to allocate memory on fixed cores. Such as,

  - Array[0..N] will be allocation among different cores. 
  - create multiple thread on those cores using OpenMP affinity 
  - compute the results 
* Bibliography
bibliographystyle:abbrv
bibliography:../parallel-numa.bib
